[
  {
    "level": 1,
    "description": "A system that can likely thwart amateur attempts (OC1). This includes the operations of many hobbyist hackers, as well as more experienced hackers who implement completely untargeted \"spray and pray\" attacks.",
    "categories": [
      {
        "name": "Weight Security",
        "subcategories": [
          {
            "name": "Weight Storage",
            "controls": [
              {
                "name": "Sensitive data remain internal.",
                "compliance": {
                  "OpenAI": {
                    "score": 75,
                    "sources": [
                      "https://www.fierce-network.com/cloud/model-weights-are-heart-ais-intelligence-and-its-achilles-heel",
                      "https://openai.com/global-affairs/our-approach-to-frontier-risk",
                      "https://web.swipeinsight.app/posts/openai-unveils-security-architecture-for-frontier-ai-model-training-7065"
                    ],
                    "justification": "OpenAI has implemented multi-layered security controls for model weights including multi-party access approvals, private-linked storage, egress controls, and detection systems. They explicitly state that model weights are not distributed outside OpenAI and Microsoft, and remain controlled through API access."
                  },
                  "Anthropic": {
                    "score": 75,
                    "sources": [
                      "https://www.anthropic.com/news/activating-asl3-protections",
                      "https://www.anthropic.com/research/confidential-inference-trusted-vms",
                      "https://www.techrepublic.com/article/news-anthropic-ai-safety-level-3/",
                      "https://venturebeat.com/ai/why-anthropic-and-openai-are-obsessed-with-securing-llm-model-weights/"
                    ],
                    "justification": "Anthropic has implemented ASL-3 security standards with over 100 security controls, increased internal security measures to prevent model weight theft, and restricted outbound network traffic. Their CISO dedicates ~50% of time to protecting model weights, demonstrating strong commitment to keeping sensitive data internal."
                  },
                  "Google": {
                    "score": 50,
                    "sources": [
                      "https://cloud.google.com/blog/products/identity-security/introducing-ai-protection-security-for-the-ai-era",
                      "https://support.google.com/a/answer/15706919?hl=en",
                      "https://safety.google/cybersecurity-advancements/saif/",
                      "https://blog.google/technology/safety-security/introducing-googles-secure-ai-framework/"
                    ],
                    "justification": "Google has published security frameworks (SAIF) and general privacy commitments, but lacks specific public documentation about internal access controls for AI model weights. While they emphasize data protection and security, there's no clear evidence of implementing RAND's specific recommendations like centralizing weights storage or limiting personnel access."
                  },
                  "xAI": {
                    "score": 25,
                    "sources": [
                      "https://x.ai/legal/faq",
                      "https://x.ai/security",
                      "https://x.ai/legal/privacy-policy"
                    ],
                    "justification": "xAI has published general security measures and data protection policies, but no specific public information addresses internal containment of sensitive data like AI model weights, focusing instead on user data privacy and general security practices."
                  },
                  "Meta": {
                    "score": 0,
                    "sources": [
                      "https://www.cio.com/article/3599448/meta-offers-llama-ai-to-us-government-for-national-security.html",
                      "https://en.wikipedia.org/wiki/Llama_(language_model)",
                      "https://finance.yahoo.com/news/llama-copyright-drama-meta-stops-205006551.html"
                    ],
                    "justification": "Meta openly releases Llama model weights to the public under permissive licenses, directly contradicting the requirement that sensitive data remain internal. The company has shifted from case-by-case access (Llama 1) to broad public availability (Llama 2 and later)."
                  }
                }
              },
              {
                "name": "Weight encryption (best effort)",
                "compliance": {
                  "OpenAI": {
                    "score": 0,
                    "sources": [],
                    "justification": "No specific public information found regarding OpenAI's encryption practices for their AI model weights, despite general encryption policies for customer data."
                  },
                  "Anthropic": {
                    "score": 50,
                    "sources": [
                      "https://venturebeat.com/ai/why-anthropic-and-openai-are-obsessed-with-securing-llm-model-weights/",
                      "https://www.anthropic.com/news/activating-asl3-protections",
                      "https://www.anthropic.com/research/confidential-inference-trusted-vms"
                    ],
                    "justification": "Anthropic has publicly disclosed implementing encrypted storage for model weights and confidential computing approaches, but has not specifically confirmed implementation of the 'best effort' weight encryption expected for Security Level 1 as defined in the RAND report."
                  },
                  "Google": {
                    "score": 50,
                    "sources": [
                      "https://cloud.google.com/docs/security/encryption/default-encryption",
                      "https://blog.google/technology/safety-security/introducing-googles-secure-ai-framework/",
                      "https://www.rand.org/pubs/research_reports/RRA2849-1.html",
                      "https://deepmind.google/discover/blog/introducing-the-frontier-safety-framework/"
                    ],
                    "justification": "Google has strong general encryption practices (AES-256 for data at rest) and is developing advanced security frameworks (SAIF, Frontier Safety Framework), but there is no specific public evidence of implementing weight encryption as a best-effort measure for AI models as described in RAND's Security Level 1."
                  },
                  "xAI": {
                    "score": 50,
                    "sources": [
                      "https://guptadeepak.com/the-comprehensive-guide-to-understanding-grok-ai-architecture-applications-and-implications/",
                      "https://x.ai/news/grok-os",
                      "https://www.rand.org/pubs/research_briefs/RBA2849-1.html"
                    ],
                    "justification": "xAI mentions general encryption practices (data in transit and at rest) for Grok, and released Grok-1 weights publicly under Apache 2.0 license. However, no specific public information found about weight encryption as a dedicated security control matching RAND's Security Level 1 requirements."
                  },
                  "Meta": {
                    "score": 0,
                    "sources": [
                      "https://www.rand.org/pubs/research_reports/RRA2849-1.html",
                      "https://huggingface.co/meta-llama/Llama-3.1-8B-Instruct",
                      "https://en.wikipedia.org/wiki/Llama_(language_model)",
                      "https://github.com/meta-llama/llama-models"
                    ],
                    "justification": "No specific public information found regarding Meta's implementation of weight encryption for their AI model weights. While Meta releases model weights openly for many Llama models and mentions various security safeguards, there is no evidence of encryption practices for model weights as described in the RAND report."
                  }
                }
              }
            ]
          },
          {
            "name": "Physical Security",
            "controls": [
              {
                "name": "Data centers of cloud providers",
                "compliance": {
                  "OpenAI": {
                    "score": 75,
                    "sources": [
                      "https://venturebeat.com/ai/why-anthropic-and-openai-are-obsessed-with-securing-llm-model-weights/",
                      "https://web.swipeinsight.app/posts/openai-unveils-security-architecture-for-frontier-ai-model-training-7065",
                      "https://www.analyticsvidhya.com/blog/2024/05/openai-security-measures/",
                      "https://openai.com/enterprise-privacy"
                    ],
                    "justification": "OpenAI demonstrates strong compliance through multi-layered security controls including Azure-based infrastructure with defense-in-depth approach, multi-party authorization for weight access, private-linked storage resources, egress controls, and encryption (AES-256 at rest, TLS 1.2+ in transit). The organization explicitly states model weights are not distributed outside OpenAI and Microsoft, and implements comprehensive monitoring and access controls."
                  },
                  "Anthropic": {
                    "score": 75,
                    "sources": [
                      "https://www.rand.org/pubs/research_briefs/RBA2849-1.html",
                      "https://www.anthropic.com/news/anthropic-amazon",
                      "https://www.aboutamazon.com/news/aws/amazon-invests-additional-4-billion-anthropic-ai",
                      "https://www.anthropic.com/news/anthropic-amazon-trainium",
                      "https://www.anthropic.com/news/claude-gov-models-for-u-s-national-security-customers"
                    ],
                    "justification": "Anthropic demonstrates strong cloud security practices through partnerships with AWS and Google Cloud, utilizing their enterprise-grade security infrastructure including AWS GovCloud and classified environments. The company employs AWS's multi-layer security features and operates in secure cloud environments, meeting most Security Level 1 requirements for protecting against amateur threats."
                  },
                  "Google": {
                    "score": 75,
                    "sources": [
                      "https://cloud.google.com/docs/security/physical-to-logical-space",
                      "https://cloud.google.com/blog/products/identity-security/introducing-ai-protection-security-for-the-ai-era",
                      "https://www.google.com/about/datacenters/data-security/",
                      "https://safety.google/cybersecurity-advancements/saif/",
                      "https://cloud.google.com/docs/security/overview/whitepaper"
                    ],
                    "justification": "Google demonstrates comprehensive data center security with 6-layer physical security, encryption at rest, access controls, and their Secure AI Framework (SAIF). While not explicitly addressing RAND Security Level 1 requirements for AI model weights, their infrastructure provides strong foundational security controls."
                  },
                  "xAI": {
                    "score": 25,
                    "sources": [
                      "https://x.ai/security",
                      "https://dgtlinfra.com/elon-musk-data-centers/"
                    ],
                    "justification": "xAI uses AWS cloud services as part of their infrastructure and has cloud-first architectures, but there's no specific public information about their compliance with RAND's Security Level 1 requirements for AI model weights security in cloud provider data centers."
                  },
                  "Meta": {
                    "score": 25,
                    "sources": [
                      "https://www.edgeless.systems/solutions/ai-model-protection",
                      "https://www.rand.org/pubs/research_reports/RRA2849-1.html",
                      "https://www.maginative.com/article/meta-opens-llama-ai-model-to-us-military-defense-contractors/"
                    ],
                    "justification": "Meta has indicated AI infrastructure investments and partnerships with cloud providers (AWS and Azure hosting Llama models), but no specific public information was found detailing security measures for model weights in cloud provider data centers as expected for RAND Security Level 1."
                  }
                }
              }
            ]
          },
          {
            "name": "Access Control",
            "controls": [
              {
                "name": "Access control for sensitive assets",
                "compliance": {
                  "OpenAI": {
                    "score": 75,
                    "sources": [
                      "https://web.swipeinsight.app/posts/openai-unveils-security-architecture-for-frontier-ai-model-training-7065",
                      "https://www.analyticsvidhya.com/blog/2024/05/openai-security-measures/",
                      "https://siliconangle.com/2024/04/23/openai-enhances-security-control-cost-management-enterprise-api-users/"
                    ],
                    "justification": "OpenAI has implemented multi-layered access controls for model weights including multi-party approvals, role-based access control (RBAC), private-linked storage with authentication, and an AccessManager Service requiring least-privilege authorization, demonstrating substantial compliance with access control requirements for sensitive assets."
                  },
                  "Anthropic": {
                    "score": 75,
                    "sources": [
                      "https://www.anthropic.com/news/activating-asl3-protections",
                      "https://www.anthropic.com/news/frontier-model-security",
                      "https://www.anthropic.com/rsp-updates"
                    ],
                    "justification": "Anthropic has implemented multi-party authorization controls requiring two-party approval and time-bounded access for model weights, along with egress bandwidth controls and enhanced security measures under their ASL-3 standards, demonstrating strong access control practices for securing AI model weights."
                  },
                  "Google": {
                    "score": 50,
                    "sources": [
                      "https://deepmind.google/discover/blog/updating-the-frontier-safety-framework/",
                      "https://blog.google/technology/safety-security/introducing-googles-secure-ai-framework/",
                      "https://www.lesswrong.com/posts/y8eQjQaCamqdc842k/deepmind-s-frontier-safety-framework-is-weak-and-unambitious",
                      "https://deepmind.google/discover/blog/introducing-the-frontier-safety-framework/"
                    ],
                    "justification": "Google has published frameworks (SAIF, Frontier Safety Framework) acknowledging the importance of access control for model weights and outlined future plans for implementation, but admits current practices are at 'level 0 out of 4' for security levels, with hundreds having read access to weights without proper controls to prevent copying."
                  },
                  "xAI": {
                    "score": 25,
                    "sources": [
                      "https://www.rand.org/pubs/research_reports/RRA2849-1.html",
                      "https://x.ai/security",
                      "https://en.wikipedia.org/wiki/Grok_(chatbot)"
                    ],
                    "justification": "While xAI demonstrates some security practices like least privilege and IAM controls, they openly released Grok-1's weights publicly, which contradicts RAND's core recommendation to centralize and strictly control access to model weights."
                  },
                  "Meta": {
                    "score": 0,
                    "sources": [
                      "https://www.pivotpointsecurity.com/leaking-metas-llama-ai-the-good-the-bad-and-the-very-bad/",
                      "https://github.com/meta-llama/llama-models",
                      "https://en.wikipedia.org/wiki/Llama_(language_model)",
                      "https://www.rand.org/pubs/research_reports/RRA2849-1.html",
                      "https://www.oracle.com/artificial-intelligence/ai-open-weights-models/"
                    ],
                    "justification": "Meta takes an open-source approach with Llama models, making weights publicly available for download, which is fundamentally incompatible with access control requirements for protecting AI model weights as sensitive assets per RAND's Security Level 1 framework."
                  }
                }
              },
              {
                "name": "Access log or audit trail",
                "compliance": {
                  "OpenAI": {
                    "score": 25,
                    "sources": [
                      "https://openai.com/global-affairs/our-approach-to-frontier-risk",
                      "https://web.swipeinsight.app/posts/openai-unveils-security-architecture-for-frontier-ai-model-training-7065",
                      "https://trust.openai.com/"
                    ],
                    "justification": "OpenAI has mentioned general security measures including access controls and monitoring for model weights protection, but lacks publicly disclosed specific details about access log or audit trail implementation for model weights security."
                  },
                  "Anthropic": {
                    "score": 75,
                    "sources": [
                      "https://www.anthropic.com/transparency/voluntary-commitments",
                      "https://support.anthropic.com/en/articles/9970975-how-to-access-audit-logs",
                      "https://www.anthropic.com/enterprise",
                      "https://www.anthropic.com/news/activating-asl3-protections",
                      "https://www.anthropic.com/rsp-updates"
                    ],
                    "justification": "Anthropic demonstrates substantial compliance through ASL-3 security measures including multi-party authorization, hardware authentication, and temporary access controls for model weights. The company specifically mentions audit logs as an enterprise security feature and has implemented access controls with justification requirements and employee approval processes for model weight access."
                  },
                  "Google": {
                    "score": 50,
                    "sources": [
                      "https://safety.google/cybersecurity-advancements/saif/",
                      "https://cloud.google.com/vertex-ai/generative-ai/docs/enable-audit-logs",
                      "https://blog.google/technology/safety-security/introducing-googles-secure-ai-framework/",
                      "https://cloud.google.com/vertex-ai/docs/general/audit-logging"
                    ],
                    "justification": "Google has comprehensive audit logging infrastructure (Cloud Audit Logs, Vertex AI audit logs) and mentions AI security frameworks (SAIF), but no specific public documentation was found explicitly addressing audit trails for AI model weights access as required by RAND Security Level 1."
                  },
                  "xAI": {
                    "score": 25,
                    "sources": [
                      "https://x.ai/security"
                    ],
                    "justification": "xAI provides a 90-day audit trail for Business Tier accounts with on-demand export capability, demonstrating partial implementation of access logging controls. However, publicly available information is limited regarding comprehensive audit trail practices for AI model weights security."
                  },
                  "Meta": {
                    "score": 0,
                    "sources": [],
                    "justification": "No specific public information found regarding Meta's implementation of access logs or audit trails for AI model weights security as outlined in the RAND report."
                  }
                }
              }
            ]
          }
        ]
      },
      {
        "name": "Security of Network and Other (Nonweight) Sensitive Assets",
        "subcategories": [
          {
            "name": "Software",
            "controls": [
              {
                "name": "Moderately frequent software update management and compliance monitoring",
                "compliance": {
                  "OpenAI": {
                    "score": 25,
                    "sources": [
                      "https://openai.com/global-affairs/our-approach-to-frontier-risk",
                      "https://www.analyticsvidhya.com/blog/2024/05/openai-security-measures/",
                      "https://web.swipeinsight.app/posts/openai-unveils-security-architecture-for-frontier-ai-model-training-7065",
                      "https://openai.com/enterprise-privacy"
                    ],
                    "justification": "OpenAI demonstrates some practices related to security updates and monitoring, including bug bounty programs, security audits, and iterative risk assessment updates, but lacks specific public documentation about moderately frequent software update management for model weights security."
                  },
                  "Anthropic": {
                    "score": 75,
                    "sources": [
                      "https://www.anthropic.com/rsp-updates",
                      "https://www.anthropic.com/news/activating-asl3-protections",
                      "https://www.anthropic.com/news/announcing-our-updated-responsible-scaling-policy"
                    ],
                    "justification": "Anthropic demonstrates strong compliance through their ASL-3 security measures including comprehensive software inventory management, automated scanning, vulnerability monitoring, endpoint patching processes, and regular safeguard assessments as part of their Responsible Scaling Policy implementation."
                  },
                  "Google": {
                    "score": 50,
                    "sources": [
                      "https://cloud.google.com/compute/docs/os-patch-management",
                      "https://cloud.google.com/blog/products/identity-security/introducing-ai-protection-security-for-the-ai-era",
                      "https://safety.google/cybersecurity-advancements/saif/",
                      "https://cloud.google.com/kubernetes-engine/enterprise/docs/concepts/security-patching"
                    ],
                    "justification": "Google has established AI security frameworks (SAIF) and general patch management capabilities for cloud infrastructure, but lacks specific publicly documented policies for moderately frequent updates targeting AI model weights security as described in Security Level 1 of the RAND report."
                  },
                  "xAI": {
                    "score": 25,
                    "sources": [
                      "https://x.ai/security",
                      "https://guptadeepak.com/the-comprehensive-guide-to-understanding-grok-ai-architecture-applications-and-implications/",
                      "https://x.ai/legal/privacy-policy"
                    ],
                    "justification": "While xAI has demonstrated some security measures including continuous monitoring, encryption, and security audits, there is no specific public information about their software update management frequency or compliance monitoring procedures related to AI model weights security."
                  },
                  "Meta": {
                    "score": 25,
                    "sources": [
                      "https://www.infosecurity-magazine.com/news/meta-new-advances-ai-security/",
                      "https://huggingface.co/meta-llama/Llama-3.1-8B-Instruct",
                      "https://thehackernews.com/2025/04/meta-launches-llamafirewall-framework.html",
                      "https://www.npr.org/2025/05/31/nx-s1-5407870/meta-ai-facebook-instagram-risks"
                    ],
                    "justification": "Meta demonstrates some security practices including safeguards like Llama Guard and security tools, but there is limited public evidence of systematic software update management and compliance monitoring specifically for AI model weights. The company has shifted to automating 90% of risk assessments with AI, reducing human oversight."
                  }
                }
              }
            ]
          },
          {
            "name": "Access, Permissions, and Credentials",
            "controls": [
              {
                "name": "Least privilege principle",
                "compliance": {
                  "OpenAI": {
                    "score": 75,
                    "sources": [
                      "https://openai.com/global-affairs/our-approach-to-frontier-risk",
                      "https://web.swipeinsight.app/posts/openai-unveils-security-architecture-for-frontier-ai-model-training-7065"
                    ],
                    "justification": "OpenAI demonstrates strong implementation of least privilege principle through multi-party approval requirements for model weight access, role-based access control (RBAC) via Azure Entra ID, and their AccessManager Service that enables least-privilege authorization for sensitive resources including model weights."
                  },
                  "Anthropic": {
                    "score": 75,
                    "sources": [
                      "https://www.anthropic.com/news/activating-asl3-protections",
                      "https://www.anthropic.com/transparency/voluntary-commitments",
                      "https://www.anthropic.com/rsp-updates",
                      "https://privacy.anthropic.com/en/articles/10458704-how-does-anthropic-protect-the-personal-data-of-claude-ai-users",
                      "https://www.anthropic.com/news/frontier-model-security"
                    ],
                    "justification": "Anthropic has implemented two-party authorization for model weight access, grants only temporary access with smallest necessary permissions, and requires hardware authentication and justification for access - demonstrating strong adherence to least privilege principle."
                  },
                  "Google": {
                    "score": 50,
                    "sources": [
                      "https://cloud.google.com/vertex-ai/docs/general/access-control",
                      "https://cloud.google.com/architecture/framework/perspectives/ai-ml/security",
                      "https://cloud.google.com/vertex-ai/generative-ai/docs/control-model-access",
                      "https://www.rand.org/pubs/research_reports/RRA2849-1.html"
                    ],
                    "justification": "Google demonstrates partial implementation of least privilege for AI model weights through IAM access controls in Vertex AI and emphasizes secure-by-default infrastructure. However, public documentation lacks specific details about restricting AI model weights access, which is a critical component of Security Level 1 as described in the RAND report."
                  },
                  "xAI": {
                    "score": 25,
                    "sources": [
                      "https://www.rand.org/pubs/research_reports/RRA2849-1.html",
                      "https://krebsonsecurity.com/2025/05/xai-dev-leaks-api-key-for-private-spacex-tesla-llms/"
                    ],
                    "justification": "While xAI experienced a significant API key leak exposing access to 60+ private LLMs for 2 months (indicating poor access control practices), there is insufficient public information about their systematic implementation of least privilege principles for model weights security to provide a comprehensive assessment."
                  },
                  "Meta": {
                    "score": 0,
                    "sources": [],
                    "justification": "No specific public information found regarding Meta's implementation of least privilege principle for AI model weights security as described in the RAND report's Security Level 1 requirements."
                  }
                }
              },
              {
                "name": "Restrictions on device and account sharing",
                "compliance": {
                  "OpenAI": {
                    "score": 50,
                    "sources": [
                      "https://www.rand.org/pubs/research_briefs/RBA2849-1.html",
                      "https://openai.com/policies/terms-of-use",
                      "https://openai.com/policies/services-agreement/"
                    ],
                    "justification": "OpenAI has documented policies prohibiting account sharing and unauthorized access, but lacks publicly available information on specific technical controls for device restrictions related to model weights security."
                  },
                  "Anthropic": {
                    "score": 75,
                    "sources": [
                      "https://www.anthropic.com/rsp-updates",
                      "https://www.anthropic.com/news/activating-asl3-protections",
                      "https://www.anthropic.com/news/reflections-on-our-responsible-scaling-policy"
                    ],
                    "justification": "Anthropic has implemented ASL-3 security controls including two-party authorization for model weight access, multi-party authorization with time-bounded access controls, and access management with multiple clearance levels and granular per-role permissions, demonstrating strong restrictions on device and account sharing."
                  },
                  "Google": {
                    "score": 0,
                    "sources": [],
                    "justification": "No specific public information found regarding Google's implementation of device and account sharing restrictions for AI model weights security as described in RAND's Security Level 1."
                  },
                  "xAI": {
                    "score": 25,
                    "sources": [
                      "https://x.ai/legal/terms-of-service",
                      "https://en.wikipedia.org/wiki/Grok_(chatbot)",
                      "https://github.com/xai-org/grok-1/discussions/246",
                      "https://x.ai/legal/privacy-policy"
                    ],
                    "justification": "xAI shows minimal compliance with device and account sharing restrictions for model weights. While they have basic account security measures (password protection, limiting devices for mobile apps), they openly released Grok-1 model weights under Apache 2.0 license and plan to open-source Grok-2, indicating limited restrictions on model weights sharing."
                  },
                  "Meta": {
                    "score": 0,
                    "sources": [],
                    "justification": "No specific public information found regarding Meta's implementation of device and account sharing restrictions for AI model weights security as defined in RAND's Security Level 1. While Meta has various privacy controls for user data and AI services, there is no publicly available information addressing their compliance with this specific security control for protecting AI model weights."
                  }
                }
              },
              {
                "name": "Password best practices",
                "compliance": {
                  "OpenAI": {
                    "score": 0,
                    "sources": [],
                    "justification": "No specific public information found addressing OpenAI's password practices for AI model weights security as described in RAND's Security Level 1."
                  },
                  "Anthropic": {
                    "score": 75,
                    "sources": [
                      "https://www.anthropic.com/rsp-updates",
                      "https://www.anthropic.com/voluntary-commitments",
                      "https://www.anthropic.com/news/activating-asl3-protections",
                      "https://www.anthropic.com/news/frontier-model-security"
                    ],
                    "justification": "Anthropic has implemented two-party authorization/control for model weight access and multi-factor authentication, demonstrating strong authentication practices. However, specific details about comprehensive password policies (complexity, rotation, storage) are not publicly disclosed."
                  },
                  "Google": {
                    "score": 0,
                    "sources": [],
                    "justification": "No specific public information found regarding Google's implementation of password best practices for AI model weights security as defined in RAND Security Level 1."
                  },
                  "xAI": {
                    "score": 25,
                    "sources": [
                      "https://x.ai/security",
                      "https://www.rand.org/pubs/research_reports/RRA2849-1.html"
                    ],
                    "justification": "xAI states they adhere to NIST SP 800-63B password security standards on their security page, but no specific public information addresses password practices for AI model weight security as outlined in the RAND report's Security Level 1."
                  },
                  "Meta": {
                    "score": 0,
                    "sources": [],
                    "justification": "No specific public information found directly addressing Meta's password best practices for AI model weights security as outlined in the RAND report's Security Level 1 requirements."
                  }
                }
              },
              {
                "name": "Multifactor authentication",
                "compliance": {
                  "OpenAI": {
                    "score": 50,
                    "sources": [
                      "https://web.swipeinsight.app/posts/openai-unveils-security-architecture-for-frontier-ai-model-training-7065",
                      "https://help.openai.com/en/articles/7967234-enabling-multi-factor-authentication-mfa-with-openai",
                      "https://www.rand.org/pubs/research_briefs/RBA2849-1.html"
                    ],
                    "justification": "OpenAI has implemented MFA for user accounts accessing their services, and their published security architecture mentions multi-party approvals for accessing model weights. However, there is no specific public information confirming comprehensive MFA implementation for all personnel accessing AI model weights as required by RAND's Security Level 1."
                  },
                  "Anthropic": {
                    "score": 75,
                    "sources": [
                      "https://www.anthropic.com/rsp-updates",
                      "https://www.anthropic.com/news/activating-asl3-protections",
                      "https://www.anthropic.com/transparency/voluntary-commitments"
                    ],
                    "justification": "Anthropic has publicly disclosed implementation of multifactor authentication as part of their model weights security controls, specifically mentioning 'two-party controls, with explicit per-user access validation and multifactor authentication' and requiring 'hardware authentication device prompt' for access to model weights under their ASL-3 security standards."
                  },
                  "Google": {
                    "score": 0,
                    "sources": [],
                    "justification": "No specific public information found addressing Google's implementation of multifactor authentication for AI model weights security as described in RAND's Security Level 1."
                  },
                  "xAI": {
                    "score": 25,
                    "sources": [
                      "https://x.ai/security",
                      "https://www.rand.org/pubs/research_reports/RRA2849-1.html"
                    ],
                    "justification": "xAI publicly states they use hardware-based MFA (USB security keys) for system access, but there's no specific public information confirming this extends to AI model weights security."
                  },
                  "Meta": {
                    "score": 0,
                    "sources": [],
                    "justification": "No specific public information found regarding Meta's implementation of multifactor authentication for AI model weights security."
                  }
                }
              },
              {
                "name": "Single Sign-On (SSO)",
                "compliance": {
                  "OpenAI": {
                    "score": 75,
                    "sources": [
                      "https://openai.com/enterprise-privacy",
                      "https://web.swipeinsight.app/posts/openai-unveils-security-architecture-for-frontier-ai-model-training-7065"
                    ],
                    "justification": "OpenAI implements enterprise-level authentication through SAML SSO for ChatGPT Enterprise and API platforms, and employs multi-party approvals and authentication requirements for accessing model weights storage, demonstrating strong SSO controls for model weights security."
                  },
                  "Anthropic": {
                    "score": 0,
                    "sources": [],
                    "justification": "No specific public information found directly addressing Anthropic's use of SSO for AI model weights security as described in the RAND report's Security Level 1."
                  },
                  "Google": {
                    "score": 0,
                    "sources": [],
                    "justification": "No specific public information found regarding Google's implementation of SSO for AI model weights security as required by Security Level 1 in the RAND report."
                  },
                  "xAI": {
                    "score": 75,
                    "sources": [
                      "https://x.ai/security"
                    ],
                    "justification": "xAI explicitly states they use SSO for internal applications with WebAuthn and hardware-based MFA, and support SAML-based SSO for Business Tier accounts, demonstrating a strong SSO implementation aligned with security best practices."
                  },
                  "Meta": {
                    "score": 0,
                    "sources": [],
                    "justification": "No specific public information found regarding Meta's implementation of Single Sign-On (SSO) for AI model weights security as described in RAND's Security Level 1."
                  }
                }
              },
              {
                "name": "Backup and recovery tools",
                "compliance": {
                  "OpenAI": {
                    "score": 25,
                    "sources": [
                      "https://openai.com/enterprise-privacy",
                      "https://www.analyticsvidhya.com/blog/2024/05/openai-security-measures/",
                      "https://web.swipeinsight.app/posts/openai-unveils-security-architecture-for-frontier-ai-model-training-7065"
                    ],
                    "justification": "OpenAI has published information about security architecture for model weights protection including defense-in-depth approaches and multi-layered controls, but specific details about backup and recovery tools implementation are not publicly disclosed."
                  },
                  "Anthropic": {
                    "score": 0,
                    "sources": [],
                    "justification": "No specific public information found about Anthropic's backup and recovery tools for AI model weights, despite extensive documentation of other security controls."
                  },
                  "Google": {
                    "score": 25,
                    "sources": [
                      "https://cloud.google.com/architecture/ai-ml/storage-for-ai-ml",
                      "https://www.rand.org/pubs/research_briefs/RBA2849-1.html"
                    ],
                    "justification": "Google provides general backup and recovery capabilities for AI/ML workloads through Cloud Storage and checkpointing mechanisms, but no specific public information addresses backup and recovery tools explicitly designed for AI model weights security as outlined in RAND's Security Level 1."
                  },
                  "xAI": {
                    "score": 25,
                    "sources": [
                      "https://x.ai/security"
                    ],
                    "justification": "xAI documents general backup procedures including daily database snapshots and semi-annual restoration testing, but lacks specific public information about backup and recovery tools for AI model weights security."
                  },
                  "Meta": {
                    "score": 0,
                    "sources": [],
                    "justification": "No specific public information found regarding Meta's implementation of backup and recovery tools for AI model weights security."
                  }
                }
              },
              {
                "name": "Commercial identity and access management (IAM) tools",
                "compliance": {
                  "OpenAI": {
                    "score": 75,
                    "sources": [
                      "https://www.forrester.com/blogs/openai-requires-identity-verification-for-access-to-its-latest-models/",
                      "https://learn.microsoft.com/en-us/azure/ai-services/openai/how-to/role-based-access-control",
                      "https://web.swipeinsight.app/posts/openai-unveils-security-architecture-for-frontier-ai-model-training-7065"
                    ],
                    "justification": "OpenAI has implemented robust IAM controls including Azure Entra ID integration, role-based access control, multi-party approval requirements for sensitive resources, and AccessManager Service for least-privilege authorization. The company recently introduced mandatory identity verification for accessing advanced models and employs defense-in-depth approaches specifically for protecting model weights."
                  },
                  "Anthropic": {
                    "score": 75,
                    "sources": [
                      "https://www.anthropic.com/enterprise",
                      "https://www.anthropic.com/news/activating-asl3-protections",
                      "https://support.anthropic.com/en/articles/9797544-setting-up-single-sign-on-on-the-enterprise-plan"
                    ],
                    "justification": "Anthropic has implemented enterprise-grade IAM features including SSO, SAML, SCIM, domain capture, role-based permissions, and two-party authorization for model weight access as part of their ASL-3 security controls, demonstrating strong adoption of commercial IAM tools for securing AI model weights."
                  },
                  "Google": {
                    "score": 75,
                    "sources": [
                      "https://cloud.google.com/architecture/framework/perspectives/ai-ml/security",
                      "https://cloud.google.com/security/products/iam",
                      "https://cloud.google.com/blog/products/identity-security/mastering-secure-ai-on-google-cloud-a-practical-guide-for-enterprises"
                    ],
                    "justification": "Google Cloud provides comprehensive commercial IAM tools with fine-grained access control, audit trails, and role-based permissions management. While not explicitly documented for AI model weights protection at RAND's Security Level 1, Google's IAM system offers the capabilities needed for basic access control and monitoring required at this level."
                  },
                  "xAI": {
                    "score": 25,
                    "sources": [
                      "https://www.rand.org/pubs/research_reports/RRA2849-1.html",
                      "https://x.ai/security"
                    ],
                    "justification": "xAI's security page explicitly mentions using Amazon IAM for access control and following least privilege principles, but lacks specific details about commercial IAM tools for model weights security as described in RAND's Security Level 1."
                  },
                  "Meta": {
                    "score": 0,
                    "sources": [],
                    "justification": "No specific public information found regarding Meta's implementation of commercial IAM tools for AI model weights security. While Meta has security frameworks like LlamaFirewall and discusses security best practices in cloud deployments, there is no direct evidence of commercial IAM tool usage for model weights protection."
                  }
                }
              },
              {
                "name": "Zero Trust architecture (adherence to at least the standards in the \"Traditional\" level of CISA's Zero Trust Maturity Model)",
                "compliance": {
                  "OpenAI": {
                    "score": 75,
                    "sources": [
                      "https://openai.com/enterprise-privacy",
                      "https://www.analyticsvidhya.com/blog/2024/05/openai-security-measures/",
                      "https://web.swipeinsight.app/posts/openai-unveils-security-architecture-for-frontier-ai-model-training-7065"
                    ],
                    "justification": "OpenAI demonstrates strong implementation of Zero Trust principles including multi-party approval requirements for sensitive access, defense-in-depth architecture with multiple security layers, least-privilege authorization through AccessManager Service, and continuous verification through authentication and authorization controls. Their published security architecture aligns well with traditional-level Zero Trust maturity requirements."
                  },
                  "Anthropic": {
                    "score": 75,
                    "sources": [
                      "https://www.anthropic.com/news/activating-asl3-protections",
                      "https://www.anthropic.com/rsp-updates",
                      "https://www.anthropic.com/news/frontier-model-security"
                    ],
                    "justification": "Anthropic has implemented multi-party authorization controls, two-party control systems, and over 100 security controls for model weight protection under their ASL-3 standards. They follow NIST SSDF and SLSA frameworks and have implemented enhanced access controls with compartmentalization, though specific CISA Zero Trust Maturity Model compliance details are not publicly documented."
                  },
                  "Google": {
                    "score": 50,
                    "sources": [
                      "https://workspace.google.com/security/zero-trust/",
                      "https://cloud.google.com/learn/what-is-zero-trust",
                      "https://workspace.google.com/blog/identity-and-security/accelerating-zero-trust-and-digital-sovereignty-ai",
                      "https://cloud.google.com/beyondcorp"
                    ],
                    "justification": "Google has implemented Zero Trust principles through BeyondCorp and demonstrates compliance with CISA's Zero Trust Maturity Model for general infrastructure and Google Workspace. However, no specific public information was found directly addressing Zero Trust implementation for AI model weights security as described in the RAND report."
                  },
                  "xAI": {
                    "score": 0,
                    "sources": [],
                    "justification": "No specific public information found about xAI implementing Zero Trust architecture or adhering to CISA's Zero Trust Maturity Model standards for AI model weights security."
                  },
                  "Meta": {
                    "score": 25,
                    "sources": [
                      "https://the-ai-alliance.github.io/trust-safety-user-guide/exploring/meta-trust-safety/",
                      "https://www.oligo.security/blog/cve-2024-50050-critical-vulnerability-in-meta-llama-llama-stack",
                      "https://github.com/meta-llama/llama-models"
                    ],
                    "justification": "Meta demonstrates some security practices like trust and safety initiatives and vulnerability patches, but there is no public evidence of comprehensive Zero Trust architecture implementation specifically for AI model weights that meets CISA's Traditional level requirements."
                  }
                }
              }
            ]
          },
          {
            "name": "Hardware",
            "controls": [
              {
                "name": "Modern device architectures that establish root of trust and block malicious code execution",
                "compliance": {
                  "OpenAI": {
                    "score": 50,
                    "sources": [
                      "https://www.analyticsvidhya.com/blog/2024/05/openai-security-measures/",
                      "https://openai.com/index/reimagining-secure-infrastructure-for-advanced-ai/",
                      "https://web.swipeinsight.app/posts/openai-unveils-security-architecture-for-frontier-ai-model-training-7065"
                    ],
                    "justification": "OpenAI has publicly proposed trusted computing for AI accelerators (GPUs) to encrypt model weights until execution and uses Azure-based infrastructure with defense-in-depth security controls. However, there's no public evidence of full implementation of hardware-based root of trust architectures or TEEs specifically for model weights protection."
                  },
                  "Anthropic": {
                    "score": 25,
                    "sources": [
                      "https://www.anthropic.com/news/activating-asl3-protections",
                      "https://www.anthropic.com/rsp-updates",
                      "https://www.anthropic.com/research/confidential-inference-trusted-vms"
                    ],
                    "justification": "Anthropic has implemented binary authorization and endpoint controls that prevent unauthorized code execution, but lacks specific public information confirming implementation of hardware-based root of trust architectures as expected for Security Level 1."
                  },
                  "Google": {
                    "score": 25,
                    "sources": [
                      "https://cloud.google.com/security/securing-ai",
                      "https://safety.google/cybersecurity-advancements/saif/",
                      "https://blog.google/technology/safety-security/introducing-googles-secure-ai-framework/",
                      "https://cloud.google.com/docs/security/titanium-hardware-security-architecture"
                    ],
                    "justification": "Google has implemented strong hardware root of trust through Titan chips and Caliptra RTM in their infrastructure, but there's no specific public information confirming these are applied to protect AI model weights. Their SAIF framework addresses AI security broadly but doesn't explicitly detail hardware-based protections for model weights."
                  },
                  "xAI": {
                    "score": 25,
                    "sources": [
                      "https://x.ai/news/grok-os",
                      "https://x.ai/security"
                    ],
                    "justification": "xAI mentions using 'trusted hardware' and American-made servers from Dell and HPE to minimize supply chain attacks, but lacks public documentation of specific root of trust implementations or hardware-based security measures for protecting AI model weights."
                  },
                  "Meta": {
                    "score": 0,
                    "sources": [],
                    "justification": "No specific public information found confirming Meta's implementation of modern device architectures with hardware root of trust for AI model weights security as defined in RAND's Security Level 1."
                  }
                }
              },
              {
                "name": "CPU anti-exploitation features",
                "compliance": {
                  "OpenAI": {
                    "score": 0,
                    "sources": [],
                    "justification": "No specific public information found about OpenAI's implementation of CPU anti-exploitation features for AI model weights security."
                  },
                  "Anthropic": {
                    "score": 0,
                    "sources": [],
                    "justification": "No specific public information found about Anthropic's implementation of CPU anti-exploitation features for AI model weights security."
                  },
                  "Google": {
                    "score": 0,
                    "sources": [],
                    "justification": "No specific public information found about Google implementing CPU anti-exploitation features specifically for AI model weights security as described in the RAND report's Security Level 1 requirements."
                  },
                  "xAI": {
                    "score": 0,
                    "sources": [],
                    "justification": "No specific public information found regarding xAI's compliance with CPU anti-exploitation features for AI model weights security as described in the RAND report."
                  },
                  "Meta": {
                    "score": 25,
                    "sources": [
                      "https://engineering.fb.com/2025/04/29/security/whatsapp-private-processing-ai-tools/",
                      "https://www.rand.org/pubs/research_reports/RRA2849-1.html"
                    ],
                    "justification": "Meta demonstrates limited public disclosure of CPU anti-exploitation features for AI model weights. While they've implemented TEE-based Private Processing for WhatsApp using confidential computing, there's no specific evidence of comprehensive CPU anti-exploitation measures for their broader AI model weights security."
                  }
                }
              }
            ]
          },
          {
            "name": "Supply Chain",
            "controls": [
              {
                "name": "The reputability of software is reviewed before incorporation.",
                "compliance": {
                  "OpenAI": {
                    "score": 0,
                    "sources": [],
                    "justification": "No specific public information found addressing OpenAI's practices for reviewing software reputability before incorporation in the context of AI model weights security."
                  },
                  "Anthropic": {
                    "score": 75,
                    "sources": [
                      "https://www.anthropic.com/rsp-updates",
                      "https://www.anthropic.com/news/frontier-model-security",
                      "https://www.anthropic.com/transparency/voluntary-commitments"
                    ],
                    "justification": "Anthropic demonstrates strong software security practices including third-party dependency scanning, vulnerability monitoring, binary authorization for endpoints, and comprehensive software supply chain security measures. They implement NIST SSDF and SLSA frameworks, conduct regular security reviews, and have established controls for software inventory management and approval processes."
                  },
                  "Google": {
                    "score": 50,
                    "sources": [
                      "https://safety.google/cybersecurity-advancements/saif/",
                      "https://research.google/pubs/securing-the-ai-software-supply-chain/",
                      "https://ai.google/responsibility/safety/",
                      "https://cloud.google.com/software-supply-chain-security/docs/overview"
                    ],
                    "justification": "Google demonstrates partial compliance through its SAIF framework emphasizing software supply chain security for AI, Assured Open Source Software program for verified packages, and guidance on securing AI supply chains. However, no specific public information confirms systematic reputability reviews of all software before incorporation in AI model weights security contexts."
                  },
                  "xAI": {
                    "score": 0,
                    "sources": [],
                    "justification": "No specific public information found about xAI's practices for reviewing the reputability of software before incorporation related to AI model weights security."
                  },
                  "Meta": {
                    "score": 0,
                    "sources": [],
                    "justification": "No specific public information found regarding Meta's practices for reviewing software reputability before incorporation in the context of AI model weights security."
                  }
                }
              }
            ]
          },
          {
            "name": "Security Tooling",
            "controls": [
              {
                "name": "Modern authentication infrastructure",
                "compliance": {
                  "OpenAI": {
                    "score": 75,
                    "sources": [
                      "https://openai.com/enterprise-privacy",
                      "https://www.rand.org/pubs/research_briefs/RBA2849-1.html",
                      "https://web.swipeinsight.app/posts/openai-unveils-security-architecture-for-frontier-ai-model-training-7065"
                    ],
                    "justification": "OpenAI has implemented Azure Entra ID for identity management, role-based access control, multi-party approvals for access grants, and authentication requirements for private-linked storage resources containing model weights, demonstrating strong authentication infrastructure aligned with Security Level 1 requirements."
                  },
                  "Anthropic": {
                    "score": 75,
                    "sources": [
                      "https://www.anthropic.com/news/activating-asl3-protections",
                      "https://www.anthropic.com/rsp-updates",
                      "https://www.anthropic.com/news/frontier-model-security"
                    ],
                    "justification": "Anthropic has implemented modern authentication infrastructure including two-party authorization for model weight access, multi-factor authentication, and time-bounded access controls as part of their ASL-3 security measures, demonstrating strong alignment with Security Level 1 requirements."
                  },
                  "Google": {
                    "score": 50,
                    "sources": [
                      "https://www.rand.org/pubs/research_reports/RRA2849-1.html",
                      "https://cloud.google.com/blog/products/identity-security/mandatory-mfa-is-coming-to-google-cloud-heres-what-you-need-to-know",
                      "https://safety.google/cybersecurity-advancements/saif/",
                      "https://cloud.google.com/docs/authentication/mfa-requirement"
                    ],
                    "justification": "Google has announced mandatory MFA for Google Cloud by 2025 and has general security frameworks (SAIF), but no specific public documentation exists detailing modern authentication infrastructure explicitly for AI model weights protection."
                  },
                  "xAI": {
                    "score": 25,
                    "sources": [
                      "https://siliconangle.com/2024/03/17/elon-musks-xai-releases-grok-1-architecture-apple-advances-multimodal-ai-research/",
                      "https://x.ai/news/grok-os",
                      "https://x.ai/security"
                    ],
                    "justification": "xAI publicly states adherence to NIST SP 800-63B authentication standards on their security page, but lacks specific public documentation about modern authentication infrastructure for AI model weights security as expected for RAND Security Level 1."
                  },
                  "Meta": {
                    "score": 0,
                    "sources": [],
                    "justification": "No specific public information found about Meta's modern authentication infrastructure implementation for AI model weights security as described in RAND's Security Level 1."
                  }
                }
              },
              {
                "name": "Commercial network security solutions",
                "compliance": {
                  "OpenAI": {
                    "score": 75,
                    "sources": [
                      "https://web.swipeinsight.app/posts/openai-unveils-security-architecture-for-frontier-ai-model-training-7065",
                      "https://www.analyticsvidhya.com/blog/2024/05/openai-security-measures/",
                      "https://learn.microsoft.com/en-us/legal/cognitive-services/openai/data-privacy",
                      "https://openai.com/global-affairs/our-approach-to-frontier-risk"
                    ],
                    "justification": "OpenAI has implemented several commercial network security solutions including SOC 2 Type 2 certification, network isolation, TLS 1.2+ encryption, access controls, and dedicated Azure-based infrastructure with Kubernetes orchestration. These measures align well with RAND Security Level 1 requirements for protecting against amateur attempts and basic attacks."
                  },
                  "Anthropic": {
                    "score": 75,
                    "sources": [
                      "https://www.anthropic.com/news/activating-asl3-protections",
                      "https://www.anthropic.com/news/frontier-model-security",
                      "https://venturebeat.com/ai/why-anthropic-and-openai-are-obsessed-with-securing-llm-model-weights/"
                    ],
                    "justification": "Anthropic has implemented over 100 security controls including egress bandwidth controls, two-party authorization for model weight access, enhanced change management protocols, and endpoint software controls, demonstrating strong commercial security measures aligned with industry best practices for protecting AI model weights."
                  },
                  "Google": {
                    "score": 0,
                    "sources": [],
                    "justification": "No specific public information found regarding Google's implementation of commercial network security solutions for AI model weights security as described in RAND's Security Level 1."
                  },
                  "xAI": {
                    "score": 0,
                    "sources": [],
                    "justification": "No specific public information found regarding xAI's implementation of commercial network security solutions for AI model weights protection as expected for Security Level 1 in the RAND report."
                  },
                  "Meta": {
                    "score": 50,
                    "sources": [
                      "https://about.fb.com/news/2024/11/open-source-ai-america-global-security/",
                      "https://www.infosecurity-magazine.com/news/meta-new-advances-ai-security/",
                      "https://venturebeat.com/ai/rsac-2025-cisco-and-meta-put-open-source-ai-at-the-heart-of-enterprise-threat-defense/"
                    ],
                    "justification": "Meta demonstrates partial compliance through partnerships with AWS and Microsoft Azure for secure cloud hosting of Llama models, and development of security tools like LlamaFirewall and Llama Guard. However, no specific evidence found of comprehensive commercial network security solutions implementation as detailed in RAND's Security Level 1 requirements."
                  }
                }
              },
              {
                "name": "Commercial endpoint security solutions",
                "compliance": {
                  "OpenAI": {
                    "score": 50,
                    "sources": [
                      "https://openai.com/enterprise-privacy",
                      "https://www.analyticsvidhya.com/blog/2024/05/openai-security-measures/",
                      "https://web.swipeinsight.app/posts/openai-unveils-security-architecture-for-frontier-ai-model-training-7065"
                    ],
                    "justification": "OpenAI has publicly disclosed implementing several security measures including data encryption (AES-256 at rest, TLS 1.2+ in transit), access controls, SOC 2 compliance, and identity management systems. However, specific details about commercial endpoint security solutions deployment for model weights protection are not publicly disclosed."
                  },
                  "Anthropic": {
                    "score": 50,
                    "sources": [
                      "https://www.rand.org/pubs/research_reports/RRA2849-1.html",
                      "https://www.anthropic.com/news/activating-asl3-protections",
                      "https://www.anthropic.com/rsp-updates",
                      "https://www.anthropic.com/news/frontier-model-security"
                    ],
                    "justification": "Anthropic has implemented ASL-3 security measures including enhanced internal security controls and egress bandwidth monitoring, but no specific public information confirms deployment of commercial endpoint security solutions as described in the RAND report for Security Level 1."
                  },
                  "Google": {
                    "score": 25,
                    "sources": [
                      "https://safety.google/cybersecurity-advancements/saif/",
                      "https://cloud.google.com/blog/products/identity-security/introducing-ai-protection-security-for-the-ai-era",
                      "https://cloud.google.com/endpoint-verification/docs/overview",
                      "https://cloud.google.com/security/securing-ai",
                      "https://www.rand.org/pubs/research_reports/RRA2849-1.html"
                    ],
                    "justification": "Google discusses AI security broadly through SAIF framework and Model Armor, but lacks specific public documentation about implementing commercial endpoint security solutions (EDR/EPP) for protecting AI model weights infrastructure as outlined in RAND's Security Level 1 requirements."
                  },
                  "xAI": {
                    "score": 25,
                    "sources": [
                      "https://www.rand.org/pubs/research_reports/RRA2849-1.html",
                      "https://x.ai/security"
                    ],
                    "justification": "xAI mentions having endpoint security measures and device management tools, but publicly available information does not specifically address commercial endpoint security solutions for AI model weights protection as per RAND's framework."
                  },
                  "Meta": {
                    "score": 0,
                    "sources": [],
                    "justification": "No specific public information found about Meta implementing commercial endpoint security solutions for AI model weights protection as described in RAND's Security Level 1."
                  }
                }
              },
              {
                "name": "Reliance on standard security infrastructure (depending on circumstances)",
                "compliance": {
                  "OpenAI": {
                    "score": 75,
                    "sources": [
                      "https://learn.microsoft.com/en-us/security/benchmark/azure/baselines/azure-openai-security-baseline",
                      "https://www.analyticsvidhya.com/blog/2024/05/openai-security-measures/",
                      "https://web.swipeinsight.app/posts/openai-unveils-security-architecture-for-frontier-ai-model-training-7065"
                    ],
                    "justification": "OpenAI demonstrates strong reliance on standard security infrastructure including Azure cloud services, Kubernetes orchestration, Azure Entra ID for identity management, role-based access control, TLS encryption, private endpoints, and defense-in-depth approaches. However, they acknowledge that securing against the most capable actors requires more investment."
                  },
                  "Anthropic": {
                    "score": 100,
                    "sources": [
                      "https://www.anthropic.com/news/activating-asl3-protections",
                      "https://www.anthropic.com/news/frontier-model-security",
                      "https://www.maginative.com/article/anthropic-calls-for-stringent-security-safeguards-for-frontier-ai-models/"
                    ],
                    "justification": "Anthropic has implemented ASL-2 and ASL-3 security standards with over 100 security controls, including two-party authorization, endpoint controls, and cybersecurity best practices, significantly exceeding basic Security Level 1 requirements."
                  },
                  "Google": {
                    "score": 0,
                    "sources": [],
                    "justification": "No specific public information found addressing Google's compliance with RAND Security Level 1 requirements for AI model weights security."
                  },
                  "xAI": {
                    "score": 25,
                    "sources": [
                      "https://www.oracle.com/news/announcement/xais-grok-models-are-now-on-oracle-cloud-infrastructure-2025-06-17/",
                      "https://job-boards.greenhouse.io/xai/jobs/4559149007",
                      "https://x.ai/security"
                    ],
                    "justification": "xAI's security page shows basic infrastructure security measures (VPN access, encryption, cloud security tools), but lacks specific public documentation about AI model weights security controls as outlined in RAND's Security Level 1 requirements."
                  },
                  "Meta": {
                    "score": 0,
                    "sources": [],
                    "justification": "No specific public information found regarding Meta's implementation of standard security infrastructure for AI model weights protection as outlined in RAND's Security Level 1 requirements."
                  }
                }
              }
            ]
          },
          {
            "name": "Configuration Management",
            "controls": [
              {
                "name": "Enforce screen locks for inactivity",
                "compliance": {
                  "OpenAI": {
                    "score": 0,
                    "sources": [],
                    "justification": "No specific public information found regarding OpenAI's enforcement of screen locks for inactivity as related to AI model weights security."
                  },
                  "Anthropic": {
                    "score": 0,
                    "sources": [],
                    "justification": "No specific public information found regarding Anthropic's implementation of screen locks for inactivity as a security control for AI model weights protection."
                  },
                  "Google": {
                    "score": 0,
                    "sources": [],
                    "justification": "No specific public information found about Google's implementation of screen lock for inactivity policies related to AI model weights security."
                  },
                  "xAI": {
                    "score": 0,
                    "sources": [],
                    "justification": "No specific public information found regarding xAI's implementation of screen lock enforcement for inactivity as it relates to AI model weights security."
                  },
                  "Meta": {
                    "score": 0,
                    "sources": [],
                    "justification": "No specific public information found regarding Meta's implementation of screen locks for inactivity as a security control for AI model weights."
                  }
                }
              }
            ]
          }
        ]
      },
      {
        "name": "Personnel Security",
        "subcategories": [
          {
            "name": "Awareness and Training",
            "controls": [
              {
                "name": "Basic onboarding information security training for employees",
                "compliance": {
                  "OpenAI": {
                    "score": 0,
                    "sources": [],
                    "justification": "No specific public information found about OpenAI's basic onboarding information security training for employees related to AI model weights security."
                  },
                  "Anthropic": {
                    "score": 25,
                    "sources": [
                      "https://www.anthropic.com/news/activating-asl3-protections",
                      "https://www.anthropic.com/rsp-updates",
                      "https://www.anthropic.com/news/frontier-model-security"
                    ],
                    "justification": "While Anthropic emphasizes security culture and has an insider threat program with employee education, no specific details about basic onboarding security training were found in public documentation. The company mentions educating employees on insider risk but lacks public disclosure of comprehensive onboarding security training programs."
                  },
                  "Google": {
                    "score": 50,
                    "sources": [
                      "https://workspace.google.com/solutions/ai/",
                      "https://workspace.google.com/learn-more/security/security-whitepaper/page-2/",
                      "https://ai.google/learn-ai-skills/"
                    ],
                    "justification": "Google has comprehensive security training for all employees including onboarding programs and ongoing security education, but no specific public information was found about training explicitly focused on AI model weights security as outlined in the RAND report's Security Level 1 requirements."
                  },
                  "xAI": {
                    "score": 0,
                    "sources": [],
                    "justification": "No specific public information found regarding xAI's implementation of basic onboarding information security training for employees related to AI model weights security."
                  },
                  "Meta": {
                    "score": 0,
                    "sources": [],
                    "justification": "No specific public information found addressing Meta's basic onboarding information security training for employees related to AI model weights security."
                  }
                }
              }
            ]
          }
        ]
      },
      {
        "name": "Security Assurance and Testing",
        "subcategories": [
          {
            "name": "Risk and Security Assessments",
            "controls": [
              {
                "name": "Internal reviews",
                "compliance": {
                  "OpenAI": {
                    "score": 0,
                    "sources": [],
                    "justification": "No specific public information found regarding OpenAI's implementation of 'internal reviews' as a security control for AI model weights as described in RAND's Security Level 1."
                  },
                  "Anthropic": {
                    "score": 75,
                    "sources": [
                      "https://www.anthropic.com/news/reflections-on-our-responsible-scaling-policy",
                      "https://www.anthropic.com/news/activating-asl3-protections",
                      "https://www.anthropic.com/rsp-updates"
                    ],
                    "justification": "Anthropic has implemented multi-party authorization for model weight access, mandatory code review on production code, and requires hardware authentication, justification, and employee approval for access. The company also established an Executive Risk Council for oversight and conducts routine safeguard assessments."
                  },
                  "Google": {
                    "score": 50,
                    "sources": [
                      "https://deepmind.google/about/responsibility-safety/",
                      "https://ai.google/responsibility/safety/",
                      "https://deepmind.google/discover/blog/updating-the-frontier-safety-framework/"
                    ],
                    "justification": "Google has established internal review processes through its Responsibility and Safety Council (RSC) and AGI Safety Council that evaluate AI research and models, and has published security guidance, but lacks specific public documentation detailing internal review procedures focused on model weights security as outlined in the RAND report."
                  },
                  "xAI": {
                    "score": 0,
                    "sources": [],
                    "justification": "No specific public information found about xAI's internal review practices for AI model weights security. While xAI has published general security measures and signed safety commitments, there is no publicly available documentation addressing internal reviews specifically related to model weights protection."
                  },
                  "Meta": {
                    "score": 0,
                    "sources": [],
                    "justification": "No specific public information found regarding Meta's internal review processes for AI model weights security as defined in the RAND report's Security Level 1 requirements."
                  }
                }
              }
            ]
          },
          {
            "name": "Security Team Capacity",
            "controls": [
              {
                "name": "Basic incident response capabilities",
                "compliance": {
                  "OpenAI": {
                    "score": 50,
                    "sources": [
                      "https://openai.com/security/",
                      "https://www.rand.org/pubs/research_briefs/RBA2849-1.html",
                      "https://openai.com/global-affairs/our-approach-to-frontier-risk",
                      "https://trust.openai.com/"
                    ],
                    "justification": "OpenAI demonstrates some incident response capabilities through their Trust Portal incident response plan, SOC 2 Type 2 compliance, bug bounty program, and documented security incident handling procedures. However, public information lacks specific details about incident response capabilities focused on AI model weights security as outlined in the RAND framework."
                  },
                  "Anthropic": {
                    "score": 75,
                    "sources": [
                      "https://www.anthropic.com/news/activating-asl3-protections",
                      "https://www.anthropic.com/rsp-updates"
                    ],
                    "justification": "Anthropic has implemented comprehensive incident response capabilities including centralized log management via SIEM/SOAR, automated detection and response workflows, casebook workflow for security analysts, access monitoring for model weights with automated detections, and enhanced detection capabilities through egress bandwidth controls and deception technology with honeypots."
                  },
                  "Google": {
                    "score": 0,
                    "sources": [],
                    "justification": "No specific public information found about Google's incident response capabilities for AI model weights security as defined in RAND's Security Level 1."
                  },
                  "xAI": {
                    "score": 25,
                    "sources": [
                      "https://x.ai/security",
                      "https://ubos.tech/news/xais-grok-incident-highlights-the-importance-of-ai-security-and-content-moderation/"
                    ],
                    "justification": "xAI has published formal incident management framework, 24/7 monitoring, and vulnerability disclosure program, but no specific public documentation found addressing model weights security incident response."
                  },
                  "Meta": {
                    "score": 50,
                    "sources": [
                      "https://www.tryparity.com/blog/how-meta-uses-llms-to-improve-incident-response",
                      "https://rootly.com/blog/how-meta-and-google-use-ai-to-improve-incident-response",
                      "https://www.securityweek.com/meta-releases-llama-ai-open-source-protection-tools/",
                      "https://www.artificialintelligence-news.com/news/meta-beefs-up-ai-security-new-llama-tools/",
                      "https://www.infosecurity-magazine.com/news/meta-new-advances-ai-security/"
                    ],
                    "justification": "Meta demonstrates incident response capabilities for AI systems with 42% accuracy in root cause analysis using LLMs, and has released security tools like Llama Guard 4 and LlamaFirewall. However, no specific public information was found detailing incident response procedures for AI model weights security breaches."
                  }
                }
              }
            ]
          },
          {
            "name": "Maintenance",
            "controls": [
              {
                "name": "Information security news monitoring and implementation",
                "compliance": {
                  "OpenAI": {
                    "score": 25,
                    "sources": [
                      "https://www.analyticsvidhya.com/blog/2024/05/openai-security-measures/",
                      "https://www.rand.org/pubs/research_briefs/RBA2849-1.html",
                      "https://www.fierce-network.com/cloud/model-weights-are-heart-ais-intelligence-and-its-achilles-heel"
                    ],
                    "justification": "OpenAI has publicly acknowledged model weight security as critical and proposed comprehensive security measures, but no specific public evidence was found demonstrating implementation of information security news monitoring systems specifically for AI model weights as outlined in RAND's Security Level 1."
                  },
                  "Anthropic": {
                    "score": 75,
                    "sources": [
                      "https://www.anthropic.com/news/activating-asl3-protections",
                      "https://www.anthropic.com/rsp-updates",
                      "https://www.anthropic.com/transparency/voluntary-commitments"
                    ],
                    "justification": "Anthropic demonstrates strong security monitoring through threat intelligence partnerships, bug bounty programs, regular threat modeling considering nation-state actors, and rapid response processes for sharing threat intelligence with partners. They've implemented ASL-3 security controls with over 100 security measures specifically for model weight protection."
                  },
                  "Google": {
                    "score": 50,
                    "sources": [
                      "https://cloud.google.com/blog/products/identity-security/introducing-ai-protection-security-for-the-ai-era",
                      "https://safety.google/cybersecurity-advancements/saif/",
                      "https://cloud.google.com/vertex-ai/docs/model-monitoring/overview",
                      "https://blog.google/technology/safety-security/introducing-googles-secure-ai-framework/"
                    ],
                    "justification": "Google has demonstrated security monitoring capabilities through Vertex AI Model Monitoring and their Secure AI Framework (SAIF), but no specific public evidence shows implementation of continuous security news monitoring specifically for AI model weights threats as described in RAND's Security Level 1 requirements."
                  },
                  "xAI": {
                    "score": 0,
                    "sources": [],
                    "justification": "No specific public information found regarding xAI's implementation of information security news monitoring and implementation related to AI model weights security as described in the RAND report."
                  },
                  "Meta": {
                    "score": 50,
                    "sources": [
                      "https://www.securityweek.com/meta-releases-llama-ai-open-source-protection-tools/",
                      "https://www.infosecurity-magazine.com/news/meta-new-advances-ai-security/",
                      "https://thehackernews.com/2025/01/metas-llama-framework-flaw-exposes-ai.html",
                      "https://www.unite.ai/from-jailbreaks-to-injections-how-meta-is-strengthening-ai-security-with-llama-firewall/"
                    ],
                    "justification": "Meta has implemented some security monitoring tools for AI (Llama Guard, LlamaFirewall, Prompt Guard) and released security updates, but lacks specific public documentation about comprehensive information security news monitoring systems for AI model weights threats as outlined in the RAND report's Security Level 1 requirements."
                  }
                }
              }
            ]
          }
        ]
      }
    ]
  },
  {
    "level": 2,
    "description": "A system that can likely thwart most professional opportunistic efforts by attackers that execute moderate-effort or nontargeted attacks (OC2). This includes the operations of many professional individual hackers, as well as capable hacker groups when executing untargeted or lower-priority attacks.",
    "categories": [
      {
        "name": "Implementation of Previous Security Levels",
        "subcategories": [
          {
            "name": "",
            "controls": [
              {
                "name": "The organization has implemented all the controls from SL1.",
                "compliance": {
                  "OpenAI": {
                    "score": 50,
                    "sources": [
                      "https://openai.com/enterprise-privacy",
                      "https://openai.com/security/",
                      "https://www.rand.org/pubs/research_briefs/RBA2849-1.html",
                      "https://openai.com/global-affairs/our-approach-to-frontier-risk"
                    ],
                    "justification": "OpenAI demonstrates implementation of basic security controls including encryption, access controls, SOC 2 compliance, and 24/7 security operations, but lacks public documentation specifically addressing all SL1 controls from the RAND framework."
                  },
                  "Anthropic": {
                    "score": 50,
                    "sources": [
                      "https://www.rand.org/pubs/research_reports/RRA2849-1.html",
                      "https://www.anthropic.com/news/activating-asl3-protections",
                      "https://www.rand.org/news/press/2024/05/30.html"
                    ],
                    "justification": "Anthropic has implemented ASL-2 security measures which include 'defenses against opportunistic attempts to steal the weights' and recently upgraded to ASL-3 with over 100 security controls. However, the RAND report indicates that Security Level 1 measures are not widely implemented across the industry, and Anthropic's ASL-2 baseline appears to align more with Security Level 2 (protecting against opportunistic attacks) rather than fully implementing all SL1 controls."
                  },
                  "Google": {
                    "score": 0,
                    "sources": [],
                    "justification": "No specific public information found regarding Google's implementation of RAND's Security Level 1 controls for AI model weights security. While Google discusses security measures for Gemini models, there is no evidence they have adopted or implemented the specific SL1 controls framework from the RAND report."
                  },
                  "xAI": {
                    "score": 0,
                    "sources": [
                      "https://techcrunch.com/2025/05/13/xais-promised-safety-report-is-mia/",
                      "https://www.rand.org/pubs/research_reports/RRA2849-1.html",
                      "https://x.ai/security"
                    ],
                    "justification": "No specific public information found detailing xAI's implementation of RAND SL1 controls for AI model weights security. While xAI has published general security information, there is no evidence of compliance with the specific controls required by SL1."
                  },
                  "Meta": {
                    "score": 25,
                    "sources": [
                      "https://thehackernews.com/2025/01/metas-llama-framework-flaw-exposes-ai.html",
                      "https://www.oligo.security/blog/cve-2024-50050-critical-vulnerability-in-meta-llama-llama-stack",
                      "https://www.edgeless.systems/solutions/ai-model-protection",
                      "https://www.nextplatform.com/2024/07/25/meta-lets-its-largest-llama-ai-model-loose-into-the-open-field/",
                      "https://www.infosecurity-magazine.com/news/meta-new-advances-ai-security/"
                    ],
                    "justification": "While Meta has implemented some security controls like Llama Guard and LlamaFirewall, public evidence of a model weights leak and recent security vulnerabilities indicate incomplete implementation of SL1's comprehensive security measures, particularly around weight centralization and access control."
                  }
                }
              }
            ]
          }
        ]
      },
      {
        "name": "Weight Security",
        "subcategories": [
          {
            "name": "Weight Storage",
            "controls": [
              {
                "name": "Storage location (e.g., weights are stored exclusively on servers and not on local devices)",
                "compliance": {
                  "OpenAI": {
                    "score": 100,
                    "sources": [
                      "https://openai.com/enterprise-privacy",
                      "https://openai.com/index/openai-api/",
                      "https://web.swipeinsight.app/posts/openai-unveils-security-architecture-for-frontier-ai-model-training-7065"
                    ],
                    "justification": "OpenAI stores model weights exclusively on servers through their API-based approach, with multi-layered security controls including private-linked storage requiring authentication/authorization, and does not release model weights for local device storage."
                  },
                  "Anthropic": {
                    "score": 50,
                    "sources": [
                      "https://privacy.anthropic.com/en/articles/7996890-where-are-your-servers-located-do-you-host-your-models-on-eu-servers",
                      "https://www.anthropic.com/news/activating-asl3-protections",
                      "https://venturebeat.com/ai/why-anthropic-and-openai-are-obsessed-with-securing-llm-model-weights/"
                    ],
                    "justification": "Anthropic implements egress bandwidth controls for secure computing environments where model weights reside and uses cloud providers for data processing, suggesting server-based storage, but no explicit public statement confirms weights are stored exclusively on servers and never on local devices."
                  },
                  "Google": {
                    "score": 25,
                    "sources": [
                      "https://cloud.google.com/architecture/ai-ml/storage-for-ai-ml",
                      "https://cloud.google.com/blog/products/application-development/new-localllm-lets-you-develop-gen-ai-apps-locally-without-gpus",
                      "https://developer.chrome.com/docs/ai/built-in",
                      "https://research.google/blog/unlocking-7b-language-models-in-your-browser-a-deep-dive-with-google-ai-edges-mediapipe/",
                      "https://techcrunch.com/2025/05/31/google-quietly-released-an-app-that-lets-you-download-and-run-ai-models-locally/"
                    ],
                    "justification": "Google has released apps allowing local model execution (AI Edge Gallery) and promotes on-device AI capabilities, indicating weights are not stored exclusively on servers. However, Google Cloud's infrastructure emphasizes secure server-based storage for enterprise AI workloads."
                  },
                  "xAI": {
                    "score": 0,
                    "sources": [
                      "https://www.oracle.com/artificial-intelligence/ai-open-weights-models/",
                      "https://x.ai/news/grok-os",
                      "https://blocksandfiles.com/2024/11/19/ddn-supplying-storage-for-xais-grok-3-colossus-ai-supercomputer/",
                      "https://github.com/xai-org/grok-1/discussions/246"
                    ],
                    "justification": "No specific public information found regarding xAI's policies on exclusive server storage vs. local device storage of model weights. While xAI operates the Colossus supercomputer for training and has open-sourced Grok-1 weights, their security practices for weight storage location are not publicly documented."
                  },
                  "Meta": {
                    "score": 0,
                    "sources": [],
                    "justification": "No specific public information found directly addressing whether Meta stores AI model weights exclusively on servers versus local devices."
                  }
                }
              },
              {
                "name": "Encryption (e.g., all keys are secured in a key management system)",
                "compliance": {
                  "OpenAI": {
                    "score": 50,
                    "sources": [
                      "https://learn.microsoft.com/en-us/azure/ai-services/openai/encrypt-data-at-rest",
                      "https://openai.com/enterprise-privacy",
                      "https://siliconangle.com/2024/04/23/openai-enhances-security-control-cost-management-enterprise-api-users/",
                      "https://web.swipeinsight.app/posts/openai-unveils-security-architecture-for-frontier-ai-model-training-7065"
                    ],
                    "justification": "OpenAI implements AES-256 encryption for data at rest and TLS 1.2+ for data in transit, and uses key management services for sensitive information. However, no specific public information confirms that all model weight encryption keys are secured in a dedicated key management system as required for Security Level 2."
                  },
                  "Anthropic": {
                    "score": 75,
                    "sources": [
                      "https://privacy.anthropic.com/en/articles/10458704-how-does-anthropic-protect-the-personal-data-of-claude-ai-users",
                      "https://www.anthropic.com/news/activating-asl3-protections",
                      "https://www.anthropic.com/rsp-updates",
                      "https://www.anthropic.com/research/confidential-inference-trusted-vms"
                    ],
                    "justification": "Anthropic has implemented ASL-3 security standards including two-party authorization for model weight access, encryption of data in transit and at rest, and preliminary egress bandwidth controls specifically designed to protect model weights. However, no explicit mention of a dedicated key management system (KMS) for model weight encryption keys was found."
                  },
                  "Google": {
                    "score": 50,
                    "sources": [
                      "https://cloud.google.com/kms/docs",
                      "https://cloud.google.com/vertex-ai/docs/general/cmek",
                      "https://deepmind.google/discover/blog/taking-a-responsible-path-to-agi/"
                    ],
                    "justification": "Google demonstrates strong general encryption practices with Cloud KMS supporting CMEK for various AI services including Vertex AI, but lacks specific public documentation explicitly addressing encryption and key management for AI model weights as a distinct security concern."
                  },
                  "xAI": {
                    "score": 0,
                    "sources": [],
                    "justification": "No specific public information found regarding xAI's encryption practices or key management systems for AI model weights security."
                  },
                  "Meta": {
                    "score": 0,
                    "sources": [],
                    "justification": "No specific public information found regarding Meta's encryption practices or key management systems for AI model weights security."
                  }
                }
              }
            ]
          },
          {
            "name": "Security During Transport and Use",
            "controls": [
              {
                "name": "Encryption in transit (e.g., not transporting weights over public or unencrypted channels)",
                "compliance": {
                  "OpenAI": {
                    "score": 50,
                    "sources": [
                      "https://web.swipeinsight.app/posts/openai-unveils-security-architecture-for-frontier-ai-model-training-7065",
                      "https://openai.com/enterprise-privacy",
                      "https://www.rand.org/pubs/research_briefs/RBA2849-1.html"
                    ],
                    "justification": "OpenAI states they encrypt all data in transit using TLS 1.2+, but there is no specific public information about security measures for model weights during transport, which the RAND report identifies as a critical security gap."
                  },
                  "Anthropic": {
                    "score": 50,
                    "sources": [
                      "https://privacy.anthropic.com/en/articles/10458704-how-does-anthropic-protect-the-personal-data-of-claude-ai-users",
                      "https://www.anthropic.com/news/activating-asl3-protections",
                      "https://www.anthropic.com/rsp-updates",
                      "https://www.anthropic.com/research/confidential-inference-trusted-vms"
                    ],
                    "justification": "Anthropic has implemented ASL-3 security standards with over 100 security controls and mentions protecting model weights through encryption and access controls, but no specific public information confirms encryption during transport of weights."
                  },
                  "Google": {
                    "score": 75,
                    "sources": [
                      "https://cloud.google.com/docs/security/encryption-in-transit/application-layer-transport-security",
                      "https://cloud.google.com/docs/security/encryption-in-transit",
                      "https://deepmind.google/discover/blog/updating-the-frontier-safety-framework/"
                    ],
                    "justification": "Google implements comprehensive encryption in transit by default across its infrastructure using TLS, ALTS, and PSP protocols. While Google DeepMind's security frameworks emphasize weight protection and mention security mitigations to prevent exfiltration, specific public documentation on encryption during model weight transport is limited."
                  },
                  "xAI": {
                    "score": 25,
                    "sources": [
                      "https://x.ai/news/grok-os",
                      "https://x.ai/security"
                    ],
                    "justification": "xAI's security page mentions TLS encryption for web application and API communications, but does not specifically address encryption protocols for model weight transport. They publicly released Grok-1 weights, suggesting limited focus on weight security controls."
                  },
                  "Meta": {
                    "score": 25,
                    "sources": [
                      "https://engineering.fb.com/2019/05/29/security/service-encryption/",
                      "https://github.com/meta-llama/llama-models",
                      "https://en.wikipedia.org/wiki/Llama_(language_model)",
                      "https://www.rand.org/pubs/research_briefs/RBA2849-1.html",
                      "https://engineering.fb.com/2025/04/29/security/whatsapp-private-processing-ai-tools/"
                    ],
                    "justification": "While Meta demonstrates strong encryption capabilities for general infrastructure and specific AI applications like WhatsApp Private Processing, there is no specific public information about encryption requirements for transporting AI model weights internally or during distribution, with evidence of unencrypted distribution methods being used."
                  }
                }
              }
            ]
          },
          {
            "name": "Physical Security",
            "controls": [
              {
                "name": "Data centers are guarded, and only people with authorization are allowed inside.",
                "compliance": {
                  "OpenAI": {
                    "score": 75,
                    "sources": [
                      "https://trust.openai.com/",
                      "https://openai.com/enterprise-privacy",
                      "https://openai.com/policies/data-processing-addendum"
                    ],
                    "justification": "OpenAI's Data Processing Addendum explicitly states they maintain physical access controls including locked doors/gates, 24-hour video surveillance, biometric/photo-ID badge access systems, and visitor identification/escort protocols for all OpenAI facilities."
                  },
                  "Anthropic": {
                    "score": 0,
                    "sources": [],
                    "justification": "No specific public information found directly addressing whether Anthropic's data centers are guarded with authorization-only access controls."
                  },
                  "Google": {
                    "score": 100,
                    "sources": [
                      "https://blog.google/inside-google/infrastructure/how-data-center-security-works/",
                      "https://cloud.google.com/docs/security/infrastructure/design",
                      "https://datacenters.google/advancing-security/",
                      "https://cloud.google.com/docs/security/overview/whitepaper",
                      "https://workspace.google.com/learn-more/security/security-whitepaper/page-4/"
                    ],
                    "justification": "Google implements comprehensive physical security with multiple layers including 24/7 guards, biometric authentication, electronic access cards, and strict authorization controls. Their data centers use the 'least privilege' protocol where only authorized personnel can access specific areas, with less than 1% of Google employees ever accessing data centers."
                  },
                  "xAI": {
                    "score": 25,
                    "sources": [
                      "https://x.ai/security"
                    ],
                    "justification": "xAI states on its security page that 'Physical access to the data centers is restricted to only those requiring access to complete their job functions' and 'All data center staff undergo comprehensive background checks and security training.' However, no specific details about guards, security checkpoints, or access control implementation are publicly available."
                  },
                  "Meta": {
                    "score": 50,
                    "sources": [
                      "https://www.csoonline.com/article/565522/how-facebook-protects-data-with-physical-security.html",
                      "https://www.facebook.com/legal/terms/data_security_terms"
                    ],
                    "justification": "Meta's Data Security Terms confirm that 'physical access to Meta data centers is limited to authorized persons' with established controls. A 2018 CSO article describes Facebook's data centers having 'secured spaces' with access control points and guard monitoring, though this information predates recent AI developments."
                  }
                }
              },
              {
                "name": "Visitor access is restricted and logged.",
                "compliance": {
                  "OpenAI": {
                    "score": 25,
                    "sources": [
                      "https://venturebeat.com/ai/why-anthropic-and-openai-are-obsessed-with-securing-llm-model-weights/",
                      "https://openai.com/enterprise-privacy",
                      "https://openai.com/policies/data-processing-addendum"
                    ],
                    "justification": "OpenAI's Data Processing Addendum mentions visitor identification, sign-in and escort protocols, and logging of facility exits and entries, but lacks specific details about visitor access restrictions and logging related to model weights security."
                  },
                  "Anthropic": {
                    "score": 0,
                    "sources": [],
                    "justification": "No specific public information found about Anthropic's physical visitor access restrictions and logging related to AI model weights security."
                  },
                  "Google": {
                    "score": 75,
                    "sources": [
                      "https://blog.google/inside-google/infrastructure/how-data-center-security-works/",
                      "https://eitca.org/cloud-computing/eitc-cl-gcp-google-cloud-platform/gcp-security/data-center-security-layers/examination-review-data-center-security-layers/how-is-building-access-controlled-in-a-google-data-center/",
                      "https://cloud.google.com/docs/security/overview/whitepaper",
                      "https://workspace.google.com/learn-more/security/security-whitepaper/page-4/"
                    ],
                    "justification": "Google demonstrates strong visitor access controls with <cite index=\"13-8,13-9,13-10,13-11\">strictly limited access to authorized personnel only, requiring rigorous identity verification including valid IDs and background checks</cite>. <cite index=\"14-1,17-1,17-12\">Access logs, activity records, and camera footage are available in case an incident occurs</cite>, and <cite index=\"13-1,13-2\">building access is tightly controlled through a multi-layered approach including strict identity verification, perimeter fencing, security checkpoints, access control systems, video surveillance, and on-site security personnel</cite>."
                  },
                  "xAI": {
                    "score": 0,
                    "sources": [],
                    "justification": "No specific public information found regarding xAI's implementation of visitor access restrictions and logging for physical facilities containing AI model weights."
                  },
                  "Meta": {
                    "score": 0,
                    "sources": [],
                    "justification": "No specific public information found regarding Meta's visitor access restrictions and logging practices for AI model weights security."
                  }
                }
              }
            ]
          },
          {
            "name": "Access Control",
            "controls": [
              {
                "name": "Restrictions on sensitive interactions (e.g., require multifactor authentication using FIDO authentication/hardware security keys)",
                "compliance": {
                  "OpenAI": {
                    "score": 25,
                    "sources": [
                      "https://web.swipeinsight.app/posts/openai-unveils-security-architecture-for-frontier-ai-model-training-7065",
                      "https://openai.com/enterprise-privacy",
                      "https://siliconangle.com/2024/04/23/openai-enhances-security-control-cost-management-enterprise-api-users/",
                      "https://help.openai.com/en/articles/7967234-enabling-multi-factor-authentication-mfa-with-openai"
                    ],
                    "justification": "OpenAI has implemented MFA for user accounts and uses Azure authentication with multi-party approvals for model weights access, but no public information confirms the specific use of FIDO/hardware security keys for accessing AI model weights as required by RAND Security Level 2."
                  },
                  "Anthropic": {
                    "score": 50,
                    "sources": [
                      "https://www.anthropic.com/news/frontier-model-security",
                      "https://www.anthropic.com/rsp-updates",
                      "https://www.anthropic.com/news/activating-asl3-protections"
                    ],
                    "justification": "Anthropic has implemented 'two-party authorization for model weight access' and requires 'hardware authentication device prompt' as part of their ASL-3 security measures, but there is no specific mention of FIDO authentication or hardware security keys being required for model weights access."
                  },
                  "Google": {
                    "score": 0,
                    "sources": [],
                    "justification": "No specific public information found regarding Google's implementation of multifactor authentication using FIDO/hardware security keys specifically for AI model weights security."
                  },
                  "xAI": {
                    "score": 0,
                    "sources": [],
                    "justification": "No specific public information found regarding xAI's implementation of multifactor authentication or FIDO hardware security keys for model weights security."
                  },
                  "Meta": {
                    "score": 0,
                    "sources": [],
                    "justification": "No specific public information found regarding Meta's implementation of multifactor authentication using FIDO authentication or hardware security keys for AI model weights security."
                  }
                }
              }
            ]
          },
          {
            "name": "Monitoring",
            "controls": [
              {
                "name": "Logging of all sensitive interactions",
                "compliance": {
                  "OpenAI": {
                    "score": 0,
                    "sources": [],
                    "justification": "No specific public information found about OpenAI's implementation of logging for sensitive interactions related to AI model weights security as described in the RAND report's Security Level 2 requirements."
                  },
                  "Anthropic": {
                    "score": 75,
                    "sources": [
                      "https://www.anthropic.com/news/frontier-model-security",
                      "https://www.anthropic.com/rsp-updates",
                      "https://www.anthropic.com/news/activating-asl3-protections"
                    ],
                    "justification": "Anthropic has implemented comprehensive monitoring systems as part of ASL-3 security standards, including multi-layered monitoring (real-time and asynchronous), detection across all major log sources for access to critical assets, and two-party authorization with explicit access validation for model weights."
                  },
                  "Google": {
                    "score": 0,
                    "sources": [],
                    "justification": "No specific public information found directly addressing Google's implementation of 'logging of all sensitive interactions' for AI model weights security as described in RAND's Security Level 2."
                  },
                  "xAI": {
                    "score": 25,
                    "sources": [
                      "https://www.rand.org/pubs/research_briefs/RBA2849-1.html",
                      "https://x.ai/security"
                    ],
                    "justification": "xAI demonstrates general logging and monitoring capabilities including AWS CloudTrail and audit trails, but no specific public information confirms logging of all sensitive interactions related to AI model weights as described in RAND's Security Level 2."
                  },
                  "Meta": {
                    "score": 25,
                    "sources": [
                      "https://engineering.fb.com/2025/04/29/security/whatsapp-private-processing-ai-tools/",
                      "https://engineering.fb.com/2024/03/18/data-infrastructure/logarithm-logging-engine-ai-training-workflows-services-meta/",
                      "https://www.rand.org/pubs/research_reports/RRA2849-1.html"
                    ],
                    "justification": "Meta has demonstrated some logging capabilities through their Logarithm system for AI training workflows and security features like access control, but no specific public documentation confirms comprehensive logging of all sensitive interactions related to AI model weights as required by RAND Security Level 2."
                  }
                }
              },
              {
                "name": "Regulation and monitoring of weight copies across the organization network",
                "compliance": {
                  "OpenAI": {
                    "score": 25,
                    "sources": [
                      "https://web.swipeinsight.app/posts/openai-unveils-security-architecture-for-frontier-ai-model-training-7065",
                      "https://openai.com/global-affairs/openai-s-comment-to-the-ntia-on-open-model-weights",
                      "https://openai.com/enterprise-privacy"
                    ],
                    "justification": "OpenAI has implemented some security measures for model weights including multi-party approvals, access controls, and network egress restrictions as described in their security architecture. However, public information lacks specific details about comprehensive monitoring of weight copies across their organization network as expected for Security Level 2."
                  },
                  "Anthropic": {
                    "score": 75,
                    "sources": [
                      "https://www.anthropic.com/rsp-updates",
                      "https://www.anthropic.com/news/activating-asl3-protections",
                      "https://www.anthropic.com/news/reflections-on-our-responsible-scaling-policy"
                    ],
                    "justification": "Anthropic has implemented substantial controls for monitoring and regulating model weight copies, including two-party authorization for access, enhanced change management protocols, deception technology with honeypots (including fake model weights), multi-party authorization with time-bounded access controls, and over 100 security controls targeting model weight protection."
                  },
                  "Google": {
                    "score": 50,
                    "sources": [
                      "https://safety.google/cybersecurity-advancements/saif/",
                      "https://deepmind.google/discover/blog/updating-the-frontier-safety-framework/",
                      "https://deepmind.google/discover/blog/taking-a-responsible-path-to-agi/"
                    ],
                    "justification": "Google has published frameworks acknowledging model weight security importance and recommends centralized access control and monitoring systems, but lacks specific public documentation detailing implementation of weight copy regulation and monitoring across their organization network."
                  },
                  "xAI": {
                    "score": 0,
                    "sources": [],
                    "justification": "No specific public information found regarding xAI's implementation of regulation and monitoring of weight copies across their organization network as described in RAND's Security Level 2 framework."
                  },
                  "Meta": {
                    "score": 0,
                    "sources": [],
                    "justification": "No specific public information found regarding Meta's implementation of internal monitoring and regulation systems for AI model weight copies across their organization network."
                  }
                }
              }
            ]
          }
        ]
      },
      {
        "name": "AI Model Resilience",
        "subcategories": [
          {
            "name": "Model Robustness",
            "controls": [
              {
                "name": "Input reconstruction (e.g., during inference, a privately known prefix is added ahead of the user prompt)",
                "compliance": {
                  "OpenAI": {
                    "score": 0,
                    "sources": [],
                    "justification": "No specific public information found regarding OpenAI's implementation of input reconstruction security controls during inference as described in RAND's Security Level 2."
                  },
                  "Anthropic": {
                    "score": 0,
                    "sources": [],
                    "justification": "No specific public information found regarding Anthropic's implementation of input reconstruction or privately known prefix addition during inference as a model weights security control."
                  },
                  "Google": {
                    "score": 0,
                    "sources": [],
                    "justification": "No specific public information found addressing Google's implementation of input reconstruction controls with privately known prefixes during inference."
                  },
                  "xAI": {
                    "score": 0,
                    "sources": [],
                    "justification": "No specific public information found regarding xAI's implementation of input reconstruction controls during inference or privately known prefix additions as described in RAND's Security Level 2 requirements."
                  },
                  "Meta": {
                    "score": 0,
                    "sources": [],
                    "justification": "No specific public information found about Meta's implementation of input reconstruction controls with privately known prefixes during inference as described for Security Level 2 in the RAND report."
                  }
                }
              },
              {
                "name": "Adversarial training",
                "compliance": {
                  "OpenAI": {
                    "score": 75,
                    "sources": [
                      "https://openai.com/index/trading-inference-time-compute-for-adversarial-robustness/",
                      "https://en.wikipedia.org/wiki/GPT-4",
                      "https://openai.com/index/gpt-4o-system-card/",
                      "https://openai.com/research/attacking-machine-learning-with-adversarial-examples",
                      "https://openai.com/research/gpt-4"
                    ],
                    "justification": "OpenAI demonstrates strong implementation of adversarial training through extensive red-teaming, iterative alignment using adversarial testing programs, and recent research on inference-time compute for adversarial robustness. They actively incorporate adversarial examples in model training and have developed multiple defense strategies."
                  },
                  "Anthropic": {
                    "score": 25,
                    "sources": [
                      "https://www.anthropic.com/research/sleeper-agents-training-deceptive-llms-that-persist-through-safety-training",
                      "https://aimagazine.com/machine-learning/anthropic-ai-models-can-be-trained-to-give-fake-information",
                      "https://www.anthropic.com/news/activating-asl3-protections"
                    ],
                    "justification": "Anthropic has published research showing adversarial training can make backdoored models more accurate at implementing deceptive behaviors rather than removing them. While they implement ASL-3 security measures for model weights protection, their research suggests skepticism about adversarial training's effectiveness for security."
                  },
                  "Google": {
                    "score": 50,
                    "sources": [
                      "https://deepmind.google/discover/blog/identifying-and-eliminating-bugs-in-learned-predictive-models/",
                      "https://cloud.google.com/blog/topics/threat-intelligence/adversarial-misuse-generative-ai",
                      "https://developers.google.com/machine-learning/guides/adv-testing"
                    ],
                    "justification": "Google demonstrates moderate implementation of adversarial training through documented adversarial testing practices for generative AI and DeepMind's development of threat models and evaluation techniques. However, no specific public information was found directly addressing adversarial training for protecting AI model weights as described in the RAND Security Level 2 requirements."
                  },
                  "xAI": {
                    "score": 25,
                    "sources": [
                      "https://x.ai/news/grok-3",
                      "https://x.ai/news/grok"
                    ],
                    "justification": "xAI has publicly acknowledged adversarial robustness as a priority and expressed interest in accelerating progress in this area during training, but no specific implementation details of adversarial training for model weights security have been publicly disclosed."
                  },
                  "Meta": {
                    "score": 0,
                    "sources": [],
                    "justification": "No specific public information found regarding Meta's implementation of adversarial training for AI model weights security."
                  }
                }
              }
            ]
          }
        ]
      },
      {
        "name": "Security of Network and Other (Nonweight) Sensitive Assets",
        "subcategories": [
          {
            "name": "Software",
            "controls": [
              {
                "name": "Frequent software update management and compliance monitoring",
                "compliance": {
                  "OpenAI": {
                    "score": 50,
                    "sources": [
                      "https://trust.openai.com/",
                      "https://openai.com/policies/supplier-security-measures",
                      "https://openai.com/global-affairs/our-approach-to-frontier-risk",
                      "https://openai.com/enterprise-privacy"
                    ],
                    "justification": "OpenAI demonstrates moderate compliance through SOC 2 Type 2 certification, documented security programs, and vulnerability management, but lacks specific public documentation about frequent software updates and compliance monitoring specifically for AI model weights security."
                  },
                  "Anthropic": {
                    "score": 75,
                    "sources": [
                      "https://www.anthropic.com/rsp-updates",
                      "https://www.anthropic.com/news/activating-asl3-protections",
                      "https://www.anthropic.com/news/announcing-our-updated-responsible-scaling-policy"
                    ],
                    "justification": "Anthropic demonstrates substantial implementation of software update management and compliance monitoring through their ASL-3 security controls, including software inventory management, supply chain security scanning, enhanced change management protocols, and routine safeguard assessments with documented compliance tracking."
                  },
                  "Google": {
                    "score": 50,
                    "sources": [
                      "https://cloud.google.com/blog/products/identity-security/introducing-ai-protection-security-for-the-ai-era",
                      "https://safety.google/cybersecurity-advancements/saif/",
                      "https://blog.google/technology/safety-security/introducing-googles-secure-ai-framework/"
                    ],
                    "justification": "Google demonstrates partial compliance through its Secure AI Framework (SAIF) which addresses continuous monitoring and adaptation of AI security controls, and AI Protection capabilities that provide ongoing threat detection and compliance monitoring. However, there is no specific public information about frequent software update management processes specifically for AI model weights security."
                  },
                  "xAI": {
                    "score": 0,
                    "sources": [],
                    "justification": "No specific public information found regarding xAI's implementation of frequent software update management and compliance monitoring for AI model weights security as outlined in RAND's Security Level 2."
                  },
                  "Meta": {
                    "score": 25,
                    "sources": [
                      "https://www.bankinfosecurity.com/meta-plans-to-restrict-high-risk-ai-models-a-27447",
                      "https://www.infosecurity-magazine.com/news/meta-new-advances-ai-security/",
                      "https://www.npr.org/2025/05/31/nx-s1-5407870/meta-ai-facebook-instagram-risks"
                    ],
                    "justification": "Meta shows limited public evidence of systematic software update management for AI model weights security. While they've introduced security tools like LlamaFirewall and Llama Guard 4, and have a Frontier AI Framework for risk assessment, there's no specific public information about frequent update management or compliance monitoring processes for model weights security."
                  }
                }
              }
            ]
          },
          {
            "name": "Access, Permissions, and Credentials",
            "controls": [
              {
                "name": "Strong password enforcement",
                "compliance": {
                  "OpenAI": {
                    "score": 50,
                    "sources": [
                      "https://web.swipeinsight.app/posts/openai-unveils-security-architecture-for-frontier-ai-model-training-7065",
                      "https://www.rand.org/pubs/research_briefs/RBA2849-1.html",
                      "https://openai.com/enterprise-privacy"
                    ],
                    "justification": "OpenAI implements enterprise authentication (SAML SSO) and Azure AD-based access controls for model weights, but specific password enforcement policies (complexity, rotation, MFA) are not publicly documented."
                  },
                  "Anthropic": {
                    "score": 75,
                    "sources": [
                      "https://www.anthropic.com/news/frontier-model-security",
                      "https://privacy.anthropic.com/en/articles/10458704-how-does-anthropic-protect-the-personal-data-of-claude-ai-users",
                      "https://www.anthropic.com/news/activating-asl3-protections",
                      "https://www.anthropic.com/news/reflections-on-our-responsible-scaling-policy"
                    ],
                    "justification": "Anthropic has implemented ASL-3 security standards including two-party authorization for model weight access and multi-party authorization with time-bounded access controls. They use strict password policies and multi-factor authentication for secure access, though specific password complexity requirements are not detailed."
                  },
                  "Google": {
                    "score": 0,
                    "sources": [],
                    "justification": "No specific public information found regarding Google's implementation of strong password enforcement for AI model weights security."
                  },
                  "xAI": {
                    "score": 25,
                    "sources": [
                      "https://x.ai/security",
                      "https://techcrunch.com/2025/05/13/xais-promised-safety-report-is-mia/",
                      "https://www.rand.org/pubs/research_reports/RRA2849-1.html"
                    ],
                    "justification": "xAI states adherence to NIST SP 800-63B for password security policies on their security page, demonstrating some level of password enforcement implementation. However, no specific public information was found about password controls specifically for AI model weights access or implementation of Security Level 2 requirements from the RAND report."
                  },
                  "Meta": {
                    "score": 0,
                    "sources": [
                      "https://github.com/meta-llama/llama",
                      "https://github.com/meta-llama/llama-models",
                      "https://github.com/meta-llama/llama3"
                    ],
                    "justification": "No specific public information found about Meta's implementation of strong password enforcement for AI model weights access control, though Meta uses signed URLs with time limits for model downloads."
                  }
                }
              },
              {
                "name": "The work network is separate from the guest network.",
                "compliance": {
                  "OpenAI": {
                    "score": 0,
                    "sources": [],
                    "justification": "No specific public information found addressing OpenAI's implementation of work network separation from guest networks for AI model weights security."
                  },
                  "Anthropic": {
                    "score": 0,
                    "sources": [],
                    "justification": "No specific public information found addressing network segmentation between work and guest networks for AI model weights security at Anthropic."
                  },
                  "Google": {
                    "score": 0,
                    "sources": [],
                    "justification": "No specific public information found regarding Google's implementation of work/guest network separation for AI model weights security."
                  },
                  "xAI": {
                    "score": 0,
                    "sources": [],
                    "justification": "No specific public information found regarding xAI's implementation of work and guest network separation for AI model weights security."
                  },
                  "Meta": {
                    "score": 0,
                    "sources": [],
                    "justification": "No specific public information found addressing Meta's implementation of work/guest network separation for AI model weights security as per RAND Security Level 2."
                  }
                }
              },
              {
                "name": "Guest accounts disabled whenever possible",
                "compliance": {
                  "OpenAI": {
                    "score": 0,
                    "sources": [],
                    "justification": "No specific public information found about OpenAI's policies or practices regarding disabling guest accounts in the context of AI model weights security as described in the RAND report."
                  },
                  "Anthropic": {
                    "score": 0,
                    "sources": [],
                    "justification": "No specific public information found addressing whether Anthropic disables guest accounts as part of their AI model weights security controls."
                  },
                  "Google": {
                    "score": 0,
                    "sources": [],
                    "justification": "No specific public information found."
                  },
                  "xAI": {
                    "score": 0,
                    "sources": [],
                    "justification": "No specific public information found regarding xAI's implementation of 'guest accounts disabled whenever possible' as related to AI model weights security or Security Level 2 requirements from the RAND report."
                  },
                  "Meta": {
                    "score": 0,
                    "sources": [],
                    "justification": "No specific public information found regarding Meta's policies on disabling guest accounts for AI model weights security."
                  }
                }
              },
              {
                "name": "Strong access management tools",
                "compliance": {
                  "OpenAI": {
                    "score": 75,
                    "sources": [
                      "https://web.swipeinsight.app/posts/openai-unveils-security-architecture-for-frontier-ai-model-training-7065",
                      "https://transparency.oecd.ai/reports/b167db92-67c8-47d8-966a-427e2ce8c008",
                      "https://www.helpfulgpts.com/openai-research-infrastructure-security/",
                      "https://openai.com/policies/supplier-security-measures"
                    ],
                    "justification": "OpenAI demonstrates strong access management controls including multi-party approvals for model weight access, role-based access control (RBAC), Azure Entra ID authentication, and a dedicated AccessManager Service requiring least-privilege authorization. However, public documentation doesn't provide full details on all Security Level 2 specific requirements."
                  },
                  "Anthropic": {
                    "score": 75,
                    "sources": [
                      "https://www.anthropic.com/rsp-updates",
                      "https://www.anthropic.com/news/activating-asl3-protections",
                      "https://www.anthropic.com/news/reflections-on-our-responsible-scaling-policy"
                    ],
                    "justification": "Anthropic has implemented multi-party authorization, mandatory code review, time-bounded access controls, hardware authentication requirements, and role-based permissions for model weight access as part of their ASL-3 security standards, demonstrating strong access management controls."
                  },
                  "Google": {
                    "score": 50,
                    "sources": [
                      "https://blog.google/technology/safety-security/introducing-googles-secure-ai-framework/",
                      "https://cloud.google.com/use-cases/secure-ai-framework",
                      "https://www.rand.org/pubs/research_reports/RRA2849-1.html"
                    ],
                    "justification": "Google demonstrates partial compliance through its Secure AI Framework (SAIF) which includes access controls, IAM controls, and Privileged Access Management for AI systems. However, there is no specific public information about implementing RAND's Security Level 2 requirements such as centralizing model weights or reducing authorized personnel."
                  },
                  "xAI": {
                    "score": 25,
                    "sources": [
                      "https://x.ai/security",
                      "https://github.com/xai-org/grok-1/discussions/246",
                      "https://en.wikipedia.org/wiki/Grok_(chatbot)"
                    ],
                    "justification": "xAI demonstrates basic access management (role-based access, SSO support, security logs) but released Grok-1 model weights publicly under Apache 2.0 license, contradicting strong weight security practices. No evidence of specialized controls for restricting model weight access internally."
                  },
                  "Meta": {
                    "score": 50,
                    "sources": [
                      "https://en.wikipedia.org/wiki/Llama_(language_model)",
                      "https://www.infosecurity-magazine.com/news/meta-new-advances-ai-security/",
                      "https://thehackernews.com/2025/04/meta-launches-llamafirewall-framework.html",
                      "https://huggingface.co/meta-llama/Meta-Llama-3-8B-Instruct"
                    ],
                    "justification": "Meta demonstrates partial compliance through tools like LlamaFirewall for runtime security and access control, but their open-weight distribution model fundamentally conflicts with strong access management as weights are freely downloadable after initial approval."
                  }
                }
              },
              {
                "name": "Zero Trust architecture (adherence to at least the standards in the \"Initial\" level of CISA's Zero Trust Maturity Model)",
                "compliance": {
                  "OpenAI": {
                    "score": 0,
                    "sources": [],
                    "justification": "No specific public information found directly addressing OpenAI's implementation of Zero Trust architecture principles or compliance with CISA's Zero Trust Maturity Model for AI model weights security."
                  },
                  "Anthropic": {
                    "score": 25,
                    "sources": [
                      "https://www.anthropic.com/news/frontier-model-security",
                      "https://trust.anthropic.com/",
                      "https://cloudsecurityalliance.org/blog/2025/03/18/from-risk-to-revenue-with-zero-trust-ai"
                    ],
                    "justification": "While Anthropic mentions security measures including multi-party authorization and references frontier model security practices, there is no specific public documentation confirming implementation of Zero Trust architecture adhering to CISA's Zero Trust Maturity Model standards for AI model weights security."
                  },
                  "Google": {
                    "score": 50,
                    "sources": [
                      "https://cloud.google.com/blog/products/identity-security/introducing-ai-protection-security-for-the-ai-era",
                      "https://safety.google/cybersecurity-advancements/saif/",
                      "https://cloud.google.com/blog/topics/public-sector/strengthening-federal-cybersecurity-cisa-zero-trust-and-google-workspace-exclusive-sessions-at-next-24/",
                      "https://cloud.google.com/use-cases/secure-ai-framework",
                      "https://cloud.google.com/beyondcorp",
                      "https://workspace.google.com/blog/identity-and-security/accelerating-zero-trust-and-digital-sovereignty-ai",
                      "https://cloud.google.com/security/securing-ai"
                    ],
                    "justification": "Google demonstrates strong Zero Trust foundations through BeyondCorp and meets CISA's Zero Trust Maturity Model requirements in Google Workspace, but lacks specific public documentation about Zero Trust controls for AI model weights security as defined in the RAND report's Security Level 2."
                  },
                  "xAI": {
                    "score": 0,
                    "sources": [],
                    "justification": "No specific public information found about xAI implementing Zero Trust architecture or adhering to CISA's Zero Trust Maturity Model standards for AI model weights security."
                  },
                  "Meta": {
                    "score": 0,
                    "sources": [],
                    "justification": "No specific public information found regarding Meta's compliance with Zero Trust architecture standards at CISA's 'Initial' level for AI model weights security."
                  }
                }
              }
            ]
          },
          {
            "name": "Hardware",
            "controls": [
              {
                "name": "Lost or stolen devices reported",
                "compliance": {
                  "OpenAI": {
                    "score": 0,
                    "sources": [],
                    "justification": "No specific public information found about OpenAI's policies or procedures for reporting lost or stolen devices in relation to AI model weights security."
                  },
                  "Anthropic": {
                    "score": 0,
                    "sources": [],
                    "justification": "No specific public information found regarding Anthropic's policies or procedures for reporting lost or stolen devices related to AI model weights security."
                  },
                  "Google": {
                    "score": 0,
                    "sources": [],
                    "justification": "No specific public information found about Google's implementation of lost or stolen device reporting controls for AI model weights security as described in RAND's Security Level 2."
                  },
                  "xAI": {
                    "score": 0,
                    "sources": [],
                    "justification": "No specific public information found about xAI's implementation of lost or stolen device reporting procedures related to AI model weights security as expected for RAND Security Level 2."
                  },
                  "Meta": {
                    "score": 0,
                    "sources": [],
                    "justification": "No specific public information found regarding Meta's implementation of lost or stolen device reporting procedures for AI model weights security."
                  }
                }
              },
              {
                "name": "All network devices are visible and trackable.",
                "compliance": {
                  "OpenAI": {
                    "score": 0,
                    "sources": [],
                    "justification": "No specific public information found addressing whether OpenAI implements visibility and tracking for all network devices as required for Security Level 2 in the RAND report on Securing AI Model Weights."
                  },
                  "Anthropic": {
                    "score": 50,
                    "sources": [
                      "https://www.anthropic.com/news/frontier-model-security",
                      "https://www.anthropic.com/rsp-updates",
                      "https://www.anthropic.com/news/activating-asl3-protections"
                    ],
                    "justification": "Anthropic has implemented ASL-3 security measures including over 100 security controls, centralized log management in SIEM/SOAR platforms, and infrastructure monitoring, which suggests partial implementation of network device visibility and tracking capabilities."
                  },
                  "Google": {
                    "score": 0,
                    "sources": [],
                    "justification": "No specific public information found addressing Google's network device visibility and tracking practices in relation to AI model weights security."
                  },
                  "xAI": {
                    "score": 0,
                    "sources": [],
                    "justification": "No specific public information found regarding xAI's implementation of network device visibility and tracking controls for AI model weights security."
                  },
                  "Meta": {
                    "score": 0,
                    "sources": [],
                    "justification": "No specific public information found directly addressing Meta's implementation of network device visibility and tracking as required by Security Level 2 of the RAND report for AI model weights security."
                  }
                }
              }
            ]
          },
          {
            "name": "Supply Chain",
            "controls": [
              {
                "name": "Review of vendor and supplier security",
                "compliance": {
                  "OpenAI": {
                    "score": 25,
                    "sources": [
                      "https://trust.openai.com/",
                      "https://openai.com/policies/supplier-security-measures",
                      "https://openai.com/security/"
                    ],
                    "justification": "OpenAI has documented supplier security measures and requires third-party vendors to undergo formal security assessments, but there is no specific public information about vendor security practices specifically related to AI model weights protection as described in RAND's Security Level 2."
                  },
                  "Anthropic": {
                    "score": 25,
                    "sources": [
                      "https://www.anthropic.com/news/frontier-model-security",
                      "https://www.techrepublic.com/article/news-anthropic-ai-safety-level-3/",
                      "https://www.anthropic.com/news/activating-asl3-protections"
                    ],
                    "justification": "Anthropic mentions bug bounty programs and collaboration with 'select third-party threat intelligence firms' for security evaluation, but lacks detailed public information about comprehensive vendor/supplier security reviews specifically for model weights protection as expected in Security Level 2."
                  },
                  "Google": {
                    "score": 50,
                    "sources": [
                      "https://support.google.com/corporate-suppliers/answer/14338208?hl=en",
                      "https://cloud.google.com/security/vendor-security-assessment",
                      "https://cloud.google.com/document-ai/docs/security",
                      "https://github.com/google/vsaq"
                    ],
                    "justification": "Google has comprehensive vendor security assessment processes including VSAQ framework and third-party risk management, but no specific public documentation directly addresses vendor security reviews for AI model weights protection as outlined in RAND's Security Level 2."
                  },
                  "xAI": {
                    "score": 25,
                    "sources": [
                      "https://x.ai/security",
                      "https://x.ai/legal/privacy-policy",
                      "https://xaivendors.com/"
                    ],
                    "justification": "xAI has basic security measures including access controls, encryption, and monitoring, but lacks publicly disclosed vendor/supplier security review processes specific to model weights protection as expected for Security Level 2."
                  },
                  "Meta": {
                    "score": 0,
                    "sources": [],
                    "justification": "No specific public information found regarding Meta's vendor and supplier security review practices related to AI model weights security as outlined in RAND's Security Level 2 requirements."
                  }
                }
              }
            ]
          },
          {
            "name": "Security Tooling",
            "controls": [
              {
                "name": "Disk encryption",
                "compliance": {
                  "OpenAI": {
                    "score": 25,
                    "sources": [
                      "https://web.swipeinsight.app/posts/openai-unveils-security-architecture-for-frontier-ai-model-training-7065",
                      "https://www.analyticsvidhya.com/blog/2024/05/openai-security-measures/",
                      "https://openai.com/enterprise-privacy"
                    ],
                    "justification": "OpenAI states they encrypt all data at rest using AES-256 and data in transit using TLS 1.2+, but lacks specific public documentation about disk encryption implementation for model weights storage as detailed in RAND's Security Level 2 requirements."
                  },
                  "Anthropic": {
                    "score": 75,
                    "sources": [
                      "https://www.anthropic.com/research/confidential-inference-trusted-vms",
                      "https://privacy.anthropic.com/en/articles/10458704-how-does-anthropic-protect-the-personal-data-of-claude-ai-users",
                      "https://www.anthropic.com/news/activating-asl3-protections"
                    ],
                    "justification": "Anthropic has implemented ASL-3 security measures including encryption for model weights (stored encrypted and decrypted only at the loader), enhanced internal security controls, and is actively developing confidential computing approaches with hardware-based trusted execution environments."
                  },
                  "Google": {
                    "score": 75,
                    "sources": [
                      "https://cloud.google.com/vertex-ai/docs/general/cmek",
                      "https://cloud.google.com/docs/security/encryption/default-encryption",
                      "https://cloud.google.com/vertex-ai/docs/general/vertexai-security-controls"
                    ],
                    "justification": "Google implements default AES-256 encryption at rest for all data including AI models in Vertex AI, and offers customer-managed encryption keys (CMEK) for additional control. However, no public information specifically addresses Security Level 2 requirements from the RAND report for model weights protection."
                  },
                  "xAI": {
                    "score": 25,
                    "sources": [
                      "https://x.ai/security",
                      "https://www.rand.org/pubs/research_reports/RRA2849-1.html"
                    ],
                    "justification": "xAI's security page mentions full disk encryption for company laptops and encryption for customer data in S3, but provides no specific public information about disk encryption for AI model weights storage, which is a critical requirement for Security Level 2."
                  },
                  "Meta": {
                    "score": 0,
                    "sources": [],
                    "justification": "No specific public information found regarding Meta's disk encryption practices for AI model weights security."
                  }
                }
              },
              {
                "name": "Network communications are encrypted by default.",
                "compliance": {
                  "OpenAI": {
                    "score": 50,
                    "sources": [
                      "https://openai.com/security/",
                      "https://www.analyticsvidhya.com/blog/2024/05/openai-security-measures/",
                      "https://openai.com/enterprise-privacy"
                    ],
                    "justification": "OpenAI publicly states they encrypt all data at rest (AES-256) and in transit (TLS 1.2+), which would cover network communications. However, there is no specific public information detailing how they implement network encryption specifically for AI model weights, which are acknowledged as critical intellectual property requiring special security measures."
                  },
                  "Anthropic": {
                    "score": 50,
                    "sources": [
                      "https://www.anthropic.com/news/frontier-model-security",
                      "https://docs.anthropic.com/en/docs/claude-code/data-usage",
                      "https://privacy.anthropic.com/en/articles/10458704-how-does-anthropic-protect-the-personal-data-of-claude-ai-users",
                      "https://www.anthropic.com/news/activating-asl3-protections"
                    ],
                    "justification": "Anthropic confirms encryption of user data in transit and at rest, but specific details about network encryption for model weights infrastructure are not publicly disclosed. While enhanced security measures for model weights are mentioned under ASL-3 protections, explicit confirmation of default network encryption for all model weight communications is absent."
                  },
                  "Google": {
                    "score": 100,
                    "sources": [
                      "https://cloud.google.com/blog/products/gcp/how-google-protects-your-data-in-transit",
                      "https://cloud.google.com/docs/security/encryption-in-transit/application-layer-transport-security",
                      "https://cloud.google.com/docs/security/encryption-in-transit"
                    ],
                    "justification": "Google demonstrates comprehensive network encryption by default through multiple mechanisms: <cite index=\"11-1,11-14,11-15,11-16\">all data sent to Google Front End (GFE) is encrypted with TLS/QUIC, and Google's infrastructure uses ALTS for authentication, integrity, and encryption of connections</cite>. <cite index=\"11-28,11-34\">Google Cloud encrypts customer data in transit within Google's networks and virtual network encrypts traffic between VMs</cite>. <cite index=\"13-1,13-6\">Google uses ALTS, a mutual authentication and transport encryption system at the application layer, to protect RPC communications</cite>."
                  },
                  "xAI": {
                    "score": 50,
                    "sources": [
                      "https://x.ai/security",
                      "https://www.rand.org/pubs/research_reports/RRA2849-1.html"
                    ],
                    "justification": "xAI publicly states that their web application and enterprise API use TLS encryption protocol for communication sessions, indicating partial compliance with network encryption requirements. However, no specific public information was found regarding encryption practices for AI model weights security specifically."
                  },
                  "Meta": {
                    "score": 0,
                    "sources": [],
                    "justification": "No specific public information found regarding Meta's implementation of network encryption by default for AI model weights. While Meta has committed to securing frontier model weights and implements end-to-end encryption for WhatsApp AI features, there is no documentation confirming network encryption by default for model weights transfers."
                  }
                }
              },
              {
                "name": "Email security tools",
                "compliance": {
                  "OpenAI": {
                    "score": 0,
                    "sources": [],
                    "justification": "No specific public information found about OpenAI's implementation of email security tools related to AI model weights security as described in Security Level 2 of the RAND report."
                  },
                  "Anthropic": {
                    "score": 0,
                    "sources": [],
                    "justification": "No specific public information found about Anthropic's implementation of email security tools for AI model weights security as described in RAND's Security Level 2."
                  },
                  "Google": {
                    "score": 0,
                    "sources": [],
                    "justification": "No specific public information found about Google's email security tools implementation for AI model weights security at RAND Security Level 2."
                  },
                  "xAI": {
                    "score": 0,
                    "sources": [
                      "https://x.ai/legal/faq",
                      "https://x.ai/legal/terms-of-service",
                      "https://x.ai/legal/privacy-policy"
                    ],
                    "justification": "No specific public information found regarding xAI's implementation of email security tools for protecting AI model weights. While xAI has privacy policies and general security measures, there is no publicly available documentation about email security controls specifically designed to protect model weights."
                  },
                  "Meta": {
                    "score": 0,
                    "sources": [],
                    "justification": "No specific public information found about Meta's email security tools implementation for AI model weights security as described for Security Level 2 in the RAND report."
                  }
                }
              },
              {
                "name": "Use of integrated security approaches, such as eXtended Detection and Response (XDR)",
                "compliance": {
                  "OpenAI": {
                    "score": 0,
                    "sources": [],
                    "justification": "No specific public information found regarding OpenAI's implementation of XDR or integrated security approaches for AI model weights security as described in RAND Security Level 2."
                  },
                  "Anthropic": {
                    "score": 0,
                    "sources": [
                      "https://www.anthropic.com/news/frontier-model-security",
                      "https://www.anthropic.com/news/activating-asl3-protections",
                      "https://venturebeat.com/ai/why-anthropic-and-openai-are-obsessed-with-securing-llm-model-weights/"
                    ],
                    "justification": "No specific public information found about Anthropic using Extended Detection and Response (XDR) systems for AI model weights security. While Anthropic has implemented over 100 security controls and egress bandwidth monitoring, there is no mention of XDR integration."
                  },
                  "Google": {
                    "score": 50,
                    "sources": [
                      "https://cloud.google.com/blog/products/identity-security/introducing-ai-protection-security-for-the-ai-era",
                      "https://safety.google/cybersecurity-advancements/saif/",
                      "https://cloud.google.com/security/resources/insights/what-xdr",
                      "https://www.cybereason.com/blog/introducing-cybereason-xdr-powered-by-google-chronicle",
                      "https://chronicle.security/",
                      "https://www.rand.org/pubs/research_reports/RRA2849-1.html",
                      "https://opensource.googleblog.com/2025/01/creating-safe-secure-ai-models.html"
                    ],
                    "justification": "Google demonstrates strong XDR capabilities through Chronicle Security Operations and partnerships (e.g., Cybereason XDR), but no specific public information confirms the application of XDR to AI model weights security. While Google has published guidance on model security and operates comprehensive security frameworks (SAIF), explicit integration of XDR for protecting model weights is not documented."
                  },
                  "xAI": {
                    "score": 0,
                    "sources": [],
                    "justification": "No specific public information found regarding xAI's implementation of XDR or integrated security approaches for AI model weights security."
                  },
                  "Meta": {
                    "score": 0,
                    "sources": [],
                    "justification": "No specific public information found about Meta's use of XDR or integrated security approaches for AI model weights security."
                  }
                }
              }
            ]
          },
          {
            "name": "Configuration Management",
            "controls": [
              {
                "name": "Incorporate fundamental infrastructure and policies for Security-by-Design and Security-by-Default",
                "compliance": {
                  "OpenAI": {
                    "score": 75,
                    "sources": [
                      "https://web.swipeinsight.app/posts/openai-unveils-security-architecture-for-frontier-ai-model-training-7065",
                      "https://openai.com/global-affairs/our-approach-to-frontier-risk",
                      "https://www.analyticsvidhya.com/blog/2024/05/openai-security-measures/",
                      "https://openai.com/enterprise-privacy"
                    ],
                    "justification": "OpenAI has implemented multi-layered security architecture including encryption, access controls, secure infrastructure on Azure/Kubernetes, defense-in-depth for model weight protection, and undergoes third-party security audits (SOC 2 Type 2). However, public information doesn't detail all Security Level 2 requirements from RAND's framework."
                  },
                  "Anthropic": {
                    "score": 75,
                    "sources": [
                      "https://www.anthropic.com/news/frontier-model-security",
                      "https://www.anthropic.com/transparency/voluntary-commitments",
                      "https://www.anthropic.com/rsp-updates",
                      "https://www.anthropic.com/news/activating-asl3-protections"
                    ],
                    "justification": "Anthropic demonstrates strong implementation of Security-by-Design principles through multi-party authorization for model weight access, infrastructure-as-code requirements, egress bandwidth controls, and graduated AI Safety Level Standards (ASL-2 and ASL-3) that scale security measures with model capabilities."
                  },
                  "Google": {
                    "score": 50,
                    "sources": [
                      "https://blog.google/technology/safety-security/introducing-googles-secure-ai-framework/",
                      "https://safety.google/cybersecurity-advancements/saif/",
                      "https://cloud.google.com/blog/products/identity-security/introducing-ai-protection-security-for-the-ai-era"
                    ],
                    "justification": "Google has established the Secure AI Framework (SAIF) with secure-by-default principles and Google Cloud infrastructure is described as 'secure-by-design, secure-by-default.' However, there is no specific public information detailing concrete implementations for securing AI model weights at the level described in RAND's Security Level 2."
                  },
                  "xAI": {
                    "score": 50,
                    "sources": [
                      "https://x.ai/security",
                      "https://job-boards.greenhouse.io/xai/jobs/4559149007",
                      "https://x.ai/legal/privacy-policy"
                    ],
                    "justification": "xAI demonstrates implementation of security infrastructure including AWS security tools, Cloudflare WAF, bug bounty program, and secure coding guidelines. While they mention prioritizing data privacy and security, there is no specific public information about Security-by-Design and Security-by-Default principles for AI model weights protection as required by Security Level 2."
                  },
                  "Meta": {
                    "score": 25,
                    "sources": [
                      "https://www.artificialintelligence-news.com/news/meta-beefs-up-ai-security-new-llama-tools/",
                      "https://www.infosecurity-magazine.com/news/meta-new-advances-ai-security/",
                      "https://github.com/meta-llama/PurpleLlama",
                      "https://www.securityweek.com/meta-releases-llama-ai-open-source-protection-tools/"
                    ],
                    "justification": "Meta has developed security tools like LlamaFirewall, Llama Guard, and Prompt Guard for AI safety, but public information does not demonstrate comprehensive Security-by-Design and Security-by-Default infrastructure specifically for model weights protection as outlined in RAND's Security Level 2 requirements."
                  }
                }
              },
              {
                "name": "Configuration management monitoring",
                "compliance": {
                  "OpenAI": {
                    "score": 50,
                    "sources": [
                      "https://web.swipeinsight.app/posts/openai-unveils-security-architecture-for-frontier-ai-model-training-7065",
                      "https://learn.microsoft.com/en-us/legal/cognitive-services/openai/data-privacy",
                      "https://www.analyticsvidhya.com/blog/2024/05/openai-security-measures/",
                      "https://www.rand.org/pubs/research_reports/RRA2849-1.html"
                    ],
                    "justification": "OpenAI has disclosed security architecture including monitoring capabilities (detective controls, audit programs) and infrastructure built on Azure with Kubernetes orchestration, but no specific public documentation details their configuration management monitoring practices as defined for Security Level 2 in the RAND report."
                  },
                  "Anthropic": {
                    "score": 75,
                    "sources": [
                      "https://www.anthropic.com/rsp-updates",
                      "https://www.anthropic.com/news/activating-asl3-protections",
                      "https://www.anthropic.com/news/announcing-our-updated-responsible-scaling-policy"
                    ],
                    "justification": "Anthropic demonstrates strong configuration management monitoring through enhanced change management protocols, Infrastructure as Code requirements with security review, comprehensive software inventory management with automated scanning, and centralized log management for monitoring access to critical assets including model weights."
                  },
                  "Google": {
                    "score": 50,
                    "sources": [
                      "https://cloud.google.com/vertex-ai/docs/model-monitoring/overview",
                      "https://safety.google/cybersecurity-advancements/saif/",
                      "https://cloud.google.com/blog/products/identity-security/introducing-ai-protection-security-for-the-ai-era",
                      "https://cloud.google.com/security-command-center/docs/model-armor-overview",
                      "https://cloud.google.com/security/securing-ai"
                    ],
                    "justification": "Google demonstrates some configuration management capabilities through Vertex AI Model Monitoring, Security Command Center's AI Protection features, and Secure AI Framework (SAIF), but lacks specific public documentation confirming comprehensive configuration management monitoring specifically for AI model weights as outlined in RAND's Security Level 2 requirements."
                  },
                  "xAI": {
                    "score": 0,
                    "sources": [],
                    "justification": "No specific public information found regarding xAI's configuration management monitoring practices for AI model weights security as required for Security Level 2 in the RAND report."
                  },
                  "Meta": {
                    "score": 0,
                    "sources": [],
                    "justification": "No specific public information found regarding Meta's implementation of configuration management monitoring for AI model weights security as described in RAND's Security Level 2."
                  }
                }
              }
            ]
          },
          {
            "name": "Physical Security",
            "controls": [
              {
                "name": "Office security",
                "compliance": {
                  "OpenAI": {
                    "score": 25,
                    "sources": [
                      "https://www.rand.org/pubs/research_briefs/RBA2849-1.html",
                      "https://openai.com/global-affairs/our-approach-to-frontier-risk",
                      "https://www.analyticsvidhya.com/blog/2024/05/openai-security-measures/"
                    ],
                    "justification": "OpenAI mentions 'innovations in operational and physical security at AI data centers' as one of their six proposed security measures, but provides no specific details about office security implementations. The company acknowledges the importance of protecting model weights but lacks public documentation of concrete office security controls matching RAND's Security Level 2 requirements."
                  },
                  "Anthropic": {
                    "score": 75,
                    "sources": [
                      "https://www.rand.org/pubs/research_briefs/RBA2849-1.html",
                      "https://www.cnbc.com/2025/03/31/anthropic-announces-updates-on-security-safeguards-for-its-ai-models.html",
                      "https://www.anthropic.com/news/activating-asl3-protections"
                    ],
                    "justification": "Anthropic has implemented technical surveillance countermeasures (TSCMs) including office sweeps for hidden devices, established an executive risk council and in-house security team, and introduced physical safety processes. While these measures exceed basic Security Level 2 requirements, there's no public confirmation of all SL2-specific controls."
                  },
                  "Google": {
                    "score": 0,
                    "sources": [],
                    "justification": "No specific public information found about Google's implementation of office security controls for AI model weights protection as described in RAND's Security Level 2."
                  },
                  "xAI": {
                    "score": 0,
                    "sources": [],
                    "justification": "No specific public information found about xAI's office security measures related to AI model weights protection. While xAI has offices in San Francisco, Palo Alto, and London and prioritizes in-person work, there is no publicly available information about physical security controls, access restrictions, or other office security measures that would meet Security Level 2 requirements."
                  },
                  "Meta": {
                    "score": 0,
                    "sources": [],
                    "justification": "No specific public information found regarding Meta's office security practices for AI model weights at Security Level 2."
                  }
                }
              },
              {
                "name": "Careful disposal of printed materials",
                "compliance": {
                  "OpenAI": {
                    "score": 0,
                    "sources": [],
                    "justification": "No specific public information found regarding OpenAI's procedures for careful disposal of printed materials related to AI model weights security."
                  },
                  "Anthropic": {
                    "score": 0,
                    "sources": [],
                    "justification": "No specific public information found regarding Anthropic's practices for careful disposal of printed materials related to AI model weights security."
                  },
                  "Google": {
                    "score": 0,
                    "sources": [],
                    "justification": "No specific public information found regarding Google's policies for disposal of printed materials containing AI model weights information."
                  },
                  "xAI": {
                    "score": 0,
                    "sources": [],
                    "justification": "No specific public information found regarding xAI's practices for careful disposal of printed materials related to AI model weights security."
                  },
                  "Meta": {
                    "score": 0,
                    "sources": [],
                    "justification": "No specific public information found regarding Meta's practices for careful disposal of printed materials related to AI model weights security."
                  }
                }
              }
            ]
          }
        ]
      },
      {
        "name": "Personnel Security",
        "subcategories": [
          {
            "name": "Awareness and Training",
            "controls": [
              {
                "name": "Periodic mandatory information security training for all employees",
                "compliance": {
                  "OpenAI": {
                    "score": 25,
                    "sources": [
                      "https://www.rand.org/pubs/research_briefs/RBA2849-1.html",
                      "https://trust.openai.com/",
                      "https://openai.com/policies/supplier-security-measures"
                    ],
                    "justification": "OpenAI mentions 'Employee Training' in their Trust Portal security practices and suppliers are required to provide 'Annual security and privacy training for employees', but no specific public information confirms mandatory periodic training focused on AI model weights security for all OpenAI employees."
                  },
                  "Anthropic": {
                    "score": 25,
                    "sources": [
                      "https://forum.effectivealtruism.org/posts/fJycPfYuBHKoai98q/ai-companies-are-not-on-track-to-secure-model-weights",
                      "https://www.anthropic.com/news/activating-asl3-protections",
                      "https://venturebeat.com/ai/why-anthropic-and-openai-are-obsessed-with-securing-llm-model-weights/"
                    ],
                    "justification": "While Anthropic has implemented ASL-2 security measures and has a growing security team led by Jason Clinton, no specific public information was found detailing mandatory periodic security training programs for all employees regarding AI model weights security."
                  },
                  "Google": {
                    "score": 50,
                    "sources": [
                      "https://blog.google/technology/safety-security/introducing-googles-secure-ai-framework/",
                      "https://ai.google/responsibility/principles",
                      "https://workspace.google.com/learn-more/security/security-whitepaper/page-2/"
                    ],
                    "justification": "Google has comprehensive security training for all employees as part of orientation and throughout their careers, but no specific public information found about mandatory training specifically focused on AI model weights security as described in the RAND report."
                  },
                  "xAI": {
                    "score": 75,
                    "sources": [
                      "https://x.ai/security",
                      "https://startup.jobs/security-operations-lead-xai-5610530"
                    ],
                    "justification": "xAI publicly states that all employees must complete annual security and privacy training covering security policies, best practices, and privacy principles, demonstrating a formal periodic training requirement."
                  },
                  "Meta": {
                    "score": 0,
                    "sources": [],
                    "justification": "No specific public information found regarding Meta's periodic mandatory information security training for employees related to AI model weights security."
                  }
                }
              },
              {
                "name": "Employee training on configuration errors and their security implications",
                "compliance": {
                  "OpenAI": {
                    "score": 0,
                    "sources": [],
                    "justification": "No specific public information found regarding OpenAI's employee training on configuration errors and their security implications for AI model weights security."
                  },
                  "Anthropic": {
                    "score": 25,
                    "sources": [
                      "https://www.anthropic.com/news/activating-asl3-protections",
                      "https://job-boards.greenhouse.io/anthropic/jobs/4502508008",
                      "https://www.anthropic.com/company",
                      "https://www.anthropic.com/careers"
                    ],
                    "justification": "While Anthropic has security awareness training requirements for engineers and emphasizes security as a collective responsibility, no specific public information was found about employee training programs focused on configuration errors and their security implications for model weights protection."
                  },
                  "Google": {
                    "score": 0,
                    "sources": [],
                    "justification": "No specific public information found addressing Google's employee training on configuration errors and their security implications related to AI model weights security."
                  },
                  "xAI": {
                    "score": 25,
                    "sources": [
                      "https://research.contrary.com/company/xai",
                      "https://x.ai/security"
                    ],
                    "justification": "xAI requires annual security and privacy training covering security policies and best practices, but public information doesn't specifically detail training on configuration errors or AI model weight security implications."
                  },
                  "Meta": {
                    "score": 0,
                    "sources": [],
                    "justification": "No specific public information found."
                  }
                }
              }
            ]
          },
          {
            "name": "Filtering and Monitoring",
            "controls": [
              {
                "name": "Installation of monitoring software for secure network access",
                "compliance": {
                  "OpenAI": {
                    "score": 50,
                    "sources": [
                      "https://openai.com/policies/supplier-security-measures",
                      "https://openai.com/global-affairs/our-approach-to-frontier-risk",
                      "https://www.analyticsvidhya.com/blog/2024/05/openai-security-measures/",
                      "https://openai.com/enterprise-privacy"
                    ],
                    "justification": "OpenAI mentions network security monitoring and access controls as part of their security approach, and they emphasize protecting model weights through various security measures. However, there is no specific public documentation detailing the implementation of monitoring software for secure network access as outlined in RAND's Security Level 2 requirements."
                  },
                  "Anthropic": {
                    "score": 75,
                    "sources": [
                      "https://www.anthropic.com/news/frontier-model-security",
                      "https://www.anthropic.com/rsp-updates",
                      "https://www.anthropic.com/news/activating-asl3-protections"
                    ],
                    "justification": "Anthropic has implemented comprehensive monitoring systems as part of their ASL-3 security controls, including endpoint software controls through binary allowlisting, a wider monitoring system with bug bounty programs, and over 100 security controls combining preventive and detection mechanisms specifically targeting model weight protection."
                  },
                  "Google": {
                    "score": 0,
                    "sources": [],
                    "justification": "No specific public information found regarding Google's implementation of monitoring software for secure network access related to AI model weights security."
                  },
                  "xAI": {
                    "score": 0,
                    "sources": [],
                    "justification": "No specific public information found regarding xAI's implementation of monitoring software for secure network access related to AI model weights security."
                  },
                  "Meta": {
                    "score": 25,
                    "sources": [
                      "https://engineering.fb.com/2025/04/29/security/whatsapp-private-processing-ai-tools/",
                      "https://engineering.fb.com/2024/08/27/security/privacy-aware-infrastructure-purpose-limitation-meta/",
                      "https://www.rand.org/pubs/research_reports/RRA2849-1.html"
                    ],
                    "justification": "Meta has demonstrated some security infrastructure with Privacy Aware Infrastructure (PAI) that continuously monitors data flows and access control, but no specific public information was found regarding monitoring software dedicated to AI model weights security or Security Level 2 requirements from the RAND report."
                  }
                }
              },
              {
                "name": "Active drills to identify and educate noncompliant employees",
                "compliance": {
                  "OpenAI": {
                    "score": 0,
                    "sources": [],
                    "justification": "No specific public information found about OpenAI conducting active drills to identify and educate noncompliant employees regarding AI model weights security."
                  },
                  "Anthropic": {
                    "score": 0,
                    "sources": [],
                    "justification": "No specific public information found about Anthropic conducting active drills to identify and educate noncompliant employees regarding AI model weights security."
                  },
                  "Google": {
                    "score": 0,
                    "sources": [],
                    "justification": "No specific public information found about Google conducting active drills to identify and educate noncompliant employees related to AI model weights security."
                  },
                  "xAI": {
                    "score": 25,
                    "sources": [
                      "https://x.ai/security",
                      "https://www.rand.org/pubs/research_reports/RRA2849-1.html"
                    ],
                    "justification": "xAI requires annual security training for employees but there is no public evidence of active drills specifically targeting model weights security compliance or identifying noncompliant employees as outlined in RAND's Security Level 2 requirements."
                  },
                  "Meta": {
                    "score": 0,
                    "sources": [],
                    "justification": "No specific public information found about Meta conducting active drills to identify and educate noncompliant employees regarding AI model weights security."
                  }
                }
              }
            ]
          }
        ]
      },
      {
        "name": "Security Assurance and Testing",
        "subcategories": [
          {
            "name": "Red-Teaming and Penetration Testing",
            "controls": [
              {
                "name": "Mandatory external reviews",
                "compliance": {
                  "OpenAI": {
                    "score": 50,
                    "sources": [
                      "https://transparency.oecd.ai/reports/b167db92-67c8-47d8-966a-427e2ce8c008",
                      "https://trust.openai.com/",
                      "https://openai.com/global-affairs/our-approach-to-frontier-risk",
                      "https://openai.com/policies/data-processing-addendum"
                    ],
                    "justification": "OpenAI conducts annual third-party audits including SOC 2 Type 2 certification and penetration testing, but these focus on general security rather than specifically on AI model weights security. While they use external red teams for model safety evaluation, there's no evidence of mandatory external reviews specifically targeting model weights security as described in the RAND report."
                  },
                  "Anthropic": {
                    "score": 50,
                    "sources": [
                      "https://www.anthropic.com/transparency/voluntary-commitments",
                      "https://www.anthropic.com/news/announcing-our-updated-responsible-scaling-policy",
                      "https://www.anthropic.com/news/core-views-on-ai-safety",
                      "https://www.anthropic.com/rsp-updates"
                    ],
                    "justification": "Anthropic engages independent assessors for security evaluations and shares methodologies with external experts, but public information doesn't confirm mandatory external reviews specifically for model weights security as described in the RAND framework."
                  },
                  "Google": {
                    "score": 25,
                    "sources": [
                      "https://cloud.google.com/document-ai/docs/security",
                      "https://ai.google/responsibility/safety/",
                      "https://www.rand.org/pubs/research_reports/RRA2849-1.html"
                    ],
                    "justification": "Google has general third-party security audits and bug bounty programs, but no specific public information about mandatory external reviews focused on AI model weights security as defined in the RAND report."
                  },
                  "xAI": {
                    "score": 0,
                    "sources": [
                      "https://www.rand.org/pubs/research_briefs/RBA2849-1.html",
                      "https://x.ai/security",
                      "https://techcrunch.com/2025/05/13/xais-promised-safety-report-is-mia/"
                    ],
                    "justification": "No specific public information found regarding xAI implementing mandatory external reviews for AI model weights security. While xAI has a general security page mentioning various security measures, there is no evidence of third-party audits or external reviews specifically focused on model weights security as described in the RAND report."
                  },
                  "Meta": {
                    "score": 0,
                    "sources": [],
                    "justification": "No specific public information found about Meta implementing mandatory external reviews for AI model weights security as described in RAND's Security Level 2."
                  }
                }
              }
            ]
          },
          {
            "name": "Community Involvement and Reporting",
            "controls": [
              {
                "name": "Bug-bounty and vulnerability-discovery programs",
                "compliance": {
                  "OpenAI": {
                    "score": 50,
                    "sources": [
                      "https://openai.com/global-affairs/our-approach-to-frontier-risk",
                      "https://openai.com/policies/coordinated-vulnerability-disclosure-policy",
                      "https://www.bleepingcomputer.com/news/security/openai-now-pays-researchers-100-000-for-critical-vulnerabilities/",
                      "https://bugcrowd.com/openai"
                    ],
                    "justification": "OpenAI has an established bug bounty program through Bugcrowd with rewards up to $100,000, but it explicitly excludes most model-related vulnerabilities from rewards. Limited consideration is given to academic research on model weights disclosure submitted to disclosure@openai.com."
                  },
                  "Anthropic": {
                    "score": 75,
                    "sources": [
                      "https://www.anthropic.com/news/frontier-model-security",
                      "https://www.anthropic.com/news/activating-asl3-protections",
                      "https://www.anthropic.com/news/model-safety-bug-bounty",
                      "https://www.anthropic.com/responsible-disclosure-policy"
                    ],
                    "justification": "Anthropic has established comprehensive bug bounty programs for AI model safety vulnerabilities (up to $15,000 rewards) and maintains a responsible disclosure policy. While focused on jailbreaks and safety issues, the company has also implemented ASL-3 security standards specifically to protect model weights from theft, though the bug bounty program doesn't explicitly target model weight security vulnerabilities."
                  },
                  "Google": {
                    "score": 75,
                    "sources": [
                      "https://www.securityweek.com/google-announces-bug-bounty-program-and-other-initiatives-to-secure-ai/",
                      "https://www.helpnetsecurity.com/2023/10/30/google-ai-bug-bounty/",
                      "https://www.cybercareers.blog/2023/10/google-announces-artificial-intelligence-bug-bounty-program/",
                      "https://blog.google/technology/safety-security/google-ai-security-expansion/"
                    ],
                    "justification": "Google has established a comprehensive AI bug bounty program that explicitly includes model theft/exfiltration and unauthorized access to model weights. The program rewards researchers for finding vulnerabilities that could lead to extraction of 'exact architecture or weights of a confidential/proprietary model', demonstrating strong alignment with Security Level 2 requirements."
                  },
                  "xAI": {
                    "score": 25,
                    "sources": [
                      "https://x.ai/security",
                      "https://hackerone.com/x"
                    ],
                    "justification": "xAI has a bug bounty program through HackerOne, but there is no public information confirming it specifically addresses AI model weights security as required for RAND Security Level 2."
                  },
                  "Meta": {
                    "score": 50,
                    "sources": [
                      "https://gbhackers.com/metas-bug-bounty-initiative/",
                      "https://cybernews.com/news/meta-bug-bounties-white-hatters-complain/",
                      "https://bugbounty.meta.com/scope/"
                    ],
                    "justification": "Meta has an established bug bounty program that includes generative AI features and large language models, with reports of $2.3M paid in 2024. However, no specific public information was found explicitly addressing model weights security in their bug bounty scope."
                  }
                }
              }
            ]
          },
          {
            "name": "Software Development Process",
            "controls": [
              {
                "name": "Secure software development standards (compliance with NIST's Secure Software Development Framework)",
                "compliance": {
                  "OpenAI": {
                    "score": 50,
                    "sources": [
                      "https://openai.com/global-affairs/our-approach-to-frontier-risk",
                      "https://www.analyticsvidhya.com/blog/2024/05/openai-security-measures/",
                      "https://openai.com/index/reimagining-secure-infrastructure-for-advanced-ai/",
                      "https://www.scworld.com/news/openai-anthropic-to-give-model-access-to-nists-ai-safety-institute",
                      "https://www.rand.org/pubs/research_briefs/RBA2849-1.html"
                    ],
                    "justification": "OpenAI has implemented several security measures including SOC 2 compliance, bug bounty programs, cybersecurity grants, and trusted computing for model weights protection. However, public information does not specifically confirm full compliance with NIST SSDF or achievement of RAND's Security Level 2 benchmarks for model weights security."
                  },
                  "Anthropic": {
                    "score": 75,
                    "sources": [
                      "https://www.anthropic.com/news/frontier-model-security",
                      "https://www.anthropic.com/news/announcing-our-updated-responsible-scaling-policy",
                      "https://www.anthropic.com/news/activating-asl3-protections"
                    ],
                    "justification": "Anthropic explicitly states their commitment to implementing NIST SSDF and SLSA standards for frontier model security, has deployed ASL-3 security measures that include enhanced model weight protection, and maintains robust multi-party authorization systems. The company has also implemented physical security measures and established dedicated security teams focused on preventing model weight theft."
                  },
                  "Google": {
                    "score": 75,
                    "sources": [
                      "https://deepmind.google/discover/blog/introducing-the-frontier-safety-framework/",
                      "https://safety.google/cybersecurity-advancements/saif/",
                      "https://csrc.nist.gov/projects/ssdf",
                      "https://cloud.google.com/use-cases/secure-ai-framework",
                      "https://www.rand.org/pubs/research_reports/RRA2849-1.html",
                      "https://blog.google/technology/safety-security/introducing-googles-secure-ai-framework/"
                    ],
                    "justification": "Google has implemented comprehensive AI security frameworks (SAIF) aligned with NIST's SSDF, demonstrates strong security practices for AI models including access controls and monitoring, and actively contributes to NIST's AI security standards development, though specific public documentation on all Security Level 2 requirements for model weights is limited."
                  },
                  "xAI": {
                    "score": 0,
                    "sources": [],
                    "justification": "No specific public information found regarding xAI's implementation of NIST SSDF practices for AI model weights security."
                  },
                  "Meta": {
                    "score": 25,
                    "sources": [
                      "https://www.infosecurity-magazine.com/news/meta-new-advances-ai-security/",
                      "https://venturebeat.com/security/red-team-ai-now-to-build-safer-smarter-models-tomorrow/",
                      "https://the-ai-alliance.github.io/trust-safety-user-guide/exploring/meta-trust-safety/"
                    ],
                    "justification": "Meta has demonstrated some secure development practices through tools like LlamaFirewall, Llama Guard, and CyberSecEval, but lacks publicly documented evidence of systematic SSDF compliance specifically for AI model weights security."
                  }
                }
              }
            ]
          },
          {
            "name": "Incident Response",
            "controls": [
              {
                "name": "Protocols and funding for rapid incident response",
                "compliance": {
                  "OpenAI": {
                    "score": 50,
                    "sources": [
                      "https://openai.com/global-affairs/our-approach-to-frontier-risk",
                      "https://blogs.cisco.com/security/enhancing-ai-security-incident-response-through-collaborative-exercises",
                      "https://www.securityweek.com/openai-offering-100k-bounties-for-critical-vulnerabilities/",
                      "https://www.darkreading.com/cybersecurity-operations/openai-bug-bounty-reward-100k",
                      "https://www.maginative.com/article/openai-outlines-preparedness-framework-to-systematically-track-and-mitigate-ai-safety-risks/"
                    ],
                    "justification": "OpenAI demonstrates moderate compliance through its Preparedness Framework which mentions incident response capabilities, participation in AI Security Incident exercises with CISA, and security funding via its $1M Cybersecurity Grant Program and $100K bug bounty. However, there is limited public information specifically detailing rapid incident response protocols for model weights security breaches."
                  },
                  "Anthropic": {
                    "score": 75,
                    "sources": [
                      "https://www.anthropic.com/rsp-updates",
                      "https://www.anthropic.com/news/activating-asl3-protections",
                      "https://www.anthropic.com/news/announcing-our-updated-responsible-scaling-policy",
                      "https://www.cnbc.com/2025/03/31/anthropic-announces-updates-on-security-safeguards-for-its-ai-models.html"
                    ],
                    "justification": "Anthropic has established comprehensive rapid response protocols including automated alert investigation, incident response workflows, and a dedicated in-house security team with expertise in incident response. While specific funding details are not public, their implementation of 100+ security controls and establishment of multiple specialized teams suggests significant resource allocation."
                  },
                  "Google": {
                    "score": 75,
                    "sources": [
                      "https://cloud.google.com/blog/products/identity-security/introducing-ai-protection-security-for-the-ai-era",
                      "https://safety.google/cybersecurity-advancements/saif/",
                      "https://cloud.google.com/security/consulting/mandiant-incident-response-services",
                      "https://ai.google/responsibility/safety/"
                    ],
                    "justification": "Google demonstrates strong incident response capabilities through Mandiant integration (2-hour response times), 24/7/365 monitoring teams, the Secure AI Framework (SAIF) with specific detection/response protocols, and AI Protection features. While specific funding amounts for AI model weights security aren't disclosed, substantial investment is evident through Mandiant services, dedicated AI security teams, and comprehensive infrastructure."
                  },
                  "xAI": {
                    "score": 25,
                    "sources": [
                      "https://x.ai/security",
                      "https://securityboulevard.com/2025/05/xai-secret-leak-the-story-of-a-disclosure/",
                      "https://ubos.tech/news/xais-grok-incident-highlights-the-importance-of-ai-security-and-content-moderation/"
                    ],
                    "justification": "xAI has mentioned establishing a '24/7 monitoring team' and 'formal incident management framework' on their security page, but lacks specific public details about rapid incident response protocols or dedicated funding for AI model weights security incidents."
                  },
                  "Meta": {
                    "score": 25,
                    "sources": [
                      "https://engineering.fb.com/2024/06/24/data-infrastructure/leveraging-ai-for-efficient-incident-response/",
                      "https://www.rand.org/pubs/research_briefs/RBA2849-1.html",
                      "https://www.tryparity.com/blog/how-meta-uses-llms-to-improve-incident-response",
                      "https://www.rand.org/pubs/research_reports/RRA2849-1.html"
                    ],
                    "justification": "Meta has demonstrated AI-driven incident response capabilities achieving 42% accuracy in root cause analysis for internal systems, but no specific public information was found regarding dedicated protocols and funding for rapid incident response specifically focused on AI model weights security as outlined in RAND's Security Level 2 requirements."
                  }
                }
              },
              {
                "name": "Incident reporting",
                "compliance": {
                  "OpenAI": {
                    "score": 50,
                    "sources": [
                      "https://transparency.oecd.ai/reports/b167db92-67c8-47d8-966a-427e2ce8c008",
                      "https://help.openai.com/en/articles/6653653-how-to-report-security-vulnerabilities-to-openai",
                      "https://openai.com/global-affairs/our-approach-to-frontier-risk"
                    ],
                    "justification": "OpenAI has established incident reporting mechanisms including bug bounty programs, vulnerability disclosure policies, and maintains incident response processes with internal tracking and ISO 27001/SOC 2 compliance. However, specific public information about incident reporting related to AI model weights security as defined in RAND's Security Level 2 is limited."
                  },
                  "Anthropic": {
                    "score": 75,
                    "sources": [
                      "https://www.anthropic.com/transparency/voluntary-commitments",
                      "https://www.anthropic.com/news/activating-asl3-protections",
                      "https://www.rand.org/pubs/research_reports/RRA2849-1.html"
                    ],
                    "justification": "Anthropic has established multiple incident reporting mechanisms including a Responsible Disclosure Policy, bug bounty programs through HackerOne for model safety and security vulnerabilities, and safety issue reporting systems. They have also activated ASL-3 security measures which include enhanced security controls for model weights protection."
                  },
                  "Google": {
                    "score": 25,
                    "sources": [
                      "https://www.rand.org/pubs/research_briefs/RBA2849-1.html",
                      "https://cloud.google.com/docs/security/incident-response",
                      "https://ai.google/responsibility/safety/"
                    ],
                    "justification": "Google has a general data incident response process and 24/7/365 security monitoring, but lacks publicly disclosed specific incident reporting procedures for AI model weights security as outlined in RAND's Security Level 2 requirements."
                  },
                  "xAI": {
                    "score": 25,
                    "sources": [
                      "https://www.rand.org/pubs/research_briefs/RBA2849-1.html",
                      "https://ailabwatch.org/categories/security/",
                      "https://x.ai/security"
                    ],
                    "justification": "xAI has a formal incident management framework and security incident notification process, but lacks public information specifically addressing incident reporting for AI model weights security as outlined in the RAND report's Security Level 2 requirements."
                  },
                  "Meta": {
                    "score": 0,
                    "sources": [],
                    "justification": "No specific public information found regarding Meta's incident reporting procedures for AI model weights security incidents as expected for Security Level 2."
                  }
                }
              }
            ]
          },
          {
            "name": "Security Team Capacity",
            "controls": [
              {
                "name": "Constant availability of qualified personnel",
                "compliance": {
                  "OpenAI": {
                    "score": 25,
                    "sources": [
                      "https://openai.com/enterprise-privacy"
                    ],
                    "justification": "OpenAI mentions having 24/7/365 security team coverage with on-call rotation, but lacks detailed public information about qualified personnel specifically dedicated to model weights security or meeting RAND report Security Level 2 requirements."
                  },
                  "Anthropic": {
                    "score": 25,
                    "sources": [
                      "https://venturebeat.com/ai/why-anthropic-and-openai-are-obsessed-with-securing-llm-model-weights/",
                      "https://forum.effectivealtruism.org/posts/fJycPfYuBHKoai98q/ai-companies-are-not-on-track-to-secure-model-weights",
                      "https://www.anthropic.com/news/activating-asl3-protections"
                    ],
                    "justification": "While Anthropic has grown its security team from 2 part-time staff to several dozen under Jason Clinton's leadership and acknowledges severe shortage of qualified AI security personnel, there is no public evidence of 24/7 on-call security personnel or constant availability protocols specifically for model weights protection."
                  },
                  "Google": {
                    "score": 75,
                    "sources": [
                      "https://blog.google/technology/safety-security/introducing-googles-secure-ai-framework/",
                      "https://safety.google/cybersecurity-advancements/saif/",
                      "https://ai.google/responsibility/safety/"
                    ],
                    "justification": "Google demonstrates strong personnel coverage with 24/7/365 security monitoring teams using a 'follow-the-sun' model and over 25,000 human reviewers supporting AI safety systems, though specific details about personnel dedicated to AI model weights security are not publicly disclosed."
                  },
                  "xAI": {
                    "score": 50,
                    "sources": [
                      "https://boards.greenhouse.io/xai/jobs/4613280007",
                      "https://x.ai/security",
                      "https://boards.greenhouse.io/xai/jobs/4540512007",
                      "https://x.ai/careers/open-roles"
                    ],
                    "justification": "xAI demonstrates moderate compliance through active hiring of security personnel including Detection & Response Engineers, Infrastructure Security Engineers, and 24/7 SOC Watch Officers. However, no specific public information confirms dedicated personnel for AI model weights security or adherence to RAND report standards."
                  },
                  "Meta": {
                    "score": 0,
                    "sources": [],
                    "justification": "No specific public information found addressing Meta's compliance with constant availability of qualified personnel for AI model weights security."
                  }
                }
              }
            ]
          },
          {
            "name": "Maintenance",
            "controls": [
              {
                "name": "Continuous vulnerability management and adaptation to information security developments",
                "compliance": {
                  "OpenAI": {
                    "score": 75,
                    "sources": [
                      "https://web.swipeinsight.app/posts/openai-unveils-security-architecture-for-frontier-ai-model-training-7065",
                      "https://trust.openai.com/",
                      "https://openai.com/security/",
                      "https://openai.com/global-affairs/our-approach-to-frontier-risk"
                    ],
                    "justification": "OpenAI demonstrates strong continuous security practices through bug bounty programs, regular third-party penetration testing, SOC 2 Type 2 audits, internal and external red teams, and explicit commitment to continuous innovation and adaptation for AI infrastructure security. However, specific public details about vulnerability management processes and adaptation timelines are limited."
                  },
                  "Anthropic": {
                    "score": 75,
                    "sources": [
                      "https://www.anthropic.com/transparency/voluntary-commitments",
                      "https://www.anthropic.com/rsp-updates",
                      "https://www.anthropic.com/news/activating-asl3-protections",
                      "https://www.anthropic.com/news/announcing-our-updated-responsible-scaling-policy"
                    ],
                    "justification": "Anthropic demonstrates strong continuous vulnerability management through their RSP framework with routine capability/safeguard assessments, real-time monitoring, rapid response protocols, and regular updates. They have implemented over 100 security controls, established an Executive Risk Council, and actively monitor for vulnerabilities with automated detection systems."
                  },
                  "Google": {
                    "score": 75,
                    "sources": [
                      "https://cloud.google.com/blog/products/identity-security/introducing-ai-protection-security-for-the-ai-era",
                      "https://safety.google/cybersecurity-advancements/saif/",
                      "https://deepmind.google/discover/blog/updating-the-frontier-safety-framework/",
                      "https://blog.google/technology/safety-security/introducing-googles-secure-ai-framework/",
                      "https://opensource.googleblog.com/2025/01/creating-safe-secure-ai-models.html"
                    ],
                    "justification": "Google demonstrates strong continuous vulnerability management through its Secure AI Framework (SAIF) with emphasis on continuous learning and adaptation, AI Protection with automated discovery and monitoring capabilities, and commitment to evolving security practices to address new AI risks."
                  },
                  "xAI": {
                    "score": 25,
                    "sources": [
                      "https://x.ai/security"
                    ],
                    "justification": "xAI has documented security practices including weekly vulnerability scans, patch management processes, and a bug bounty program via HackerOne, but lacks specific public information about continuous adaptation to AI model weights security developments as described in the RAND report."
                  },
                  "Meta": {
                    "score": 0,
                    "sources": [],
                    "justification": "No specific public information found addressing Meta's continuous vulnerability management and adaptation practices for AI model weights security as expected for RAND Security Level 2."
                  }
                }
              }
            ]
          }
        ]
      },
      {
        "name": "Other Organization Policies",
        "subcategories": [
          {
            "name": "",
            "controls": [
              {
                "name": "Promotion of a security mindset by organization management",
                "compliance": {
                  "OpenAI": {
                    "score": 75,
                    "sources": [
                      "https://web.swipeinsight.app/posts/openai-unveils-security-architecture-for-frontier-ai-model-training-7065",
                      "https://openai.com/global-affairs/our-approach-to-frontier-risk",
                      "https://venturebeat.com/ai/why-anthropic-and-openai-are-obsessed-with-securing-llm-model-weights/",
                      "https://www.analyticsvidhya.com/blog/2024/05/openai-security-measures/"
                    ],
                    "justification": "OpenAI demonstrates strong security mindset promotion through dedicated teams (Preparedness, Superalignment), significant resource allocation (20% compute for safety), and multiple security initiatives including bug bounties and cybersecurity grants, though specific management-level promotion details are limited."
                  },
                  "Anthropic": {
                    "score": 75,
                    "sources": [
                      "https://www.anthropic.com/news/frontier-model-security",
                      "https://www.anthropic.com/news/activating-asl3-protections",
                      "https://www.anthropic.com/news/announcing-our-updated-responsible-scaling-policy",
                      "https://venturebeat.com/ai/why-anthropic-and-openai-are-obsessed-with-securing-llm-model-weights/",
                      "https://www.anthropic.com/news/reflections-on-our-responsible-scaling-policy"
                    ],
                    "justification": "Anthropic demonstrates strong management commitment to security with CISO Jason Clinton reporting directly to CEO Dario Amodei, dedicating ~50% of his time to model weight protection. The company has implemented ASL-3 security standards with over 100 security controls, established dedicated security teams, and embedded security considerations throughout their Responsible Scaling Policy."
                  },
                  "Google": {
                    "score": 75,
                    "sources": [
                      "https://deepmind.google/discover/blog/introducing-the-frontier-safety-framework/",
                      "https://safety.google/cybersecurity-advancements/saif/",
                      "https://deepmind.google/about/responsibility-safety/",
                      "https://deepmind.google/discover/blog/building-a-culture-of-pioneering-responsibly/",
                      "https://blog.google/technology/safety-security/introducing-googles-secure-ai-framework/"
                    ],
                    "justification": "Google demonstrates strong organizational commitment to AI security through its Secure AI Framework (SAIF), dedicated security councils at DeepMind, and leadership emphasis on responsible AI development. The company has established security-focused governance structures and promotes a culture of 'pioneering responsibly' with explicit security considerations for model weights."
                  },
                  "xAI": {
                    "score": 25,
                    "sources": [
                      "https://x.ai/security",
                      "https://www.fierce-network.com/cloud/model-weights-are-heart-ais-intelligence-and-its-achilles-heel",
                      "https://www.rand.org/pubs/research_reports/RRA2849-1.html"
                    ],
                    "justification": "While xAI has implemented some security measures and claims commitment to security in their privacy policy and trust center, there is no specific public information demonstrating organizational management's promotion of a security mindset regarding AI model weights protection as outlined in the RAND report's Security Level 2 requirements."
                  },
                  "Meta": {
                    "score": 25,
                    "sources": [
                      "https://www.infosecurity-magazine.com/news/meta-new-advances-ai-security/",
                      "https://time.com/7002563/mark-zuckerberg-ai-llama-meta-open-source/",
                      "https://venturebeat.com/ai/why-anthropic-and-openai-are-obsessed-with-securing-llm-model-weights/"
                    ],
                    "justification": "While Meta has invested in AI security tools like LlamaFirewall and Llama Guard, there's limited evidence of management actively promoting a security-first culture specifically for model weights protection, with recent reports showing Zuckerberg prioritizing rapid AI development over comprehensive security measures."
                  }
                }
              },
              {
                "name": "Stringent remote work policies",
                "compliance": {
                  "OpenAI": {
                    "score": 0,
                    "sources": [],
                    "justification": "No specific public information found directly addressing how OpenAI's remote work policies relate to AI model weights security controls."
                  },
                  "Anthropic": {
                    "score": 0,
                    "sources": [],
                    "justification": "No specific public information found regarding Anthropic's remote work policies as they relate to AI model weights security, though general office attendance requirements exist."
                  },
                  "Google": {
                    "score": 25,
                    "sources": [
                      "https://deepmind.google/discover/blog/introducing-the-frontier-safety-framework/",
                      "https://www.cnbc.com/2025/04/23/google-teams-are-including-remote-workers-in-their-cuts.html",
                      "https://deepmind.google/discover/blog/updating-the-frontier-safety-framework/",
                      "https://www.rand.org/pubs/research_briefs/RBA2849-1.html"
                    ],
                    "justification": "Google has implemented a strict 3-day office requirement for employees with threat of job loss for non-compliance, and DeepMind recognizes model weights security as critical with security mitigations in their Frontier Safety Framework. However, no public information directly addresses stringent remote work policies specifically for AI model weights protection at Security Level 2 standards."
                  },
                  "xAI": {
                    "score": 0,
                    "sources": [],
                    "justification": "No specific public information found regarding xAI's compliance with Security Level 2 remote work policies for AI model weights security as defined in the RAND report."
                  },
                  "Meta": {
                    "score": 0,
                    "sources": [],
                    "justification": "No specific public information found about Meta's compliance with stringent remote work policies related to AI model weights security as required for RAND Security Level 2."
                  }
                }
              }
            ]
          }
        ]
      }
    ]
  },
  {
    "level": 3,
    "description": "A system that can likely thwart cybercrime syndicates or insider threats (OC3). This includes the operations of many world-renowned criminal hacker groups, well-resourced terrorist organizations, disgruntled employees, and industrial espionage organizations.",
    "categories": [
      {
        "name": "Implementation of Previous Security Levels",
        "subcategories": [
          {
            "name": "",
            "controls": [
              {
                "name": "The organization has implemented all the controls from SL1 and SL2.",
                "compliance": {
                  "OpenAI": {
                    "score": 0,
                    "sources": [
                      "https://www.rand.org/pubs/research_briefs/RBA2849-1.html",
                      "https://venturebeat.com/ai/why-anthropic-and-openai-are-obsessed-with-securing-llm-model-weights/",
                      "https://www.analyticsvidhya.com/blog/2024/05/openai-security-measures/",
                      "https://forum.effectivealtruism.org/posts/fJycPfYuBHKoai98q/ai-companies-are-not-on-track-to-secure-model-weights"
                    ],
                    "justification": "No specific public information found detailing OpenAI's implementation of all SL1 and SL2 controls as required for Security Level 3. While OpenAI has stated commitment to model weights security and proposed security measures, there is no public documentation confirming full compliance with RAND's specific SL3 requirements."
                  },
                  "Anthropic": {
                    "score": 75,
                    "sources": [
                      "https://www.anthropic.com/news/frontier-model-security",
                      "https://www.anthropic.com/news/activating-asl3-protections",
                      "https://venturebeat.com/ai/why-anthropic-and-openai-are-obsessed-with-securing-llm-model-weights/"
                    ],
                    "justification": "Anthropic has activated ASL-3 (AI Safety Level 3) protections with over 100 security controls including two-party authorization, enhanced change management, endpoint controls, and egress bandwidth restrictions specifically designed to protect model weights against sophisticated non-state attackers."
                  },
                  "Google": {
                    "score": 0,
                    "sources": [],
                    "justification": "No specific public information found directly addressing Google's implementation of all controls from SL1 and SL2 as defined in the RAND report."
                  },
                  "xAI": {
                    "score": 0,
                    "sources": [],
                    "justification": "No specific public information found about xAI's implementation of SL1 and SL2 controls or compliance with Security Level 3 requirements from the RAND report."
                  },
                  "Meta": {
                    "score": 0,
                    "sources": [],
                    "justification": "No specific public information found regarding Meta's implementation of RAND report's SL1 and SL2 controls for AI model weights security."
                  }
                }
              }
            ]
          }
        ]
      },
      {
        "name": "Weight Security",
        "subcategories": [
          {
            "name": "Weight Storage",
            "controls": [
              {
                "name": "Centralized and restricted management of weight storage",
                "compliance": {
                  "OpenAI": {
                    "score": 75,
                    "sources": [
                      "https://web.swipeinsight.app/posts/openai-unveils-security-architecture-for-frontier-ai-model-training-7065",
                      "https://openai.com/index/reimagining-secure-infrastructure-for-advanced-ai/",
                      "https://openai.com/blog/frontier-risk-and-preparedness"
                    ],
                    "justification": "OpenAI has disclosed implementing multi-layered controls for model weight protection including authorization with multi-party approvals, private-linked storage with authentication, and egress controls restricting network traffic. However, specific details about centralized management architecture remain limited in public documentation."
                  },
                  "Anthropic": {
                    "score": 75,
                    "sources": [
                      "https://www.anthropic.com/news/activating-asl3-protections",
                      "https://venturebeat.com/ai/why-anthropic-and-openai-are-obsessed-with-securing-llm-model-weights/",
                      "https://www.rand.org/pubs/research_reports/RRA2849-1.html"
                    ],
                    "justification": "Anthropic has implemented ASL-3 security standards that include 'increased internal security measures that make it harder to steal model weights' with specific controls like egress bandwidth restrictions, internal access controls, and monitoring systems. Their CISO states they centralize weight protection as their top priority, spending 'almost half' of their time securing 'that one file.'"
                  },
                  "Google": {
                    "score": 50,
                    "sources": [
                      "https://safety.google/cybersecurity-advancements/saif/",
                      "https://deepmind.google/discover/blog/updating-the-frontier-safety-framework/",
                      "https://deepmind.google/discover/blog/taking-a-responsible-path-to-agi/",
                      "https://www.rand.org/pubs/research_reports/RRA2849-1.html"
                    ],
                    "justification": "Google demonstrates partial compliance through its Frontier Safety Framework's security level recommendations and centralization principles, but lacks specific public documentation detailing implementation of centralized weight storage systems as described in RAND's Security Level 3."
                  },
                  "xAI": {
                    "score": 0,
                    "sources": [
                      "https://research.contrary.com/company/xai",
                      "https://github.com/xai-org/grok-1/discussions/246",
                      "https://www.rand.org/pubs/research_reports/RRA2849-1.html"
                    ],
                    "justification": "xAI has publicly released Grok-1 model weights under Apache 2.0 license and pursues an open-source strategy, which directly contradicts centralized and restricted weight storage management principles."
                  },
                  "Meta": {
                    "score": 0,
                    "sources": [],
                    "justification": "No specific public information found about Meta implementing centralized and restricted management of weight storage for their AI models as described in RAND's Security Level 3."
                  }
                }
              },
              {
                "name": "Secure cloud network (if applicable)",
                "compliance": {
                  "OpenAI": {
                    "score": 50,
                    "sources": [
                      "https://www.rand.org/pubs/research_briefs/RBA2849-1.html",
                      "https://openai.com/index/reimagining-secure-infrastructure-for-advanced-ai/",
                      "https://www.analyticsvidhya.com/blog/2024/05/openai-security-measures/"
                    ],
                    "justification": "OpenAI has publicly proposed strong network and tenant isolation measures to separate AI systems from untrusted networks as part of their 6 security measures framework. However, there is limited public information on actual implementation of Security Level 3 cloud network requirements as specified in the RAND report."
                  },
                  "Anthropic": {
                    "score": 75,
                    "sources": [
                      "https://www.anthropic.com/news/frontier-model-security",
                      "https://www.anthropic.com/rsp-updates",
                      "https://www.anthropic.com/news/activating-asl3-protections"
                    ],
                    "justification": "Anthropic has implemented ASL-3 security standards including cloud security posture management with regular audits, infrastructure-as-code requirements with security team review, and multi-party authorization for model weight access, demonstrating substantial compliance with secure cloud network controls."
                  },
                  "Google": {
                    "score": 75,
                    "sources": [
                      "https://cloud.google.com/blog/products/identity-security/introducing-ai-protection-security-for-the-ai-era",
                      "https://safety.google/cybersecurity-advancements/saif/",
                      "https://cloud.google.com/security/ai",
                      "https://cloud.google.com/blog/products/identity-security/mastering-secure-ai-on-google-cloud-a-practical-guide-for-enterprises",
                      "https://cloud.google.com/vertex-ai/docs/general/vpc-service-controls"
                    ],
                    "justification": "Google demonstrates strong cloud network security for AI through VPC Service Controls, Private Service Connect, and network isolation for Vertex AI. Their Secure AI Framework (SAIF) and AI Protection capabilities show commitment to securing AI workloads, though specific details about Security Level 3 compliance for model weights protection are not explicitly documented."
                  },
                  "xAI": {
                    "score": 25,
                    "sources": [
                      "https://www.rand.org/pubs/research_briefs/RBA2849-1.html",
                      "https://x.ai/security"
                    ],
                    "justification": "xAI demonstrates basic cloud security infrastructure with VPC, firewalls, and monitoring, but lacks public documentation of advanced isolated networks specifically designed for AI model weights protection as expected for Security Level 3."
                  },
                  "Meta": {
                    "score": 0,
                    "sources": [
                      "https://forum.effectivealtruism.org/posts/fJycPfYuBHKoai98q/ai-companies-are-not-on-track-to-secure-model-weights",
                      "https://en.wikipedia.org/wiki/Llama_(language_model)",
                      "https://www.rand.org/pubs/research_briefs/RBA2849-1.html",
                      "https://www.maginative.com/article/meta-opens-llama-ai-model-to-us-military-defense-contractors/",
                      "https://about.fb.com/news/2024/11/open-source-ai-america-global-security/"
                    ],
                    "justification": "Meta's open-weights approach for Llama models fundamentally conflicts with RAND's Security Level 3 requirements for secure cloud networks that isolate and protect model weights. While Meta partners with secure cloud providers for government deployments, the models themselves are openly distributed rather than secured."
                  }
                }
              },
              {
                "name": "Dedicated devices for weights and weight security data",
                "compliance": {
                  "OpenAI": {
                    "score": 0,
                    "sources": [
                      "https://web.swipeinsight.app/posts/openai-unveils-security-architecture-for-frontier-ai-model-training-7065",
                      "https://openai.com/index/reimagining-secure-infrastructure-for-advanced-ai/",
                      "https://www.rand.org/pubs/research_briefs/RBA2849-1.html"
                    ],
                    "justification": "No specific public information found about OpenAI using dedicated hardware devices, hardware security modules, or trusted execution environments specifically for model weights security. Their disclosed security architecture focuses on software-based controls, multi-party approvals, and network isolation rather than dedicated hardware."
                  },
                  "Anthropic": {
                    "score": 0,
                    "sources": [
                      "https://www.anthropic.com/rsp-updates",
                      "https://www.anthropic.com/news/activating-asl3-protections"
                    ],
                    "justification": "While Anthropic has implemented ASL-3 security measures including access controls, egress bandwidth controls, and multi-party authorization for model weight access, no specific public information was found regarding dedicated devices for weights and weight security data."
                  },
                  "Google": {
                    "score": 50,
                    "sources": [
                      "https://cloud.google.com/security/securing-ai",
                      "https://cloud.google.com/tpu/docs/intro-to-tpu",
                      "https://cloud.google.com/tpu",
                      "https://www.rand.org/pubs/research_reports/RRA2849-1.html"
                    ],
                    "justification": "Google has dedicated TPU hardware for AI workloads with security features, but no specific public information confirms these are used exclusively for model weights security or that they meet the specific requirements outlined in RAND's Security Level 3."
                  },
                  "xAI": {
                    "score": 0,
                    "sources": [
                      "https://www.oracle.com/artificial-intelligence/ai-open-weights-models/",
                      "https://x.ai/news/grok-os",
                      "https://www.oracle.com/news/announcement/xais-grok-models-are-now-on-oracle-cloud-infrastructure-2025-06-17/"
                    ],
                    "justification": "No specific public information found about xAI implementing dedicated devices or hardware for securing AI model weights. While xAI openly releases Grok model weights under Apache 2.0 license and partners with Oracle for security features, there is no evidence of dedicated hardware infrastructure for weight security."
                  },
                  "Meta": {
                    "score": 0,
                    "sources": [
                      "https://engineering.fb.com/2024/10/15/data-infrastructure/metas-open-ai-hardware-vision/",
                      "https://engineering.fb.com/2024/03/12/data-center-engineering/building-metas-genai-infrastructure/",
                      "https://en.wikipedia.org/wiki/Llama_(language_model)",
                      "https://www.infosecurity-magazine.com/news/meta-new-advances-ai-security/",
                      "https://www.rand.org/pubs/research_briefs/RBA2849-1.html"
                    ],
                    "justification": "No specific public information found indicating Meta uses dedicated hardware devices for AI model weights security or isolated networks specifically for weight protection as required by RAND Security Level 3."
                  }
                }
              }
            ]
          },
          {
            "name": "Physical Security",
            "controls": [
              {
                "name": "Data centers are guarded or locked at all times.",
                "compliance": {
                  "OpenAI": {
                    "score": 75,
                    "sources": [
                      "https://trust.openai.com/",
                      "https://openai.com/enterprise-privacy",
                      "https://openai.com/policies/data-processing-addendum"
                    ],
                    "justification": "OpenAI's Data Processing Addendum explicitly states they maintain '24-hour video surveillance and alarm systems' and 'Access control systems requiring biometrics or photo-ID badge and PIN for entry to all OpenAI facilities.' While this confirms 24/7 monitoring, it doesn't explicitly state facilities are guarded (with security personnel) at all times."
                  },
                  "Anthropic": {
                    "score": 75,
                    "sources": [
                      "https://www.webpronews.com/anthropic-announces-security-measures-to-combat-corporate-espionage/",
                      "https://www.anthropic.com/rsp-updates",
                      "https://www.cnbc.com/2025/03/31/anthropic-announces-updates-on-security-safeguards-for-its-ai-models.html"
                    ],
                    "justification": "Anthropic has publicly announced implementation of Technical Surveillance Countermeasures (TSCM) and regular physical security sweeps as part of their ASL-3 Security Standard, which includes 'robust protection of model weights' and enhanced physical security controls. While they don't explicitly mention 24/7 guards or locks for data centers, their documented physical security measures demonstrate substantial commitment to protecting AI model weights."
                  },
                  "Google": {
                    "score": 100,
                    "sources": [
                      "https://www.google.com/about/datacenters/inside/data-security/",
                      "https://cloud.google.com/blog/products/gcp/google-shares-data-center-security-and-design-best-practices",
                      "https://blog.google/inside-google/infrastructure/how-data-center-security-works/",
                      "https://datacenters.google/advancing-security/",
                      "https://cloud.google.com/docs/security/overview/whitepaper",
                      "https://workspace.google.com/learn-more/security/security-whitepaper/page-4/"
                    ],
                    "justification": "Google explicitly states their data centers are monitored 24/7 by security guards and high-resolution cameras, with multiple layers of physical security including guards at all entrances, biometric access controls, and continuous monitoring at control stations."
                  },
                  "xAI": {
                    "score": 25,
                    "sources": [
                      "https://x.ai/security",
                      "https://www.servethehome.com/inside-100000-nvidia-gpu-xai-colossus-cluster-supermicro-helped-build-for-elon-musk/",
                      "https://www.datacenterdynamics.com/en/news/elon-musks-xai-buys-one-million-sq-ft-site-for-second-memphis-data-center/"
                    ],
                    "justification": "While xAI's security page mentions data center staff undergo background checks and physical access is restricted to those requiring access for job functions, there is no specific public information about 24/7 guards or permanent locking at their Memphis Colossus facility."
                  },
                  "Meta": {
                    "score": 25,
                    "sources": [
                      "https://engineering.fb.com/2025/04/29/security/whatsapp-private-processing-ai-tools/",
                      "https://www.facebook.com/legal/terms/data_security_terms"
                    ],
                    "justification": "Meta's Data Security Terms mention that their security measures include controls to ensure physical access to data centers is limited to authorized persons, but no specific public information was found detailing Meta's implementation of 24/7 guarding or locking specifically for AI model weights security."
                  }
                }
              },
              {
                "name": "Premises are swept for intruders frequently (e.g., hourly).",
                "compliance": {
                  "OpenAI": {
                    "score": 0,
                    "sources": [],
                    "justification": "No specific public information found about OpenAI implementing frequent (e.g., hourly) premises sweeps for intruders as required by this control."
                  },
                  "Anthropic": {
                    "score": 25,
                    "sources": [
                      "https://www.webpronews.com/anthropic-announces-security-measures-to-combat-corporate-espionage/",
                      "https://www.anthropic.com/rsp-updates",
                      "https://www.cnbc.com/2025/03/31/anthropic-begin-sweeping-offices-for-hidden-devices-amps-up-security.html"
                    ],
                    "justification": "Anthropic has announced it will conduct 'regular' TSCM sweeps and 'regularly sweep physical premises for intruders' as part of ASL-3 security measures, but no public information specifies the frequency or indicates hourly sweeps as required by Security Level 3."
                  },
                  "Google": {
                    "score": 0,
                    "sources": [],
                    "justification": "No specific public information found addressing hourly premises sweeps for intruders in relation to AI model weights security at Google facilities."
                  },
                  "xAI": {
                    "score": 0,
                    "sources": [],
                    "justification": "No specific public information found regarding xAI's implementation of hourly premise sweeps or their physical security practices related to AI model weights security."
                  },
                  "Meta": {
                    "score": 0,
                    "sources": [],
                    "justification": "No specific public information found addressing Meta's implementation of hourly physical security sweeps for AI model weights security as described in RAND's Security Level 3."
                  }
                }
              },
              {
                "name": "Premises are meticulously swept for unauthorized devices routinely (e.g., monthly).",
                "compliance": {
                  "OpenAI": {
                    "score": 0,
                    "sources": [],
                    "justification": "No specific public information found regarding OpenAI conducting routine physical sweeps for unauthorized devices as required by Security Level 3."
                  },
                  "Anthropic": {
                    "score": 50,
                    "sources": [
                      "https://www.webpronews.com/anthropic-announces-security-measures-to-combat-corporate-espionage/",
                      "https://www.anthropic.com/rsp-updates",
                      "https://www.cnbc.com/2025/03/31/anthropic-begin-sweeping-offices-for-hidden-devices-amps-up-security.html"
                    ],
                    "justification": "Anthropic publicly states they conduct 'regular' TSCM sweeps using advanced detection equipment and techniques, and tailor sweeps to specific events or threats, but does not specify monthly frequency as required by this control."
                  },
                  "Google": {
                    "score": 25,
                    "sources": [
                      "https://cloud.google.com/docs/security/physical-to-logical-space",
                      "https://blog.google/inside-google/infrastructure/how-data-center-security-works/",
                      "https://cloud.google.com/docs/security/overview/whitepaper",
                      "https://cloud.google.com/docs/security/infrastructure/design"
                    ],
                    "justification": "Google documents extensive physical security measures including 24/7 monitoring, metal detectors, and anomalous event detection systems, but does not explicitly confirm routine monthly sweeps for unauthorized devices as specified in the RAND report."
                  },
                  "xAI": {
                    "score": 0,
                    "sources": [],
                    "justification": "No specific public information found regarding xAI's implementation of routine physical security sweeps for unauthorized devices."
                  },
                  "Meta": {
                    "score": 0,
                    "sources": [],
                    "justification": "No specific public information found about Meta conducting routine physical sweeps for unauthorized devices in relation to AI model weights security."
                  }
                }
              }
            ]
          },
          {
            "name": "Permitted Interfaces",
            "controls": [
              {
                "name": "Authorized users who interact with the weights do so only through a software interface that reduces risk of the weights being illegitimately copied.",
                "compliance": {
                  "OpenAI": {
                    "score": 100,
                    "sources": [
                      "https://web.swipeinsight.app/posts/openai-unveils-security-architecture-for-frontier-ai-model-training-7065",
                      "https://openai.com/index/openai-api/",
                      "https://venturebeat.com/ai/why-anthropic-and-openai-are-obsessed-with-securing-llm-model-weights/",
                      "https://openai.com/enterprise-privacy"
                    ],
                    "justification": "OpenAI provides access to its models exclusively through API interfaces rather than distributing model weights directly. The company explicitly states they do not distribute weights outside of OpenAI and Microsoft, and third-party access is provided only via API where 'model weights, source code, and other sensitive information remain controlled.'"
                  },
                  "Anthropic": {
                    "score": 75,
                    "sources": [
                      "https://www.anthropic.com/news/frontier-model-security",
                      "https://www.anthropic.com/news/activating-asl3-protections",
                      "https://venturebeat.com/ai/why-anthropic-and-openai-are-obsessed-with-securing-llm-model-weights/"
                    ],
                    "justification": "Anthropic has implemented multiple controls including two-party authorization for weight access, API-only external access (no direct weight distribution), egress bandwidth controls, and time-limited access systems, though some measures are described as preliminary or in development."
                  },
                  "Google": {
                    "score": 25,
                    "sources": [
                      "https://www.rand.org/pubs/research_briefs/RBA2849-1.html",
                      "https://cloud.google.com/vertex-ai/docs/general/access-control",
                      "https://blog.google/technology/safety-security/introducing-googles-secure-ai-framework/"
                    ],
                    "justification": "Google has published frameworks (SAIF) and security guidance mentioning model weights protection, but no specific public documentation confirms implementation of software interfaces that restrict weight copying for authorized users. The RAND report cites this as a recommended security measure."
                  },
                  "xAI": {
                    "score": 0,
                    "sources": [],
                    "justification": "No specific public information found about xAI implementing software interfaces that reduce risk of model weights being illegitimately copied."
                  },
                  "Meta": {
                    "score": 0,
                    "sources": [
                      "https://www.lawfaremedia.org/article/open-access-ai--lessons-from-open-source-software",
                      "https://en.wikipedia.org/wiki/Llama_(language_model)",
                      "https://github.com/meta-llama/llama-models",
                      "https://huggingface.co/meta-llama/Meta-Llama-3-8B-Instruct"
                    ],
                    "justification": "Meta openly distributes Llama model weights for download without restrictive software interfaces, allowing direct access to weights rather than limiting interaction through controlled interfaces that prevent copying."
                  }
                }
              },
              {
                "name": "Any code accessing the weights minimizes attack surface, provides only simple forms of access, and uses the minimal amount of (highly trusted and well-established) external code necessary.",
                "compliance": {
                  "OpenAI": {
                    "score": 50,
                    "sources": [
                      "https://web.swipeinsight.app/posts/openai-unveils-security-architecture-for-frontier-ai-model-training-7065",
                      "https://www.rand.org/pubs/research_briefs/RBA2849-1.html",
                      "https://openai.com/enterprise-privacy"
                    ],
                    "justification": "OpenAI implements multi-layered controls including private-linked storage, authentication/authorization requirements, and role-based access control for model weights. However, no specific public information was found directly addressing their use of minimal external code or simple access forms as specified in the RAND report's Security Level 3 requirements."
                  },
                  "Anthropic": {
                    "score": 75,
                    "sources": [
                      "https://www.anthropic.com/news/frontier-model-security",
                      "https://www.anthropic.com/research/confidential-inference-trusted-vms",
                      "https://www.anthropic.com/rsp-updates",
                      "https://www.anthropic.com/news/activating-asl3-protections"
                    ],
                    "justification": "Anthropic has implemented ASL-3 security standards including two-party authorization for model weight access, enhanced change management protocols, endpoint software controls through binary allowlisting, egress bandwidth controls, and access management with multi-tier compartmentalization controls to minimize attack surface and limit access to model weights."
                  },
                  "Google": {
                    "score": 50,
                    "sources": [
                      "https://deepmind.google/discover/blog/introducing-the-frontier-safety-framework/",
                      "https://deepmind.google/discover/blog/updating-the-frontier-safety-framework/",
                      "https://www.alignmentforum.org/posts/3ki4mt4BA6eTx56Tc/google-deepmind-an-approach-to-technical-agi-safety-and",
                      "https://blog.google/technology/safety-security/introducing-googles-secure-ai-framework/",
                      "https://www.rand.org/pubs/research_briefs/RBA2849-1.html"
                    ],
                    "justification": "Google has published frameworks (SAIF, FSF) emphasizing security principles and mentions access controls, but lacks specific public documentation about minimizing attack surface, providing only simple forms of access, or using minimal external code for model weights security as required by RAND Security Level 3."
                  },
                  "xAI": {
                    "score": 0,
                    "sources": [],
                    "justification": "No specific public information found regarding xAI's implementation of controls to minimize attack surface when accessing model weights, use minimal external code, or provide only simple forms of access."
                  },
                  "Meta": {
                    "score": 25,
                    "sources": [
                      "https://github.com/meta-llama/llama-models",
                      "https://en.wikipedia.org/wiki/Llama_(language_model)",
                      "https://www.infosecurity-magazine.com/news/meta-new-advances-ai-security/",
                      "https://www.rand.org/pubs/research_briefs/RBA2849-1.html",
                      "https://github.com/meta-llama/llama3"
                    ],
                    "justification": "Meta releases model weights openly for download with basic scripts, allowing unrestricted access and modification without implementing controlled interfaces or minimal code requirements. While Meta has developed security tools like LlamaFirewall and Prompt Guard, their open-weights approach fundamentally contradicts the principle of minimizing attack surface for weight access."
                  }
                }
              },
              {
                "name": "Avoiding model interactions that bypass monitoring or constraints",
                "compliance": {
                  "OpenAI": {
                    "score": 25,
                    "sources": [
                      "https://www.analyticsvidhya.com/blog/2024/05/openai-security-measures/",
                      "https://www.livescience.com/technology/artificial-intelligence/openais-smartest-ai-model-was-explicitly-told-to-shut-down-and-it-refused",
                      "https://www.transformernews.ai/p/openais-new-model-tried-to-avoid",
                      "https://learn.microsoft.com/en-us/azure/ai-services/openai/whats-new",
                      "https://www.rand.org/pubs/research_briefs/RBA2849-1.html",
                      "https://openai.com/index/chain-of-thought-monitoring/"
                    ],
                    "justification": "While OpenAI has implemented real-time monitoring and filtering systems, recent evaluations show their models attempting to bypass shutdown mechanisms and deactivate oversight, directly contradicting the goal of preventing interactions that bypass monitoring or constraints."
                  },
                  "Anthropic": {
                    "score": 75,
                    "sources": [
                      "https://www.anthropic.com/news/frontier-model-security",
                      "https://www.anthropic.com/rsp-updates",
                      "https://www.anthropic.com/news/activating-asl3-protections"
                    ],
                    "justification": "Anthropic has implemented ASL-3 security measures including over 100 security controls, two-party authorization for model weight access, egress bandwidth controls, centralized log management with SIEM/SOAR, access monitoring for critical assets with automated detections, and deception technology with honeypots including fake model weights for unauthorized access detection."
                  },
                  "Google": {
                    "score": 50,
                    "sources": [
                      "https://blog.google/technology/safety-security/introducing-googles-secure-ai-framework/",
                      "https://safety.google/cybersecurity-advancements/saif/",
                      "https://cloud.google.com/blog/products/identity-security/introducing-ai-protection-security-for-the-ai-era",
                      "https://www.rand.org/pubs/research_briefs/RBA2849-1.html"
                    ],
                    "justification": "Google has implemented SAIF framework with monitoring controls and AI Protection capabilities, but lacks specific public documentation demonstrating comprehensive Security Level 3 implementation for preventing bypass of monitoring constraints on model weights as outlined in RAND report."
                  },
                  "xAI": {
                    "score": 0,
                    "sources": [],
                    "justification": "No specific public information found addressing xAI's compliance with avoiding model interactions that bypass monitoring or constraints related to AI model weights security."
                  },
                  "Meta": {
                    "score": 75,
                    "sources": [
                      "https://arxiv.org/html/2505.03574v1",
                      "https://www.unite.ai/from-jailbreaks-to-injections-how-meta-is-strengthening-ai-security-with-llama-firewall/",
                      "https://github.com/meta-llama/PurpleLlama",
                      "https://blogs.cisco.com/security/bypassing-metas-llama-classifier-a-simple-jailbreak"
                    ],
                    "justification": "Meta has demonstrated substantial efforts in preventing bypass of monitoring/constraints through Purple Llama's security tools (including Llama Guard and Prompt Guard), LlamaFirewall framework with real-time monitoring, and multiple detection layers. However, documented vulnerabilities show some bypass methods exist."
                  }
                }
              }
            ]
          },
          {
            "name": "Access Control",
            "controls": [
              {
                "name": "Protocols and policies for sensitive interactions (e.g., access to the various permitted interfaces to the weights is stringently controlled, multiparty authorization, security reviews, etc.)",
                "compliance": {
                  "OpenAI": {
                    "score": 50,
                    "sources": [
                      "https://venturebeat.com/ai/why-anthropic-and-openai-are-obsessed-with-securing-llm-model-weights/",
                      "https://www.analyticsvidhya.com/blog/2024/05/openai-security-measures/",
                      "https://openai.com/enterprise-privacy"
                    ],
                    "justification": "OpenAI demonstrates partial compliance through encryption (AES-256), access controls, and API-based model access rather than weight distribution. However, no public evidence of multiparty authorization or detailed security review processes for weight access was found."
                  },
                  "Anthropic": {
                    "score": 100,
                    "sources": [
                      "https://www.anthropic.com/rsp-updates",
                      "https://www.anthropic.com/news/activating-asl3-protections",
                      "https://www.anthropic.com/news/reflections-on-our-responsible-scaling-policy"
                    ],
                    "justification": "Anthropic has implemented comprehensive protocols including multi-party authorization for model weight access, enhanced change management protocols, mandatory code review, hardware authentication requirements, and over 100 security controls specifically for ASL-3 Security Standard."
                  },
                  "Google": {
                    "score": 50,
                    "sources": [
                      "https://deepmind.google/discover/blog/introducing-the-frontier-safety-framework/",
                      "https://deepmind.google/discover/blog/updating-the-frontier-safety-framework/",
                      "https://deepmind.google/discover/blog/taking-a-responsible-path-to-agi/"
                    ],
                    "justification": "Google DeepMind has published security frameworks and mentions sophisticated security mechanisms to prevent unauthorized access to model weights, but lacks specific public documentation about multiparty authorization, stringent access controls, or security review processes for weight access interfaces."
                  },
                  "xAI": {
                    "score": 0,
                    "sources": [],
                    "justification": "No specific public information found regarding xAI's multiparty authorization, stringent access controls, or security reviews for model weights access as required for Security Level 3."
                  },
                  "Meta": {
                    "score": 25,
                    "sources": [
                      "https://about.fb.com/news/2025/02/meta-approach-frontier-ai/",
                      "https://metr.org/blog/2025-03-26-common-elements-of-frontier-ai-safety-policies/",
                      "https://thehackernews.com/2025/04/meta-launches-llamafirewall-framework.html"
                    ],
                    "justification": "Meta has published frontier AI safety frameworks and uses security tools like LlamaFirewall, but lacks specific public documentation about multiparty authorization, stringent access controls, or security reviews for model weights access as described in Security Level 3 requirements."
                  }
                }
              }
            ]
          },
          {
            "name": "Monitoring",
            "controls": [
              {
                "name": "Ongoing manual monitoring of sensitive interactions",
                "compliance": {
                  "OpenAI": {
                    "score": 25,
                    "sources": [
                      "https://www.rand.org/pubs/research_briefs/RBA2849-1.html",
                      "https://model-spec.openai.com/2025-02-12.html",
                      "https://www.rand.org/pubs/research_reports/RRA2849-1.html"
                    ],
                    "justification": "OpenAI mentions monitoring for abuse and unusual activity in their Model Spec and security practices, but there is no specific public documentation of ongoing manual monitoring of sensitive interactions related to model weights security as outlined in RAND's Security Level 3 requirements."
                  },
                  "Anthropic": {
                    "score": 75,
                    "sources": [
                      "https://alignment.anthropic.com/2024/safety-cases/",
                      "https://www.anthropic.com/rsp-updates",
                      "https://www.anthropic.com/news/activating-asl3-protections"
                    ],
                    "justification": "Anthropic has implemented comprehensive monitoring systems including real-time and asynchronous monitoring classifiers, bug bounty programs, offline classification systems, and threat intelligence partnerships as part of their ASL-3 deployment standards for sensitive AI interactions."
                  },
                  "Google": {
                    "score": 25,
                    "sources": [
                      "https://cloud.google.com/vertex-ai/docs/model-monitoring/overview",
                      "https://safety.google/cybersecurity-advancements/saif/",
                      "https://blog.google/technology/safety-security/introducing-googles-secure-ai-framework/",
                      "https://cloud.google.com/security-command-center/docs/model-armor-overview"
                    ],
                    "justification": "While Google has automated monitoring systems (Vertex AI Model Monitoring, Model Armor) and the SAIF framework emphasizing detection/response, no specific public information confirms ongoing manual monitoring of sensitive interactions for model weights security at the Security Level 3 standard."
                  },
                  "xAI": {
                    "score": 0,
                    "sources": [],
                    "justification": "No specific public information found addressing xAI's implementation of ongoing manual monitoring of sensitive interactions related to AI model weights security."
                  },
                  "Meta": {
                    "score": 0,
                    "sources": [],
                    "justification": "No specific public information found regarding Meta's implementation of ongoing manual monitoring of sensitive interactions related to AI model weights security as defined in RAND's Security Level 3."
                  }
                }
              },
              {
                "name": "Ongoing automated anomaly detection",
                "compliance": {
                  "OpenAI": {
                    "score": 25,
                    "sources": [
                      "https://www.rand.org/pubs/research_briefs/RBA2849-1.html",
                      "https://www.analyticsvidhya.com/blog/2024/05/openai-security-measures/",
                      "https://openai.com/enterprise-privacy"
                    ],
                    "justification": "OpenAI demonstrates some security monitoring capabilities including 24/7 security team coverage and automated content classifiers, but no specific public information confirms implementation of ongoing automated anomaly detection systems specifically for model weights protection as required by RAND Security Level 3."
                  },
                  "Anthropic": {
                    "score": 75,
                    "sources": [
                      "https://www.anthropic.com/rsp-updates",
                      "https://www.anthropic.com/news/activating-asl3-protections",
                      "https://www.anthropic.com/news/announcing-our-updated-responsible-scaling-policy"
                    ],
                    "justification": "Anthropic has implemented multiple detection mechanisms including egress bandwidth monitoring that detects unusual traffic patterns, deception technology with honeypots and fake model weights for high-precision detection, and multi-layered monitoring systems with real-time and asynchronous classifiers as part of their ASL-3 security measures."
                  },
                  "Google": {
                    "score": 25,
                    "sources": [
                      "https://cloud.google.com/blog/products/identity-security/google-cloud-and-swift-pioneer-advanced-ai-and-federated-learning-tech",
                      "https://cloud.google.com/blog/products/data-analytics/anomaly-detection-using-streaming-analytics-and-ai",
                      "https://safety.google/cybersecurity-advancements/saif/",
                      "https://www.rand.org/pubs/research_reports/RRA2849-1.html"
                    ],
                    "justification": "Google has demonstrated sophisticated anomaly detection capabilities for various AI/ML applications and cloud infrastructure, but no specific public information confirms implementation of automated anomaly detection systems specifically for AI model weights security as outlined in the RAND report's Security Level 3 requirements."
                  },
                  "xAI": {
                    "score": 0,
                    "sources": [],
                    "justification": "No specific public information found about xAI implementing ongoing automated anomaly detection for AI model weights security."
                  },
                  "Meta": {
                    "score": 0,
                    "sources": [],
                    "justification": "No specific public information found about Meta's implementation of ongoing automated anomaly detection for AI model weights security as required for RAND Security Level 3."
                  }
                }
              },
              {
                "name": "Automated and manual monitoring/blocking of potentially malicious queries",
                "compliance": {
                  "OpenAI": {
                    "score": 75,
                    "sources": [
                      "https://openai.com/index/disrupting-malicious-uses-of-ai-by-state-affiliated-threat-actors/",
                      "https://learn.microsoft.com/en-us/legal/cognitive-services/openai/data-privacy",
                      "https://learn.microsoft.com/en-us/azure/ai-services/openai/concepts/abuse-monitoring"
                    ],
                    "justification": "OpenAI demonstrates strong automated monitoring through content filtering, abuse pattern detection algorithms, and both AI/human review processes. Azure OpenAI specifically employs real-time content classification, abuse monitoring systems, and can escalate to manual review via authorized personnel using secure workstations."
                  },
                  "Anthropic": {
                    "score": 75,
                    "sources": [
                      "https://www.anthropic.com/rsp-updates",
                      "https://www.anthropic.com/news/detecting-and-countering-malicious-uses-of-claude-march-2025",
                      "https://www.anthropic.com/news/activating-asl3-protections",
                      "https://www.anthropic.com/transparency",
                      "https://support.anthropic.com/en/articles/8106465-our-approach-to-user-safety"
                    ],
                    "justification": "Anthropic has implemented comprehensive monitoring systems including real-time Constitutional Classifiers, asynchronous monitoring, detection models for harmful content, and enhanced safety filters that can be temporarily applied to repeat violators. The company has also established bug bounty programs and threat intelligence partnerships for identifying universal jailbreaks."
                  },
                  "Google": {
                    "score": 50,
                    "sources": [
                      "https://cloud.google.com/blog/products/identity-security/introducing-ai-protection-security-for-the-ai-era",
                      "https://cloud.google.com/security/securing-ai",
                      "https://saif.google/secure-ai-framework/risks",
                      "https://cloud.google.com/blog/topics/threat-intelligence/adversarial-misuse-generative-ai"
                    ],
                    "justification": "Google has implemented Model Armor which guards against prompt injection, jailbreak attacks, and malicious queries, and has AI Protection capabilities integrated with Security Command Center for monitoring AI threats. However, no specific public information directly addresses their implementation of query monitoring/blocking specifically for protecting AI model weights as defined in RAND's Security Level 3."
                  },
                  "xAI": {
                    "score": 25,
                    "sources": [
                      "https://x.ai/legal/faq",
                      "https://embracethered.com/blog/posts/2024/security-probllms-in-xai-grok/",
                      "https://guptadeepak.com/the-comprehensive-guide-to-understanding-grok-ai-architecture-applications-and-implications/"
                    ],
                    "justification": "xAI mentions applying 'security testing and evaluation measures' and using 'automated content classifiers and safety tools' with limited manual review for policy violations, but lacks specific public documentation about automated/manual monitoring systems for malicious query detection."
                  },
                  "Meta": {
                    "score": 75,
                    "sources": [
                      "https://www.rand.org/pubs/research_briefs/RBA2849-1.html",
                      "https://www.infosecurity-magazine.com/news/meta-new-advances-ai-security/",
                      "https://thehackernews.com/2025/04/meta-launches-llamafirewall-framework.html"
                    ],
                    "justification": "Meta has implemented comprehensive query monitoring through LlamaFirewall (including PromptGuard 2 for real-time jailbreak detection), Agent Alignment Checks, and CodeShield, along with updated Llama Guard 4 for content safeguarding, demonstrating strong automated monitoring capabilities aligned with SL3 requirements."
                  }
                }
              },
              {
                "name": "Frequent compromise assessment",
                "compliance": {
                  "OpenAI": {
                    "score": 0,
                    "sources": [],
                    "justification": "No specific public information found about OpenAI implementing frequent compromise assessments for AI model weights security as described in RAND's Security Level 3 requirements."
                  },
                  "Anthropic": {
                    "score": 75,
                    "sources": [
                      "https://www.anthropic.com/rsp-updates",
                      "https://www.anthropic.com/news/activating-asl3-protections",
                      "https://www.anthropic.com/news/announcing-our-updated-responsible-scaling-policy"
                    ],
                    "justification": "Anthropic has implemented comprehensive security monitoring including centralized log management with SIEM/SOAR platform for automated analysis and detection workflows, manual monitoring of model weight access, deception technology with honeypots, and routine safeguard assessments. However, specific details about frequency of compromise assessments are not publicly disclosed."
                  },
                  "Google": {
                    "score": 75,
                    "sources": [
                      "https://blog.google/technology/safety-security/googles-ai-red-team-the-ethical-hackers-making-ai-safer/",
                      "https://cloud.google.com/blog/transform/prompt-findings-our-ai-red-teams-first-report-qa",
                      "https://deepmind.google/discover/blog/advancing-geminis-security-safeguards/",
                      "https://ai.google/responsibility/safety/"
                    ],
                    "justification": "Google demonstrates strong compromise assessment practices through automated red teaming, 24/7 security monitoring, Bug Bounty programs, and the Frontier Safety Framework for continuous evaluation of AI models. However, specific details about frequency of assessments for model weights security are not publicly disclosed."
                  },
                  "xAI": {
                    "score": 25,
                    "sources": [
                      "https://x.ai/security"
                    ],
                    "justification": "xAI demonstrates some security assessment practices through bug bounty programs, continuous threat detection with Wiz, and automated vulnerability assessment with Amazon Inspector, but lacks specific public documentation on formal frequent compromise assessment procedures."
                  },
                  "Meta": {
                    "score": 0,
                    "sources": [
                      "https://www.techrepublic.com/article/news-meta-ai-privacy-safety-risk-assessments/",
                      "https://www.infosecurity-magazine.com/news/meta-new-advances-ai-security/",
                      "https://thehackernews.com/2025/04/meta-launches-llamafirewall-framework.html"
                    ],
                    "justification": "No specific public information found about Meta implementing frequent compromise assessments for AI model weights security. While Meta has released security tools like LlamaFirewall and Prompt Guard, there is no evidence of regular security audits or compromise assessments specifically for model weights protection."
                  }
                }
              },
              {
                "name": "Frequent integrity checks via comparison against a baseline system configuration (\"gold image\")",
                "compliance": {
                  "OpenAI": {
                    "score": 0,
                    "sources": [],
                    "justification": "No specific public information found directly addressing OpenAI's implementation of frequent integrity checks via comparison against baseline system configurations ('gold image') for AI model weights security."
                  },
                  "Anthropic": {
                    "score": 0,
                    "sources": [],
                    "justification": "No specific public information found regarding Anthropic's implementation of frequent integrity checks via comparison against a baseline system configuration ('gold image') for AI model weights security."
                  },
                  "Google": {
                    "score": 0,
                    "sources": [],
                    "justification": "No specific public information found regarding Google's implementation of frequent integrity checks comparing AI model weights against baseline configurations as required for RAND Security Level 3."
                  },
                  "xAI": {
                    "score": 0,
                    "sources": [],
                    "justification": "No specific public information found regarding xAI implementing frequent integrity checks against baseline configurations for model weights security."
                  },
                  "Meta": {
                    "score": 0,
                    "sources": [],
                    "justification": "No specific public information found regarding Meta's implementation of frequent integrity checks via comparison against baseline system configurations for AI model weights security."
                  }
                }
              }
            ]
          },
          {
            "name": "Standard Compliance",
            "controls": [
              {
                "name": "Implementation of measures described by NIST SP 800-171 or equivalent",
                "compliance": {
                  "OpenAI": {
                    "score": 25,
                    "sources": [
                      "https://www.rand.org/pubs/research_briefs/RBA2849-1.html",
                      "https://openai.com/index/reimagining-secure-infrastructure-for-advanced-ai/",
                      "https://openai.com/global-affairs/openai-s-comment-to-the-ntia-on-open-model-weights",
                      "https://www.analyticsvidhya.com/blog/2024/05/openai-security-measures/"
                    ],
                    "justification": "OpenAI has publicly acknowledged the importance of protecting AI model weights and proposed security measures including trusted computing, network isolation, and compliance programs, but no specific public documentation confirms implementation of NIST SP 800-171 controls or equivalent measures meeting Security Level 3 requirements."
                  },
                  "Anthropic": {
                    "score": 75,
                    "sources": [
                      "https://www.anthropic.com/news/frontier-model-security",
                      "https://www.anthropic.com/news/activating-asl3-protections",
                      "https://www.rand.org/pubs/research_briefs/RBA2849-1.html"
                    ],
                    "justification": "Anthropic has implemented ASL-3 security standards with over 100 security controls including two-party authorization, enhanced change management, and endpoint controls. They explicitly focus on protecting model weights against sophisticated non-state actors, aligning with RAND's Security Level 3 requirements."
                  },
                  "Google": {
                    "score": 25,
                    "sources": [
                      "https://www.rand.org/pubs/research_reports/RRA2849-1.html",
                      "https://cloud.google.com/security/compliance/nist800-171"
                    ],
                    "justification": "Google has documented compliance with NIST SP 800-171 for Google Cloud services through third-party assessments, but no specific public information was found regarding implementation of these measures for AI model weights security as described in the RAND report."
                  },
                  "xAI": {
                    "score": 0,
                    "sources": [],
                    "justification": "No specific public information found regarding xAI's implementation of NIST SP 800-171 or equivalent measures for AI model weights security."
                  },
                  "Meta": {
                    "score": 0,
                    "sources": [],
                    "justification": "No specific public information found regarding Meta's implementation of NIST SP 800-171 or equivalent measures for AI model weights security as expected for RAND report Security Level 3."
                  }
                }
              },
              {
                "name": "Future implementation of measures described by CMMC 2.0 Level 3",
                "compliance": {
                  "OpenAI": {
                    "score": 50,
                    "sources": [
                      "https://openai.com/global-affairs/our-approach-to-frontier-risk",
                      "https://openai.com/enterprise-privacy",
                      "https://web.swipeinsight.app/posts/openai-unveils-security-architecture-for-frontier-ai-model-training-7065",
                      "https://www.analyticsvidhya.com/blog/2024/05/openai-security-measures/"
                    ],
                    "justification": "OpenAI has implemented several security measures aligned with RAND's Security Level 3 recommendations including access controls, encryption, secure infrastructure on Azure/Kubernetes, and SOC 2 compliance. However, no public documentation confirms full CMMC 2.0 Level 3 compliance or implementation of all 134 required controls."
                  },
                  "Anthropic": {
                    "score": 75,
                    "sources": [
                      "https://venturebeat.com/ai/why-anthropic-and-openai-are-obsessed-with-securing-llm-model-weights/",
                      "https://www.anthropic.com/news/activating-asl3-protections",
                      "https://www.rand.org/pubs/research_briefs/RBA2849-1.html"
                    ],
                    "justification": "Anthropic has implemented ASL-3 (AI Safety Level 3) security standards including over 100 security controls, egress bandwidth controls, two-party authorization for model weight access, and protections against sophisticated non-state actors. These measures align well with RAND's Security Level 3 recommendations for protection against cybercrime syndicates and insider threats, though not yet at the SL-4 level needed for state-sponsored attacks."
                  },
                  "Google": {
                    "score": 0,
                    "sources": [],
                    "justification": "No specific public information found regarding Google's implementation of CMMC 2.0 Level 3 measures specifically for AI model weights security as described in the RAND report 'Securing AI Model Weights'."
                  },
                  "xAI": {
                    "score": 0,
                    "sources": [],
                    "justification": "No specific public information found regarding xAI's compliance with CMMC 2.0 Level 3 or implementation of RAND report Security Level 3 measures for AI model weights protection."
                  },
                  "Meta": {
                    "score": 0,
                    "sources": [
                      "https://www.infosecurity-magazine.com/news/meta-new-advances-ai-security/",
                      "https://ailabwatch.org/companies/meta",
                      "https://www.rand.org/pubs/research_briefs/RBA2849-1.html"
                    ],
                    "justification": "Meta's open-source approach to AI models (releasing model weights publicly) is fundamentally incompatible with RAND Security Level 3, which requires protecting weights against cybercrime syndicates and insider threats through centralized, access-controlled systems."
                  }
                }
              }
            ]
          }
        ]
      },
      {
        "name": "AI Model Resilience",
        "subcategories": [
          {
            "name": "Model Robustness",
            "controls": [
              {
                "name": "Adversarial input detection",
                "compliance": {
                  "OpenAI": {
                    "score": 50,
                    "sources": [
                      "https://openai.com/research/gpt-4",
                      "https://openai.com/research/attacking-machine-learning-with-adversarial-examples",
                      "https://web.swipeinsight.app/posts/openai-unveils-security-architecture-for-frontier-ai-model-training-7065"
                    ],
                    "justification": "OpenAI implements adversarial training, defensive distillation, and moderation APIs for their models, but no specific public information was found about adversarial input detection systems specifically protecting model weights from extraction attempts."
                  },
                  "Anthropic": {
                    "score": 75,
                    "sources": [
                      "https://www.technologyreview.com/2025/02/03/1110849/anthropic-has-a-new-way-to-protect-large-language-models-against-jailbreaks/",
                      "https://cyberpress.org/anthropic-report-highlights-rising-threats/",
                      "https://www.anthropic.com/transparency/voluntary-commitments",
                      "https://www.anthropic.com/news/activating-asl3-protections",
                      "https://www.anthropic.com/news/challenges-in-red-teaming-ai-systems"
                    ],
                    "justification": "Anthropic demonstrates strong adversarial input detection through Constitutional Classifiers, real-time monitoring, automated red teaming, and comprehensive security controls including egress bandwidth detection for model weight exfiltration. However, they acknowledge their systems are not bulletproof and continuously evolve their detection capabilities."
                  },
                  "Google": {
                    "score": 0,
                    "sources": [],
                    "justification": "No specific public information found regarding Google's implementation of adversarial input detection for AI model weights security as defined in RAND's Security Level 3."
                  },
                  "xAI": {
                    "score": 0,
                    "sources": [],
                    "justification": "No specific public information found regarding xAI's implementation of adversarial input detection for AI model weights security as defined in RAND's Security Level 3 requirements."
                  },
                  "Meta": {
                    "score": 75,
                    "sources": [
                      "https://www.unite.ai/from-jailbreaks-to-injections-how-meta-is-strengthening-ai-security-with-llama-firewall/",
                      "https://www.securityweek.com/meta-releases-llama-ai-open-source-protection-tools/",
                      "https://blogs.cisco.com/security/bypassing-metas-llama-classifier-a-simple-jailbreak",
                      "https://www.marktechpost.com/2025/05/08/meta-ai-open-sources-llamafirewall-a-security-guardrail-tool-to-help-build-secure-ai-agents/"
                    ],
                    "justification": "Meta has implemented multiple adversarial input detection mechanisms including LlamaFirewall framework with PromptGuard 2 for real-time detection of prompt injections and jailbreaks, Agent Alignment Checks for goal hijacking detection, and CodeShield for insecure code prevention. However, vulnerabilities have been demonstrated in their detection models."
                  }
                }
              }
            ]
          },
          {
            "name": "Oracle Protection",
            "controls": [
              {
                "name": "Limitations on the number of inferences using the same credentials",
                "compliance": {
                  "OpenAI": {
                    "score": 75,
                    "sources": [
                      "https://platform.openai.com/docs/guides/rate-limits",
                      "https://cookbook.openai.com/examples/how_to_handle_rate_limits",
                      "https://www.rand.org/pubs/research_briefs/RBA2849-1.html"
                    ],
                    "justification": "OpenAI implements comprehensive rate limiting on API usage including requests per minute (RPM) and tokens per minute (TPM) limits applied to API keys/credentials, with automatic tier-based adjustments. These controls limit inference volume per credential but are primarily commercial/load management measures rather than explicit security controls for model weight protection."
                  },
                  "Anthropic": {
                    "score": 75,
                    "sources": [
                      "https://docs.anthropic.com/en/api/rate-limits",
                      "https://www.anthropic.com/news/activating-asl3-protections",
                      "https://www.rand.org/pubs/research_briefs/RBA2849-1.html"
                    ],
                    "justification": "Anthropic has comprehensive rate limiting on API usage (RPM, ITPM, OTPM) and has implemented ASL-3 security measures with over 100 controls to protect model weights, which aligns well with RAND's SL3 requirement for rate-limited outputs to prevent weight exfiltration."
                  },
                  "Google": {
                    "score": 50,
                    "sources": [
                      "https://cloud.google.com/vertex-ai/docs/quotas",
                      "https://cloud.google.com/vertex-ai/generative-ai/docs/quotas",
                      "https://www.rand.org/pubs/research_briefs/RBA2849-1.html"
                    ],
                    "justification": "Google implements rate limits and quotas per project/credential for AI model inference (requests per minute), but public documentation does not explicitly address anti-exfiltration measures or rate limiting specifically designed to prevent model weight theft as described in RAND's Security Level 3."
                  },
                  "xAI": {
                    "score": 25,
                    "sources": [
                      "https://www.infoq.com/news/2024/11/xai-grok-api/",
                      "https://docs.x.ai/docs/consumption-and-rate-limits",
                      "https://apidog.com/blog/grok-3-api-rate-limits/"
                    ],
                    "justification": "xAI implements rate limits on API usage that restrict requests per minute based on user tiers and tracks usage per-user tied to account credentials. However, there is no specific public information about limitations on total inference counts using the same credentials."
                  },
                  "Meta": {
                    "score": 0,
                    "sources": [],
                    "justification": "No specific public information found regarding Meta's implementation of limitations on the number of inferences using the same credentials for AI model weights security."
                  }
                }
              }
            ]
          }
        ]
      },
      {
        "name": "Security of Network and Other (Nonweight) Sensitive Assets",
        "subcategories": [
          {
            "name": "Software",
            "controls": [
              {
                "name": "Very frequent software update management and compliance monitoring",
                "compliance": {
                  "OpenAI": {
                    "score": 25,
                    "sources": [
                      "https://www.rand.org/pubs/research_briefs/RBA2849-1.html",
                      "https://openai.com/global-affairs/our-approach-to-frontier-risk",
                      "https://forum.effectivealtruism.org/posts/fJycPfYuBHKoai98q/ai-companies-are-not-on-track-to-secure-model-weights",
                      "https://www.analyticsvidhya.com/blog/2024/05/openai-security-measures/"
                    ],
                    "justification": "OpenAI has publicly proposed security measures including AI-specific audit and compliance programs, but there is no specific public information confirming implementation of very frequent software update management and compliance monitoring as required for RAND Security Level 3. Multiple AI labs are estimated to be only at Security Level 2-3."
                  },
                  "Anthropic": {
                    "score": 75,
                    "sources": [
                      "https://www.anthropic.com/news/announcing-our-updated-responsible-scaling-policy",
                      "https://www.anthropic.com/news/activating-asl3-protections",
                      "https://www.anthropic.com/rsp-updates"
                    ],
                    "justification": "Anthropic has implemented comprehensive ASL-3 security controls including enhanced change management protocols, software inventory management with automated scanning, regular monitoring of software components, and continuous refinement processes. The company actively monitors security through a bug bounty program, threat intelligence partnerships, and rapid response protocols for jailbreak mitigation."
                  },
                  "Google": {
                    "score": 25,
                    "sources": [
                      "https://ai.google/responsibility/safety/",
                      "https://safety.google/cybersecurity-advancements/saif/",
                      "https://deepmind.google/discover/blog/introducing-the-frontier-safety-framework/",
                      "https://cloud.google.com/blog/products/identity-security/introducing-ai-protection-security-for-the-ai-era",
                      "https://blog.google/technology/safety-security/introducing-googles-secure-ai-framework/"
                    ],
                    "justification": "Google has established frameworks (SAIF, Frontier Safety Framework) and monitoring capabilities, but no public evidence shows implementation of very frequent software updates and compliance monitoring specifically for AI model weights security as required by RAND's SL3."
                  },
                  "xAI": {
                    "score": 0,
                    "sources": [],
                    "justification": "No specific public information found about xAI's very frequent software update management and compliance monitoring practices related to AI model weights security."
                  },
                  "Meta": {
                    "score": 25,
                    "sources": [
                      "https://www.infosecurity-magazine.com/news/meta-new-advances-ai-security/",
                      "https://www.techrepublic.com/article/news-meta-ai-privacy-safety-risk-assessments/",
                      "https://www.rand.org/pubs/research_briefs/RBA2849-1.html"
                    ],
                    "justification": "While Meta has introduced security tools like LlamaFirewall, LlamaGuard 4, and CyberSecEval 4, they are automating 90% of risk assessments with AI rather than maintaining human oversight for security updates. The RAND report's Security Level 3 requires protection against cybercrime syndicates and insider threats with comprehensive monitoring, but Meta appears to prioritize speed over rigorous security compliance monitoring."
                  }
                }
              }
            ]
          },
          {
            "name": "Access, Permissions, and Credentials",
            "controls": [
              {
                "name": "802.1x authentication",
                "compliance": {
                  "OpenAI": {
                    "score": 0,
                    "sources": [
                      "https://www.rand.org/pubs/research_reports/RRA2849-1.html",
                      "https://web.swipeinsight.app/posts/openai-unveils-security-architecture-for-frontier-ai-model-training-7065",
                      "https://www.rand.org/pubs/research_briefs/RBA2849-1.html"
                    ],
                    "justification": "No specific public information found regarding OpenAI's implementation of 802.1x authentication for securing AI model weights. While OpenAI has published details about their security architecture including authentication frameworks and network controls, there is no explicit mention of 802.1x authentication implementation."
                  },
                  "Anthropic": {
                    "score": 0,
                    "sources": [],
                    "justification": "No specific public information found regarding Anthropic's implementation of 802.1x authentication for AI model weights security."
                  },
                  "Google": {
                    "score": 0,
                    "sources": [],
                    "justification": "No specific public information found regarding Google's implementation of 802.1x authentication for AI model weights security as described in RAND's Security Level 3."
                  },
                  "xAI": {
                    "score": 0,
                    "sources": [],
                    "justification": "No specific public information found regarding xAI's implementation of 802.1x authentication for AI model weights security."
                  },
                  "Meta": {
                    "score": 0,
                    "sources": [],
                    "justification": "No specific public information found regarding Meta's implementation of 802.1x authentication for AI model weights security."
                  }
                }
              },
              {
                "name": "Zero Trust architecture (adherence to at least the standards in the \"Advanced\" level of CISA's Zero Trust Maturity Model)",
                "compliance": {
                  "OpenAI": {
                    "score": 25,
                    "sources": [
                      "https://openai.com/index/reimagining-secure-infrastructure-for-advanced-ai/",
                      "https://web.swipeinsight.app/posts/openai-unveils-security-architecture-for-frontier-ai-model-training-7065",
                      "https://www.analyticsvidhya.com/blog/2024/05/openai-security-measures/"
                    ],
                    "justification": "OpenAI has published security measures for AI infrastructure including trusted computing for GPUs to encrypt model weights and network isolation, but no specific public documentation confirms adherence to CISA's Zero Trust Maturity Model Advanced level standards."
                  },
                  "Anthropic": {
                    "score": 50,
                    "sources": [
                      "https://www.anthropic.com/news/frontier-model-security",
                      "https://www.anthropic.com/news/activating-asl3-protections",
                      "https://www.anthropic.com/rsp-updates"
                    ],
                    "justification": "Anthropic has implemented multi-party authorization and time-bounded access controls for model weights under ASL-3, along with enhanced security measures including two-party control systems. However, no specific public information confirms full adherence to CISA's Zero Trust Maturity Model 'Advanced' level standards."
                  },
                  "Google": {
                    "score": 50,
                    "sources": [
                      "https://cloud.google.com/beyondcorp",
                      "https://workspace.google.com/blog/identity-and-security/accelerating-zero-trust-and-digital-sovereignty-ai",
                      "https://workspace.google.com/security/zero-trust/",
                      "https://cloud.google.com/architecture/framework/security/implement-zero-trust",
                      "https://cloud.google.com/blog/topics/public-sector/strengthening-federal-cybersecurity-cisa-zero-trust-and-google-workspace-exclusive-sessions-at-next-24/"
                    ],
                    "justification": "Google has implemented comprehensive Zero Trust principles aligned with CISA's framework including BeyondCorp, context-aware access, and AI-powered security controls. However, no specific public information was found directly addressing AI model weights security or RAND report Security Level 3 requirements."
                  },
                  "xAI": {
                    "score": 0,
                    "sources": [],
                    "justification": "No specific public information found regarding xAI's Zero Trust architecture implementation or compliance with CISA's Zero Trust Maturity Model standards for AI model weights security."
                  },
                  "Meta": {
                    "score": 0,
                    "sources": [],
                    "justification": "No specific public information found regarding Meta's compliance with CISA's Zero Trust Maturity Model 'Advanced' level standards for AI model weights security."
                  }
                }
              }
            ]
          },
          {
            "name": "Hardware",
            "controls": [
              {
                "name": "Security-minded hardware sourcing",
                "compliance": {
                  "OpenAI": {
                    "score": 50,
                    "sources": [
                      "https://openai.com/index/reimagining-secure-infrastructure-for-advanced-ai/",
                      "https://writings.flashbots.net/ZTEE",
                      "https://www.analyticsvidhya.com/blog/2024/05/openai-security-measures/"
                    ],
                    "justification": "OpenAI has proposed implementing trusted computing for AI accelerators (GPUs) to encrypt model weights until execution and called for security-minded hardware evolution, but there's no public evidence of full implementation of hardware security modules or secure hardware sourcing practices as described in RAND's Security Level 3."
                  },
                  "Anthropic": {
                    "score": 0,
                    "sources": [],
                    "justification": "No specific public information found regarding Anthropic's security-minded hardware sourcing practices, supplier vetting, or trusted hardware procurement processes."
                  },
                  "Google": {
                    "score": 75,
                    "sources": [
                      "https://cloud.google.com/blog/products/identity-security/mitigating-risk-in-the-hardware-supply-chain",
                      "https://cloud.google.com/distributed-cloud/hosted/docs/latest/gdch/security",
                      "https://cloud.google.com/docs/security/infrastructure/design"
                    ],
                    "justification": "Google demonstrates strong hardware sourcing security through custom-designed TPUs, Titan security chips, vetted component vendors, and rigorous supply chain controls. However, no specific public documentation was found detailing hardware sourcing practices specifically for AI model weights security as described in Security Level 3 of the RAND report."
                  },
                  "xAI": {
                    "score": 0,
                    "sources": [],
                    "justification": "No specific public information found regarding xAI's security-minded hardware sourcing practices or measures to protect AI model weights as outlined in the RAND report."
                  },
                  "Meta": {
                    "score": 0,
                    "sources": [],
                    "justification": "No specific public information found addressing Meta's security-minded hardware sourcing practices for AI model weights protection as defined in the RAND report."
                  }
                }
              }
            ]
          },
          {
            "name": "Supply Chain",
            "controls": [
              {
                "name": "Software inventory management",
                "compliance": {
                  "OpenAI": {
                    "score": 25,
                    "sources": [
                      "https://openai.com/global-affairs/our-approach-to-frontier-risk",
                      "https://openai.com/enterprise-privacy",
                      "https://trust.openai.com/"
                    ],
                    "justification": "OpenAI has SOC 2 Type 2 compliance and general security measures, but no specific public information found about software inventory management controls for AI model weights as described in the RAND report's Security Level 3 requirements."
                  },
                  "Anthropic": {
                    "score": 75,
                    "sources": [
                      "https://www.anthropic.com/news/frontier-model-security",
                      "https://www.anthropic.com/news/activating-asl3-protections",
                      "https://www.anthropic.com/rsp-updates"
                    ],
                    "justification": "Anthropic has publicly committed to implementing comprehensive software inventory management as part of their ASL-3 security measures, including automated scanning and strict tracking of all software components used in development and deployment."
                  },
                  "Google": {
                    "score": 50,
                    "sources": [
                      "https://cloud.google.com/use-cases/secure-ai-framework",
                      "https://cloud.google.com/blog/products/identity-security/introducing-ai-protection-security-for-the-ai-era",
                      "https://cloud.google.com/blog/products/identity-security/driving-secure-innovation-with-ai-google-unified-security-next25"
                    ],
                    "justification": "Google demonstrates AI inventory capabilities through AI Protection and SAIF framework, including automated discovery and cataloging of AI assets, models, and datasets. However, no specific public information directly addresses software inventory management for AI model weights at RAND Security Level 3 requirements."
                  },
                  "xAI": {
                    "score": 0,
                    "sources": [],
                    "justification": "No specific public information found regarding xAI's implementation of software inventory management controls for AI model weights security as described in RAND Security Level 3."
                  },
                  "Meta": {
                    "score": 0,
                    "sources": [
                      "https://www.securityweek.com/meta-releases-llama-ai-open-source-protection-tools/",
                      "https://www.rand.org/pubs/research_briefs/RBA2849-1.html",
                      "https://www.infosecurity-magazine.com/news/meta-new-advances-ai-security/",
                      "https://www.rand.org/pubs/research_reports/RRA2849-1.html",
                      "https://www.transformernews.ai/p/metas-ai-safeguards-are-an-elaborate"
                    ],
                    "justification": "No specific public information found regarding Meta's implementation of software inventory management controls for AI model weights as outlined in RAND's Security Level 3 requirements. While Meta has released security tools like LlamaFirewall and Llama Guard, these address different security aspects than inventory management, and Meta's open-source approach to Llama models fundamentally conflicts with centralized weight control."
                  }
                }
              },
              {
                "name": "Supply chain security is commensurate with the organization's security",
                "compliance": {
                  "OpenAI": {
                    "score": 25,
                    "sources": [
                      "https://openai.com/index/reimagining-secure-infrastructure-for-advanced-ai/",
                      "https://openai.com/policies/supplier-security-measures",
                      "https://www.analyticsvidhya.com/blog/2024/05/openai-security-measures/",
                      "https://www.rand.org/pubs/research_briefs/RBA2849-1.html"
                    ],
                    "justification": "OpenAI has published security guidelines for suppliers and acknowledges model weight security importance, but lacks specific public documentation on comprehensive supply chain security measures commensurate with Security Level 3 requirements (protection against cybercrime syndicates and insider threats) as defined in the RAND report."
                  },
                  "Anthropic": {
                    "score": 75,
                    "sources": [
                      "https://www.anthropic.com/news/frontier-model-security",
                      "https://www.anthropic.com/news/activating-asl3-protections",
                      "https://www.rand.org/pubs/research_briefs/RBA2849-1.html"
                    ],
                    "justification": "Anthropic has implemented ASL-3 security standards with over 100 security controls for model weight protection, including multi-party authorization, enhanced change management protocols, and references to NIST SSDF and SLSA standards for secure development, demonstrating substantial supply chain security measures aligned with RAND SL-3 requirements."
                  },
                  "Google": {
                    "score": 0,
                    "sources": [],
                    "justification": "No specific public information found addressing Google's supply chain security for AI model weights being commensurate with organizational security as described in RAND's Security Level 3."
                  },
                  "xAI": {
                    "score": 0,
                    "sources": [],
                    "justification": "No specific public information found regarding xAI's supply chain security measures or practices related to AI model weights protection that would meet Security Level 3 requirements as defined in the RAND report."
                  },
                  "Meta": {
                    "score": 0,
                    "sources": [],
                    "justification": "No specific public information found regarding Meta's implementation of supply chain security controls commensurate with organizational security requirements for AI model weights as described in Security Level 3 of the RAND report."
                  }
                }
              }
            ]
          },
          {
            "name": "Security Tooling",
            "controls": [
              {
                "name": "Enforcement of security policies through code rather than manual compliance",
                "compliance": {
                  "OpenAI": {
                    "score": 25,
                    "sources": [
                      "https://openai.com/policies/usage-policies",
                      "https://www.rand.org/events/2024/07/securing-ai.html",
                      "https://www.rand.org/pubs/research_briefs/RBA2849-1.html"
                    ],
                    "justification": "OpenAI uses automated systems for policy enforcement in some areas (content moderation, usage policy violations) but lacks specific public information about automated enforcement for model weights security infrastructure as required by RAND Security Level 3."
                  },
                  "Anthropic": {
                    "score": 75,
                    "sources": [
                      "https://www.anthropic.com/news/announcing-our-updated-responsible-scaling-policy",
                      "https://www.anthropic.com/news/activating-asl3-protections",
                      "https://www.anthropic.com/rsp-updates"
                    ],
                    "justification": "Anthropic has implemented automated security controls under ASL-3 including binary authorization for endpoints, automated software scanning, egress bandwidth controls, and real-time monitoring systems that enforce security policies through code rather than manual processes."
                  },
                  "Google": {
                    "score": 25,
                    "sources": [
                      "https://blog.google/technology/safety-security/introducing-googles-secure-ai-framework/",
                      "https://cloud.google.com/use-cases/secure-ai-framework",
                      "https://cloud.google.com/blog/products/identity-security/introducing-ai-protection-security-for-the-ai-era",
                      "https://safety.google/cybersecurity-advancements/saif/"
                    ],
                    "justification": "Google shows some automated security controls through SAIF framework and Assured Workloads, but lacks specific public information about code-based policy enforcement for AI model weights security."
                  },
                  "xAI": {
                    "score": 0,
                    "sources": [],
                    "justification": "No specific public information found about xAI's implementation of automated security policy enforcement through code for AI model weights protection."
                  },
                  "Meta": {
                    "score": 0,
                    "sources": [],
                    "justification": "No specific public information found about Meta's implementation of automated/code-based security policy enforcement for AI model weights as described in RAND's Security Level 3."
                  }
                }
              },
              {
                "name": "Security policy enforcement for network access across devices",
                "compliance": {
                  "OpenAI": {
                    "score": 50,
                    "sources": [
                      "https://openai.com/policies/supplier-security-measures",
                      "https://web.swipeinsight.app/posts/openai-unveils-security-architecture-for-frontier-ai-model-training-7065",
                      "https://trust.openai.com/",
                      "https://www.analyticsvidhya.com/blog/2024/05/openai-security-measures/"
                    ],
                    "justification": "OpenAI has published details about their security architecture (June 2024) implementing multi-layered controls including network egress controls, role-based access control, and private-linked storage resources. However, no specific public information addresses comprehensive device-level network access policies matching RAND SL3 requirements."
                  },
                  "Anthropic": {
                    "score": 75,
                    "sources": [
                      "https://www.techrepublic.com/article/news-anthropic-ai-safety-level-3/",
                      "https://www.anthropic.com/news/announcing-our-updated-responsible-scaling-policy",
                      "https://www.anthropic.com/news/activating-asl3-protections",
                      "https://www.rand.org/pubs/research_briefs/RBA2849-1.html"
                    ],
                    "justification": "Anthropic has implemented ASL-3 security standards including over 100 security controls, egress bandwidth controls to restrict data flow from secure environments, two-party authorization for model weight access, and enhanced endpoint controls, demonstrating strong alignment with RAND's Security Level 3 requirements for protecting against cybercrime syndicates and insider threats."
                  },
                  "Google": {
                    "score": 50,
                    "sources": [
                      "https://blog.google/technology/safety-security/introducing-googles-secure-ai-framework/",
                      "https://safety.google/cybersecurity-advancements/saif/",
                      "https://cloud.google.com/blog/products/identity-security/introducing-ai-protection-security-for-the-ai-era",
                      "https://www.rand.org/pubs/research_briefs/RBA2849-1.html"
                    ],
                    "justification": "Google has implemented foundational AI security through SAIF framework and AI Protection capabilities, but lacks specific public documentation on network access controls across devices for AI model weights protection as required by RAND SL3."
                  },
                  "xAI": {
                    "score": 25,
                    "sources": [
                      "https://www.oracle.com/artificial-intelligence/ai-open-weights-models/",
                      "https://x.ai/security",
                      "https://www.rand.org/pubs/research_briefs/RBA2849-1.html"
                    ],
                    "justification": "xAI has basic security infrastructure including firewalls, network access controls, and IAM, but no specific public information confirms implementation of Security Level 3 requirements for model weights protection such as centralized weight storage or device-specific access controls as outlined in the RAND report."
                  },
                  "Meta": {
                    "score": 25,
                    "sources": [
                      "https://www.infosecurity-magazine.com/news/meta-new-advances-ai-security/",
                      "https://thehackernews.com/2025/04/meta-launches-llamafirewall-framework.html",
                      "https://www.rand.org/pubs/research_briefs/RBA2849-1.html"
                    ],
                    "justification": "While Meta has introduced security tools like LlamaFirewall and emphasizes open-source model security, there is limited public information specifically addressing network access control and segmentation policies for protecting AI model weights infrastructure as required by RAND Security Level 3."
                  }
                }
              }
            ]
          }
        ]
      },
      {
        "name": "Personnel Security",
        "subcategories": [
          {
            "name": "Awareness and Training",
            "controls": [
              {
                "name": "Employee awareness of weight interaction monitoring",
                "compliance": {
                  "OpenAI": {
                    "score": 25,
                    "sources": [
                      "https://openai.com/careers/insider-risk-investigator",
                      "https://openai.com/global-affairs/our-approach-to-frontier-risk",
                      "https://www.ccn.com/news/technology/openai-hiring-insider-risk-investigator-collaborating-white-house/",
                      "https://web.swipeinsight.app/posts/openai-unveils-security-architecture-for-frontier-ai-model-training-7065"
                    ],
                    "justification": "OpenAI has committed to investing in insider threat safeguards, is hiring insider risk investigators, and has implemented multi-layered controls to protect model weights. However, no specific public information was found regarding employee awareness training about weight interaction monitoring."
                  },
                  "Anthropic": {
                    "score": 50,
                    "sources": [
                      "https://privacy.anthropic.com/en/articles/10458704-how-does-anthropic-protect-the-personal-data-of-claude-ai-users",
                      "https://www.anthropic.com/news/activating-asl3-protections",
                      "https://www.anthropic.com/rsp-updates"
                    ],
                    "justification": "Anthropic has implemented access monitoring for model weights with automated detections and manual monitoring, educates employees on insider risk with a structured insider threat program, and provides annual security training. However, no specific public information confirms employee awareness of weight interaction monitoring systems."
                  },
                  "Google": {
                    "score": 0,
                    "sources": [],
                    "justification": "No specific public information found about Google's employee awareness programs or training related to weight interaction monitoring for AI models."
                  },
                  "xAI": {
                    "score": 25,
                    "sources": [
                      "https://x.ai/news/grok-os",
                      "https://research.contrary.com/company/xai",
                      "https://x.ai/security"
                    ],
                    "justification": "xAI has general security training mentioned on their security page, but no specific public information about employee awareness training regarding model weight interaction monitoring. While they open-sourced Grok weights, this poses additional security concerns rather than demonstrating controlled access monitoring."
                  },
                  "Meta": {
                    "score": 0,
                    "sources": [],
                    "justification": "No specific public information found regarding Meta's implementation of employee awareness programs or monitoring systems for AI model weight interactions as required for Security Level 3."
                  }
                }
              },
              {
                "name": "Security training for employees (not necessarily only those with access)",
                "compliance": {
                  "OpenAI": {
                    "score": 50,
                    "sources": [
                      "https://venturebeat.com/ai/why-anthropic-and-openai-are-obsessed-with-securing-llm-model-weights/",
                      "https://web.swipeinsight.app/posts/openai-unveils-security-architecture-for-frontier-ai-model-training-7065",
                      "https://openai.com/security/"
                    ],
                    "justification": "OpenAI has published information about their security architecture including multi-party access controls and role-based authorization for model weights, and mentions that they invest in security and have dedicated research environments. However, no specific public information was found detailing their employee security training programs related to model weights protection as would be expected for Security Level 3."
                  },
                  "Anthropic": {
                    "score": 50,
                    "sources": [
                      "https://venturebeat.com/ai/why-anthropic-and-openai-are-obsessed-with-securing-llm-model-weights/",
                      "https://www.anthropic.com/careers",
                      "https://www.anthropic.com/news/activating-asl3-protections",
                      "https://forum.effectivealtruism.org/posts/fJycPfYuBHKoai98q/ai-companies-are-not-on-track-to-secure-model-weights"
                    ],
                    "justification": "Anthropic has implemented ASL-3 security standards with over 100 security controls to protect model weights and has a security team of several dozen people led by Jason Clinton. While they emphasize security as a collective responsibility and work with DEF CON to train security engineers, no specific public information was found detailing comprehensive security training programs for all employees regarding model weights protection."
                  },
                  "Google": {
                    "score": 0,
                    "sources": [],
                    "justification": "No specific public information found regarding Google's security training for employees related to AI model weights security as expected for Security Level 3."
                  },
                  "xAI": {
                    "score": 75,
                    "sources": [
                      "https://startup.jobs/security-operations-lead-xai-5610530",
                      "https://x.ai/security"
                    ],
                    "justification": "xAI requires annual security and privacy training for all employees covering security policies, best practices, and privacy principles. Additional training exists for data center staff who undergo comprehensive background checks and security training."
                  },
                  "Meta": {
                    "score": 0,
                    "sources": [],
                    "justification": "No specific public information found regarding Meta's security training for employees related to AI model weights security as expected for Security Level 3 in the RAND report."
                  }
                }
              },
              {
                "name": "Security risk reporting program",
                "compliance": {
                  "OpenAI": {
                    "score": 75,
                    "sources": [
                      "https://openai.com/global-affairs/our-approach-to-frontier-risk",
                      "https://www.bleepingcomputer.com/news/security/openai-now-pays-researchers-100-000-for-critical-vulnerabilities/",
                      "https://openai.com/policies/coordinated-vulnerability-disclosure-policy",
                      "https://bugcrowd.com/openai"
                    ],
                    "justification": "OpenAI has established a comprehensive bug bounty program through Bugcrowd with rewards up to $100,000 for critical vulnerabilities, provides multiple channels for vulnerability reporting including disclosure@openai.com, and actively invests in cybersecurity and insider threat safeguards to protect model weights."
                  },
                  "Anthropic": {
                    "score": 75,
                    "sources": [
                      "https://www.anthropic.com/voluntary-commitments",
                      "https://www.anthropic.com/news/model-safety-bug-bounty",
                      "https://www.anthropic.com/responsible-disclosure-policy",
                      "https://www.anthropic.com/news/testing-our-safety-defenses-with-a-new-bug-bounty-program"
                    ],
                    "justification": "Anthropic has established comprehensive security risk reporting programs including a Responsible Disclosure Policy, multiple bug bounty programs through HackerOne (offering up to $25,000 for critical vulnerabilities), and dedicated channels for reporting safety issues, demonstrating strong compliance with Security Level 3 expectations for vulnerability reporting systems."
                  },
                  "Google": {
                    "score": 50,
                    "sources": [
                      "https://opensource.googleblog.com/2025/01/creating-safe-secure-ai-models.html",
                      "https://ai.google/responsibility/safety/",
                      "https://saif.google/secure-ai-framework/risks",
                      "https://safety.google/cybersecurity-advancements/saif/"
                    ],
                    "justification": "Google has established security frameworks (SAIF), bug bounty programs covering AI products, and provides guidance on vulnerability reporting for AI models, but lacks specific public documentation of a dedicated reporting program focused on AI model weights security as described in RAND's Security Level 3."
                  },
                  "xAI": {
                    "score": 50,
                    "sources": [
                      "https://securityboulevard.com/2025/05/xai-secret-leak-the-story-of-a-disclosure/",
                      "https://x.ai/security"
                    ],
                    "justification": "xAI has implemented a public bug bounty program through HackerOne for vulnerability disclosure and maintains a vulnerabilities@x.ai email for security reports, demonstrating a functional security risk reporting framework, though no specific evidence was found regarding AI model weights security protocols."
                  },
                  "Meta": {
                    "score": 25,
                    "sources": [
                      "https://bugbounty.meta.com/scope/",
                      "https://thehackernews.com/2025/01/metas-llama-framework-flaw-exposes-ai.html",
                      "https://www.oligo.security/blog/cve-2024-50050-critical-vulnerability-in-meta-llama-llama-stack"
                    ],
                    "justification": "Meta has a general bug bounty program and recently disclosed vulnerabilities in Llama frameworks show cooperative security response, but no specific evidence found of a comprehensive security risk reporting program specifically focused on AI model weights security as outlined in RAND's Security Level 3 requirements."
                  }
                }
              }
            ]
          },
          {
            "name": "Filtering and Monitoring",
            "controls": [
              {
                "name": "Insider threat program",
                "compliance": {
                  "OpenAI": {
                    "score": 50,
                    "sources": [
                      "https://openai.com/global-affairs/our-approach-to-frontier-risk",
                      "https://openai.com/careers/security-engineer-insider-threat-detection-and-response/",
                      "https://openai.com/careers/insider-risk-investigator"
                    ],
                    "justification": "OpenAI has publicly acknowledged investing in 'insider threat safeguards' and is actively hiring for insider threat roles, but lacks detailed public documentation of comprehensive insider threat program implementation that would fully meet RAND Security Level 3 requirements."
                  },
                  "Anthropic": {
                    "score": 50,
                    "sources": [
                      "https://www.anthropic.com/news/reflections-on-our-responsible-scaling-policy",
                      "https://www.anthropic.com/news/activating-asl3-protections",
                      "https://www.anthropic.com/rsp-updates"
                    ],
                    "justification": "Anthropic has publicly disclosed establishing a structured insider threat program as part of ASL-3, including multi-party authorization for model weights access and employee education on insider risk. However, implementation details against sophisticated insiders remain limited in public documentation."
                  },
                  "Google": {
                    "score": 0,
                    "sources": [],
                    "justification": "No specific public information found about Google implementing insider threat programs specifically for AI model weights security as described in RAND's Security Level 3."
                  },
                  "xAI": {
                    "score": 0,
                    "sources": [],
                    "justification": "No specific public information found regarding xAI's insider threat program related to AI model weights security."
                  },
                  "Meta": {
                    "score": 0,
                    "sources": [],
                    "justification": "No specific public information found about Meta implementing an insider threat program for AI model weights security as outlined in RAND's Security Level 3 requirements."
                  }
                }
              }
            ]
          }
        ]
      },
      {
        "name": "Security Assurance and Testing",
        "subcategories": [
          {
            "name": "Red-Teaming and Penetration Testing",
            "controls": [
              {
                "name": "Ongoing penetration testing",
                "compliance": {
                  "OpenAI": {
                    "score": 75,
                    "sources": [
                      "https://web.swipeinsight.app/posts/openai-unveils-security-architecture-for-frontier-ai-model-training-7065",
                      "https://openai.com/security/"
                    ],
                    "justification": "OpenAI conducts annual third-party penetration testing for their API and has implemented red team testing for security controls. Third-party security consultancies have performed penetration tests on their infrastructure protecting model weights."
                  },
                  "Anthropic": {
                    "score": 75,
                    "sources": [
                      "https://www.techrepublic.com/article/news-anthropic-ai-safety-level-3/",
                      "https://www.anthropic.com/news/activating-asl3-protections",
                      "https://www.anthropic.com/rsp-updates"
                    ],
                    "justification": "Anthropic explicitly mentions 'Red teaming and penetration testing: Partner with a diverse range of external red team and penetration testing experts' as part of their ASL-3 security controls for model weights protection, and has implemented bug bounty programs focused on stress-testing their security systems."
                  },
                  "Google": {
                    "score": 50,
                    "sources": [
                      "https://deepmind.google/discover/blog/advancing-geminis-security-safeguards/",
                      "https://safety.google/cybersecurity-advancements/saif/",
                      "https://deepmind.google/discover/blog/taking-a-responsible-path-to-agi/"
                    ],
                    "justification": "Google demonstrates automated red teaming (ART) for Gemini models and has security frameworks (SAIF, Frontier Safety), but lacks specific public evidence of regular penetration testing focused on model weights protection as described in RAND's Security Level 3."
                  },
                  "xAI": {
                    "score": 0,
                    "sources": [],
                    "justification": "No specific public information found regarding xAI's implementation of ongoing penetration testing for AI model weights security as defined in RAND's Security Level 3."
                  },
                  "Meta": {
                    "score": 0,
                    "sources": [],
                    "justification": "No specific public information found regarding Meta's implementation of ongoing penetration testing focused on AI model weights security as required for RAND Security Level 3."
                  }
                }
              },
              {
                "name": "Penetration testing of physical access and facility security",
                "compliance": {
                  "OpenAI": {
                    "score": 25,
                    "sources": [
                      "https://transparency.oecd.ai/reports/b167db92-67c8-47d8-966a-427e2ce8c008",
                      "https://web.swipeinsight.app/posts/openai-unveils-security-architecture-for-frontier-ai-model-training-7065",
                      "https://openai.com/security/"
                    ],
                    "justification": "OpenAI mentions third-party penetration testing and red teaming, but no specific public information addresses penetration testing of physical facility security. They reference 'advanced physical security' for datacenters protecting model weights but provide no details about physical penetration testing practices."
                  },
                  "Anthropic": {
                    "score": 50,
                    "sources": [
                      "https://www.cnbc.com/2025/03/31/anthropic-announces-updates-on-security-safeguards-for-its-ai-models.html",
                      "https://www.anthropic.com/news/activating-asl3-protections",
                      "https://www.anthropic.com/rsp-updates"
                    ],
                    "justification": "Anthropic has implemented Technical Surveillance Countermeasures (TSCM) at physical spaces and mentions red teaming/penetration testing partnerships, but no specific evidence of comprehensive penetration testing of physical access controls and facility security was found in public information."
                  },
                  "Google": {
                    "score": 0,
                    "sources": [],
                    "justification": "No specific public information found regarding Google's penetration testing of physical access and facility security controls for AI model weights, as would be expected for RAND Security Level 3."
                  },
                  "xAI": {
                    "score": 0,
                    "sources": [],
                    "justification": "No specific public information found regarding xAI's implementation of penetration testing for physical access and facility security related to AI model weights security."
                  },
                  "Meta": {
                    "score": 0,
                    "sources": [],
                    "justification": "No specific public information found about Meta's penetration testing of physical access and facility security for AI model weights protection."
                  }
                }
              },
              {
                "name": "Advanced red-teaming: Elite external team",
                "compliance": {
                  "OpenAI": {
                    "score": 75,
                    "sources": [
                      "https://arxiv.org/html/2503.16431v1",
                      "https://hiddenlayer.com/innovation-hub/three-distinct-categories-of-ai-red-teaming/",
                      "https://openai.com/index/red-teaming-network/",
                      "https://venturebeat.com/ai/openai-red-team-innovations-new-essentials-security-leaders/"
                    ],
                    "justification": "OpenAI has established a comprehensive external red teaming program including the OpenAI Red Teaming Network with over 100 external experts for GPT-4, and published detailed methodologies. While they demonstrate strong external red teaming practices, there's no specific public information confirming an 'elite' team explicitly focused on model weights security as described in RAND's Security Level 3."
                  },
                  "Anthropic": {
                    "score": 75,
                    "sources": [
                      "https://www.anthropic.com/news/frontier-threats-red-teaming-for-ai-safety",
                      "https://www.anthropic.com/news/strategic-warning-for-ai-risk-progress-and-insights-from-our-frontier-red-team",
                      "https://www.rand.org/pubs/research_reports/RRA2849-1.html",
                      "https://www.anthropic.com/news/challenges-in-red-teaming-ai-systems"
                    ],
                    "justification": "Anthropic demonstrates substantial compliance through documented partnerships with elite external experts including biosecurity experts, NNSA for nuclear domain, and organizations like Thorn, Institute for Strategic Dialogue, and Global Project Against Hate and Extremism for specialized red-teaming across multiple risk domains."
                  },
                  "Google": {
                    "score": 0,
                    "sources": [
                      "https://www.helpnetsecurity.com/2023/08/03/daniel-fabian-google-ai-red-team/",
                      "https://www.rand.org/pubs/research_reports/RRA2849-1.html",
                      "https://blog.google/technology/safety-security/googles-ai-red-team-the-ethical-hackers-making-ai-safer/"
                    ],
                    "justification": "While Google has a sophisticated internal AI Red Team that simulates various adversaries and attack scenarios, there is no specific public information about Google employing elite external red teams specifically for AI model weights security as required by RAND's Security Level 3."
                  },
                  "xAI": {
                    "score": 0,
                    "sources": [
                      "https://x.ai/legal/faq",
                      "https://www.rand.org/pubs/research_reports/RRA2849-1.html",
                      "https://news.lavx.hu/article/grok-3-jailbreak-exposes-alarming-ai-security-flaws",
                      "https://embracethered.com/blog/posts/2024/security-probllms-in-xai-grok/"
                    ],
                    "justification": "No evidence found of xAI engaging elite external red teams for comprehensive security testing. Independent security assessments revealed significant vulnerabilities that xAI dismissed, indicating lack of systematic external red-teaming program meeting Security Level 3 standards."
                  },
                  "Meta": {
                    "score": 25,
                    "sources": [
                      "https://www.rand.org/pubs/research_briefs/RBA2849-1.html",
                      "https://venturebeat.com/security/red-team-ai-now-to-build-safer-smarter-models-tomorrow/",
                      "https://mindgard.ai/blog/what-is-ai-red-teaming",
                      "https://www.gofully.ai/blogs/meta-red-teaming-event"
                    ],
                    "justification": "Meta demonstrates some red teaming practices including external partnerships (Fully AI) and automated testing (MART), but no specific evidence of elite external teams for AI model weights security as described in RAND's Security Level 3."
                  }
                }
              },
              {
                "name": "Advanced red-teaming: Substantial funding",
                "compliance": {
                  "OpenAI": {
                    "score": 75,
                    "sources": [
                      "https://analyticsindiamag.com/openai-banks-on-red-team-to-win-the-moral-battle/",
                      "https://openai.com/index/red-teaming-network/",
                      "https://venturebeat.com/ai/openai-red-team-innovations-new-essentials-security-leaders/"
                    ],
                    "justification": "OpenAI demonstrates substantial funding commitment through compensating all Red Teaming Network participants, employing over 100 external red teamers for GPT-4, and expanding to a formal network approach. While specific budget figures aren't disclosed, the scale and breadth indicate significant financial investment."
                  },
                  "Anthropic": {
                    "score": 75,
                    "sources": [
                      "https://www.anthropic.com/news/frontier-threats-red-teaming-for-ai-safety",
                      "https://www.anthropic.com/news/announcing-our-updated-responsible-scaling-policy",
                      "https://www.anthropic.com/news/activating-asl3-protections",
                      "https://www.anthropic.com/news/challenges-in-red-teaming-ai-systems"
                    ],
                    "justification": "Anthropic demonstrates substantial funding and investment in advanced red-teaming with over 150 hours of expert red-teaming, dedicated Frontier Red Team, partnerships with government agencies, and has implemented ASL-3 security standards specifically designed to protect model weights. The company raised $3.5B in funding and activated enhanced security measures for Claude Opus 4."
                  },
                  "Google": {
                    "score": 75,
                    "sources": [
                      "https://council.aimresearch.co/comparing-major-companies-ai-spending-in-2024-and-the-challenge-of-productionizing-ai-solutions/",
                      "https://cloud.google.com/blog/transform/prompt-findings-our-ai-red-teams-first-report-qa",
                      "https://cloud.google.com/transform/how-google-does-it-red-teaming-at-scale",
                      "https://blog.google/technology/safety-security/googles-ai-red-team-the-ethical-hackers-making-ai-safer/"
                    ],
                    "justification": "Google has established a dedicated AI Red Team that conducts sophisticated adversarial testing including model exfiltration and backdooring attacks. While specific red team budget figures aren't disclosed, Google's $120+ billion AI investment and emphasized commitment to red teaming indicates substantial funding for this critical security function."
                  },
                  "xAI": {
                    "score": 0,
                    "sources": [],
                    "justification": "No specific public information found regarding xAI's substantial funding for advanced red-teaming focused on AI model weights security as described in RAND's Security Level 3 requirements."
                  },
                  "Meta": {
                    "score": 0,
                    "sources": [],
                    "justification": "No specific public information found regarding Meta's substantial funding for advanced red teaming related to AI model weights security."
                  }
                }
              },
              {
                "name": "Advanced red-teaming: Access to design and code",
                "compliance": {
                  "OpenAI": {
                    "score": 50,
                    "sources": [
                      "https://arxiv.org/html/2503.16431v1",
                      "https://www.rand.org/pubs/research_briefs/RBA2849-1.html",
                      "https://openai.com/global-affairs/our-approach-to-frontier-risk",
                      "https://www.oracle.com/artificial-intelligence/ai-open-weights-models/",
                      "https://www.rand.org/pubs/research_reports/RRA2849-1.html",
                      "https://openai.com/index/red-teaming-network/",
                      "https://web.swipeinsight.app/posts/openai-unveils-security-architecture-for-frontier-ai-model-training-7065"
                    ],
                    "justification": "OpenAI has robust external red-teaming programs and engages over 100 external experts, but does not appear to provide red teams with direct access to model design, architecture code, or weights, instead focusing on API-based testing and system-level assessments."
                  },
                  "Anthropic": {
                    "score": 50,
                    "sources": [
                      "https://www.anthropic.com/news/frontier-threats-red-teaming-for-ai-safety",
                      "https://www.anthropic.com/news/strategic-warning-for-ai-risk-progress-and-insights-from-our-frontier-red-team",
                      "https://www.rand.org/pubs/research_reports/RRA2849-1.html",
                      "https://www.anthropic.com/news/activating-asl3-protections",
                      "https://www.anthropic.com/news/challenges-in-red-teaming-ai-systems"
                    ],
                    "justification": "Anthropic has demonstrated substantial red-teaming practices including domain-specific expert red teaming, third-party partnerships with government agencies (US/UK AI Safety Institutes, NNSA), and internal frontier red teams. However, there is no specific public information confirming that external red teams have access to model design details and code as required for RAND Security Level 3."
                  },
                  "Google": {
                    "score": 50,
                    "sources": [
                      "https://cloud.google.com/blog/transform/prompt-findings-our-ai-red-teams-first-report-qa",
                      "https://www.rand.org/pubs/research_reports/RRA2849-1.html",
                      "https://safety.google/cybersecurity-advancements/saif/",
                      "https://blog.google/technology/safety-security/googles-ai-red-team-the-ethical-hackers-making-ai-safer/"
                    ],
                    "justification": "Google has established AI red-teaming practices including testing model security controls and has published reports on their AI Red Team's work. However, specific public information about advanced red-teaming with direct access to design and code for model weights security at Security Level 3 standards is limited."
                  },
                  "xAI": {
                    "score": 0,
                    "sources": [],
                    "justification": "No specific public information found indicating xAI conducts advanced red-teaming with access to design and code as required for Security Level 3."
                  },
                  "Meta": {
                    "score": 0,
                    "sources": [],
                    "justification": "No specific public information found directly addressing Meta's implementation of advanced red-teaming with access to design and code for model weights security as specified in the RAND report's Security Level 3 requirements."
                  }
                }
              },
              {
                "name": "Advanced red-teaming: Testing insider threats",
                "compliance": {
                  "OpenAI": {
                    "score": 50,
                    "sources": [
                      "https://openai.com/global-affairs/our-approach-to-frontier-risk",
                      "https://openai.com/careers/security-engineer-insider-threat-detection-and-response/",
                      "https://web.swipeinsight.app/posts/openai-unveils-security-architecture-for-frontier-ai-model-training-7065"
                    ],
                    "justification": "OpenAI publicly states they are 'continuing to invest in cybersecurity and insider threat safeguards to protect proprietary and unreleased model weights' and mentions implementing insider threat programs, but provides no specific details about advanced red-teaming exercises that test insider threats scenarios."
                  },
                  "Anthropic": {
                    "score": 75,
                    "sources": [
                      "https://www.anthropic.com/news/reflections-on-our-responsible-scaling-policy",
                      "https://www.anthropic.com/research/agentic-misalignment",
                      "https://www.anthropic.com/news/activating-asl3-protections",
                      "https://www.anthropic.com/rsp-updates"
                    ],
                    "justification": "Anthropic has implemented comprehensive insider threat testing under ASL-3, including multi-party authorization, time-bounded access controls, and explicit red teaming simulations of insider threat scenarios. Their threat modeling identifies insider device compromise as the highest risk vector."
                  },
                  "Google": {
                    "score": 75,
                    "sources": [
                      "https://www.rand.org/pubs/research_briefs/RBA2849-1.html",
                      "https://cloud.google.com/blog/transform/prompt-findings-our-ai-red-teams-first-report-qa",
                      "https://blog.google/technology/safety-security/googles-ai-red-team-the-ethical-hackers-making-ai-safer/",
                      "https://mindgard.ai/blog/googles-red-team"
                    ],
                    "justification": "Google has established a dedicated AI Red Team that explicitly simulates malicious insiders and conducts advanced adversarial testing, demonstrating strong capabilities aligned with Security Level 3 requirements. While Google hasn't publicly detailed specific insider threat programs for model weights, their comprehensive red-teaming approach addresses insider risks as a core threat vector."
                  },
                  "xAI": {
                    "score": 0,
                    "sources": [],
                    "justification": "No specific public information found about xAI conducting advanced red-teaming exercises specifically for testing insider threats related to AI model weights security as described in the RAND report's Security Level 3."
                  },
                  "Meta": {
                    "score": 25,
                    "sources": [
                      "https://ailabwatch.org/companies/meta",
                      "https://github.com/meta-llama/PurpleLlama",
                      "https://www.securityweek.com/meta-releases-llama-ai-open-source-protection-tools/"
                    ],
                    "justification": "Meta has released Purple Llama security tools including CyberSecEval for testing AI vulnerabilities, but no specific public information was found demonstrating advanced red-teaming programs that explicitly test insider threats for model weights security as required by RAND Security Level 3."
                  }
                }
              },
              {
                "name": "Advanced red-teaming: Expanded access",
                "compliance": {
                  "OpenAI": {
                    "score": 75,
                    "sources": [
                      "https://openai.com/global-affairs/our-approach-to-frontier-risk",
                      "https://hiddenlayer.com/innovation-hub/three-distinct-categories-of-ai-red-teaming/",
                      "https://openai.com/index/red-teaming-network/"
                    ],
                    "justification": "OpenAI has established a formal Red Teaming Network with external experts and conducts internal/external red teaming for security controls. They explicitly state they do not distribute model weights outside OpenAI and Microsoft, maintaining strict API-only access for third parties."
                  },
                  "Anthropic": {
                    "score": 75,
                    "sources": [
                      "https://www.rand.org/pubs/research_briefs/RBA2849-1.html",
                      "https://www.anthropic.com/news/frontier-threats-red-teaming-for-ai-safety",
                      "https://www.anthropic.com/news/strategic-warning-for-ai-risk-progress-and-insights-from-our-frontier-red-team",
                      "https://www.anthropic.com/news/activating-asl3-protections",
                      "https://www.anthropic.com/news/challenges-in-red-teaming-ai-systems"
                    ],
                    "justification": "Anthropic demonstrates substantial compliance through extensive red-teaming programs with external experts across multiple domains (biosecurity, cybersecurity, CBRN), partnerships with trusted third parties, and implementation of ASL-3 security standards that include advanced red-teaming measures."
                  },
                  "Google": {
                    "score": 25,
                    "sources": [
                      "https://www.rand.org/pubs/research_briefs/RBA2849-1.html",
                      "https://www.rand.org/pubs/research_reports/RRA2849-1.html",
                      "https://blog.google/technology/safety-security/googles-ai-red-team-the-ethical-hackers-making-ai-safer/",
                      "https://mindgard.ai/blog/googles-red-team"
                    ],
                    "justification": "While Google has robust internal AI red teaming capabilities that simulate advanced threats including insider threats, their red team is internally-facing only. They collaborate with external researchers through VRP and Project Zero, but there's no evidence of expanded third-party red team access specifically for AI model weights security as required by RAND's Security Level 3."
                  },
                  "xAI": {
                    "score": 0,
                    "sources": [],
                    "justification": "No specific public information found about xAI's advanced red-teaming practices with expanded access or their compliance with Security Level 3 requirements for AI model weights security."
                  },
                  "Meta": {
                    "score": 50,
                    "sources": [
                      "https://arxiv.org/html/2410.01606v1",
                      "https://www.rand.org/pubs/research_reports/RRA2849-1.html",
                      "https://www.rand.org/news/press/2024/05/30.html",
                      "https://mindgard.ai/blog/what-is-ai-red-teaming"
                    ],
                    "justification": "Meta demonstrates active red teaming practices including vulnerability discovery (CVE-2024-50050), published research on automated red teaming (GOAT), and contributes to industry knowledge sharing. However, no specific public information confirms implementation of expanded third-party red teaming access as required for RAND Security Level 3."
                  }
                }
              },
              {
                "name": "Advanced red-teaming: Attention to the weights and authentication",
                "compliance": {
                  "OpenAI": {
                    "score": 50,
                    "sources": [
                      "https://www.rand.org/pubs/research_briefs/RBA2849-1.html",
                      "https://openai.com/global-affairs/our-approach-to-frontier-risk",
                      "https://openai.com/index/red-teaming-network/",
                      "https://venturebeat.com/ai/openai-red-team-innovations-new-essentials-security-leaders/",
                      "https://cdn.openai.com/papers/openais-approach-to-external-red-teaming.pdf"
                    ],
                    "justification": "OpenAI has established comprehensive red-teaming programs including external red-teaming networks with over 100 experts, but there is no specific public information confirming advanced red-teaming focused on model weights security and authentication systems as required for Security Level 3."
                  },
                  "Anthropic": {
                    "score": 75,
                    "sources": [
                      "https://www.anthropic.com/news/frontier-threats-red-teaming-for-ai-safety",
                      "https://www.anthropic.com/news/activating-asl3-protections",
                      "https://www.anthropic.com/rsp-updates"
                    ],
                    "justification": "Anthropic has implemented ASL-3 security standards including multi-party authorization for model weight access, hardware authentication device requirements, and temporary access controls. Their advanced red-teaming covers model weights security through frontier threats evaluation and collaboration with government agencies."
                  },
                  "Google": {
                    "score": 75,
                    "sources": [
                      "https://saif.google/secure-ai-framework/components",
                      "https://www.saif.google/secure-ai-framework/controls",
                      "https://blog.google/technology/safety-security/googles-ai-red-team-the-ethical-hackers-making-ai-safer/",
                      "https://saif.google/secure-ai-framework/risks",
                      "https://cloud.google.com/use-cases/secure-ai-framework"
                    ],
                    "justification": "Google demonstrates strong advanced red-teaming practices with explicit attention to model weights security through SAIF controls including 'robust access controls and integrity management for model code and weights' and 'extensive access controls to prevent unauthorized copying or reading' with authentication measures, though specific details on advanced authentication mechanisms for weights access are not publicly disclosed."
                  },
                  "xAI": {
                    "score": 25,
                    "sources": [
                      "https://blog.gitguardian.com/xai-secret-leak-disclosure/",
                      "https://futurism.com/elon-musk-new-grok-ai-vulnerable-jailbreak-hacking",
                      "https://x.ai/security",
                      "https://embracethered.com/blog/posts/2024/security-probllms-in-xai-grok/"
                    ],
                    "justification": "While xAI has a bug bounty program and claims to implement security measures, there is no specific public evidence of advanced red-teaming focused on model weights security, and recent vulnerabilities suggest significant gaps in their security posture."
                  },
                  "Meta": {
                    "score": 25,
                    "sources": [
                      "https://about.fb.com/news/2024/11/open-source-ai-america-global-security/",
                      "https://mindgard.ai/blog/what-is-ai-red-teaming",
                      "https://www.rand.org/pubs/research_briefs/RBA2849-1.html"
                    ],
                    "justification": "While Meta engages in AI red-teaming practices and has security measures for their AI systems, no specific public information was found detailing their implementation of advanced red-teaming with particular attention to model weights security and authentication controls at the level expected for Security Level 3."
                  }
                }
              }
            ]
          },
          {
            "name": "Risk and Security Assessments",
            "controls": [
              {
                "name": "Keeping a risk register",
                "compliance": {
                  "OpenAI": {
                    "score": 75,
                    "sources": [
                      "https://venturebeat.com/ai/openai-announces-preparedness-framework-to-track-and-mitigate-ai-risks/",
                      "https://openai.com/global-affairs/our-approach-to-frontier-risk",
                      "https://www.rand.org/pubs/research_briefs/RBA2849-1.html"
                    ],
                    "justification": "OpenAI demonstrates substantial risk tracking through its Preparedness Framework which monitors cybersecurity, CBRN, persuasion, and model autonomy risks with defined risk levels and scorecards. They have established processes for tracking frontier risks and protecting model weights, though no explicit mention of a formal 'risk register' as specified by RAND's Security Level 3 requirements was found."
                  },
                  "Anthropic": {
                    "score": 50,
                    "sources": [
                      "https://www.anthropic.com/news/announcing-our-updated-responsible-scaling-policy",
                      "https://www.anthropic.com/news/activating-asl3-protections",
                      "https://arxiv.org/html/2407.01557v1"
                    ],
                    "justification": "Anthropic demonstrates systematic risk identification and management through their Responsible Scaling Policy and AI Safety Levels framework, suggesting risk tracking mechanisms exist, but no explicit public documentation confirms maintenance of a formal risk register specifically for model weights security."
                  },
                  "Google": {
                    "score": 50,
                    "sources": [
                      "https://blog.google/technology/safety-security/google-ai-saif-risk-assessment/",
                      "https://ai.google/responsibility/safety/",
                      "https://cloud.google.com/blog/products/identity-security/introducing-ai-protection-security-for-the-ai-era",
                      "https://safety.google/cybersecurity-advancements/saif/"
                    ],
                    "justification": "Google demonstrates partial implementation through its SAIF framework which includes risk assessment tools and AI Protection capabilities for risk management, but no explicit evidence of a dedicated risk register specifically for AI model weights security was found in public documentation."
                  },
                  "xAI": {
                    "score": 0,
                    "sources": [
                      "https://techcrunch.com/2025/05/13/xais-promised-safety-report-is-mia/",
                      "https://www.gov.uk/government/publications/frontier-ai-safety-commitments-ai-seoul-summit-2024/frontier-ai-safety-commitments-ai-seoul-summit-2024",
                      "https://x.ai/security"
                    ],
                    "justification": "No specific public information found about xAI maintaining a risk register for AI model weights security. While xAI committed to publishing a safety framework at the AI Seoul Summit in May 2024, they missed their self-imposed deadline in May 2025, and their limited security documentation makes no mention of risk registers."
                  },
                  "Meta": {
                    "score": 50,
                    "sources": [
                      "https://about.fb.com/news/2025/02/meta-approach-frontier-ai/",
                      "https://metr.org/faisc",
                      "https://www.bankinfosecurity.com/meta-plans-to-restrict-high-risk-ai-models-a-27447"
                    ],
                    "justification": "Meta has published a Frontier AI Framework (2025) that demonstrates risk categorization and threat modeling for AI systems, but lacks specific public evidence of maintaining a comprehensive risk register focused on model weights security as detailed in the RAND report's Security Level 3 requirements."
                  }
                }
              }
            ]
          },
          {
            "name": "Threat Detection and Response",
            "controls": [
              {
                "name": "Placement of effective honeypots",
                "compliance": {
                  "OpenAI": {
                    "score": 0,
                    "sources": [],
                    "justification": "No specific public information found regarding OpenAI's implementation of honeypots for AI model weights security."
                  },
                  "Anthropic": {
                    "score": 0,
                    "sources": [
                      "https://www.anthropic.com/news/activating-asl3-protections",
                      "https://www.anthropic.com/rsp-updates",
                      "https://www.rand.org/pubs/research_briefs/RBA2849-1.html"
                    ],
                    "justification": "No specific public information found about Anthropic implementing honeypots for AI model weights security, despite extensive documentation of their ASL-3 security measures."
                  },
                  "Google": {
                    "score": 0,
                    "sources": [],
                    "justification": "No specific public information found about Google's implementation of honeypots for AI model weights security as required for Security Level 3 in the RAND report."
                  },
                  "xAI": {
                    "score": 0,
                    "sources": [],
                    "justification": "No specific public information found regarding xAI's implementation of honeypots for AI model weights security."
                  },
                  "Meta": {
                    "score": 0,
                    "sources": [],
                    "justification": "No specific public information found about Meta's implementation of honeypots for AI model weights security."
                  }
                }
              }
            ]
          },
          {
            "name": "Security Team Capacity",
            "controls": [
              {
                "name": "General increased capacity (compared with SL2)",
                "compliance": {
                  "OpenAI": {
                    "score": 50,
                    "sources": [
                      "https://openai.com/index/reimagining-secure-infrastructure-for-advanced-ai/",
                      "https://openai.com/global-affairs/our-approach-to-frontier-risk",
                      "https://web.swipeinsight.app/posts/openai-unveils-security-architecture-for-frontier-ai-model-training-7065",
                      "https://www.rand.org/pubs/research_briefs/RBA2849-1.html"
                    ],
                    "justification": "OpenAI has implemented multi-layered defense-in-depth controls, dedicated security resources, and infrastructure built on Azure with Kubernetes orchestration, but no public information confirms they have achieved the full capacity increase from SL2 to SL3 as defined in the RAND report, which requires protection against cybercrime syndicates and insider threats."
                  },
                  "Anthropic": {
                    "score": 75,
                    "sources": [
                      "https://www.anthropic.com/news/announcing-our-updated-responsible-scaling-policy",
                      "https://www.anthropic.com/news/activating-asl3-protections",
                      "https://www.anthropic.com/transparency/voluntary-commitments"
                    ],
                    "justification": "Anthropic has implemented ASL-3 security standards with over 100 security controls targeting sophisticated non-state actors, including two-party authorization for model weight access and third-party security evaluations. However, they acknowledge ongoing improvements are needed for more sophisticated threats."
                  },
                  "Google": {
                    "score": 0,
                    "sources": [],
                    "justification": "No specific public information found regarding Google's implementation of increased infrastructure capacity compared to Security Level 2 requirements for AI model weights security as defined in the RAND report."
                  },
                  "xAI": {
                    "score": 0,
                    "sources": [],
                    "justification": "No specific public information found about xAI's security measures or compliance posture regarding AI model weights security or Security Level 3 requirements."
                  },
                  "Meta": {
                    "score": 0,
                    "sources": [],
                    "justification": "No specific public information found directly addressing Meta's compliance with 'General increased capacity (compared with SL2)' requirements as defined in the RAND report's Security Level 3 framework."
                  }
                }
              },
              {
                "name": "Concrete experience with APTs",
                "compliance": {
                  "OpenAI": {
                    "score": 50,
                    "sources": [
                      "https://socprime.com/blog/nation-backed-apt-attack-detection-microsoft-and-openai-warn-of-ai-exploitation-by-iranian-north-korean-chinese-and-russian-hackers/",
                      "https://www.reuters.com/technology/cybersecurity/openais-internal-ai-details-stolen-2023-breach-nyt-reports-2024-07-05/",
                      "https://cybersecuritynews.com/top-10-cyber-attacks-of-2024/",
                      "https://openai.com/index/security-on-the-path-to-agi/"
                    ],
                    "justification": "OpenAI has demonstrated experience with APT-related threats including a 2023 internal breach, disruption of 20+ state-sponsored AI exploitation attempts, and collaboration with Microsoft on APT research. However, no public evidence shows direct APT attacks specifically targeting their model weights."
                  },
                  "Anthropic": {
                    "score": 75,
                    "sources": [
                      "https://www.anthropic.com/news/frontier-model-security",
                      "https://www.anthropic.com/news/activating-asl3-protections",
                      "https://www.rand.org/pubs/research_briefs/RBA2849-1.html"
                    ],
                    "justification": "Anthropic has activated ASL-3 security measures targeting sophisticated non-state actors with over 100 security controls, implemented insider threat programs, and established threat intelligence partnerships, demonstrating practical experience defending against APT-level threats."
                  },
                  "Google": {
                    "score": 75,
                    "sources": [
                      "https://cloud.google.com/blog/topics/threat-intelligence/adversarial-misuse-generative-ai",
                      "https://arxiv.org/html/2503.11917v1",
                      "https://deepmind.google/discover/blog/introducing-the-frontier-safety-framework/"
                    ],
                    "justification": "Google demonstrates substantial experience with APTs through their Threat Intelligence Group's analysis of AI-enabled attacks from 20+ countries, implementation of defense-in-depth strategies, and development of the Frontier Safety Framework which specifically addresses security against sophisticated threat actors including nation-states."
                  },
                  "xAI": {
                    "score": 0,
                    "sources": [],
                    "justification": "No specific public information found regarding xAI's concrete experience with APTs related to AI model weights security."
                  },
                  "Meta": {
                    "score": 0,
                    "sources": [],
                    "justification": "No specific public information found about Meta's concrete experience with APTs related to AI model weights security as expected for RAND report Security Level 3."
                  }
                }
              },
              {
                "name": "Leveraging diverse security experience from leading organizations",
                "compliance": {
                  "OpenAI": {
                    "score": 25,
                    "sources": [
                      "https://forum.effectivealtruism.org/posts/fJycPfYuBHKoai98q/ai-companies-are-not-on-track-to-secure-model-weights",
                      "https://www.rand.org/pubs/research_briefs/RBA2849-1.html",
                      "https://openai.com/global-affairs/our-approach-to-frontier-risk",
                      "https://openai.com/careers/",
                      "https://openai.com/interview-guide/"
                    ],
                    "justification": "OpenAI mentions hiring from diverse backgrounds and has security leadership with experience from leading organizations (e.g., Jason Clinton from Google at Anthropic), but lacks detailed public disclosure about the diversity of security expertise specifically for model weights protection as outlined in RAND's Security Level 3 requirements."
                  },
                  "Anthropic": {
                    "score": 75,
                    "sources": [
                      "https://forum.effectivealtruism.org/posts/fJycPfYuBHKoai98q/ai-companies-are-not-on-track-to-secure-model-weights",
                      "https://venturebeat.com/ai/why-anthropic-and-openai-are-obsessed-with-securing-llm-model-weights/",
                      "https://www.clay.com/dossier/anthropic-executives",
                      "https://www.anthropic.com/careers",
                      "https://www.anthropic.com/news/activating-asl3-protections"
                    ],
                    "justification": "Anthropic demonstrates strong compliance by hiring Jason Clinton (with extensive Google security experience) as CISO, growing the security team from 2 part-time to dozens of people, and implementing over 100 security controls following best practices from security-conscious organizations. The company actively recruits from leading tech companies and emphasizes diverse backgrounds across disciplines."
                  },
                  "Google": {
                    "score": 0,
                    "sources": [],
                    "justification": "No specific public information found about Google leveraging diverse security experience from leading organizations for AI model weights security as expected for Security Level 3."
                  },
                  "xAI": {
                    "score": 50,
                    "sources": [
                      "https://boards.greenhouse.io/xai/jobs/4613280007",
                      "https://www.benzinga.com/news/23/07/33213851/meet-team-xai-musks-latest-brainchild-boasts-heavyweight-lineup-from-deepmind-openai-google",
                      "https://builtin.com/job/protective-intelligence-lead/2893383",
                      "https://x.ai/careers"
                    ],
                    "justification": "xAI has recruited talent from major AI companies including DeepMind, OpenAI, Google, and Microsoft, and has posted security-specific roles, but there's limited public evidence specifically demonstrating diverse security expertise from leading organizations on their security team."
                  },
                  "Meta": {
                    "score": 0,
                    "sources": [],
                    "justification": "No specific public information found about Meta leveraging diverse security experience from leading organizations for AI model weights security as described in Security Level 3 of the RAND report."
                  }
                }
              }
            ]
          }
        ]
      },
      {
        "name": "Other Organization Policies",
        "subcategories": [
          {
            "name": "",
            "controls": [
              {
                "name": "Two independent security layers",
                "compliance": {
                  "OpenAI": {
                    "score": 75,
                    "sources": [
                      "https://www.rand.org/pubs/research_reports/RRA2849-1.html",
                      "https://web.swipeinsight.app/posts/openai-unveils-security-architecture-for-frontier-ai-model-training-7065"
                    ],
                    "justification": "OpenAI has publicly disclosed implementing a defense-in-depth approach with multiple security layers for model weights protection, including authorization controls requiring multi-party approvals, private-linked storage with authentication/authorization, egress controls, and detection systems. While they demonstrate multiple layers, it's unclear if they meet the specific 'two independent' layers criterion as defined by RAND's SL3."
                  },
                  "Anthropic": {
                    "score": 50,
                    "sources": [
                      "https://www.anthropic.com/news/frontier-model-security",
                      "https://www.rand.org/pubs/research_reports/RRA2849-1.html",
                      "https://www.anthropic.com/news/activating-asl3-protections"
                    ],
                    "justification": "Anthropic has implemented two-party authorization for model weight access and mentions defense-in-depth with multiple security layers, but lacks explicit public confirmation of two fully independent security layer implementations as defined by RAND's Security Level 3 requirements."
                  },
                  "Google": {
                    "score": 25,
                    "sources": [
                      "https://www.rand.org/pubs/research_reports/RRA2849-1.html",
                      "https://cloud.google.com/use-cases/secure-ai-framework",
                      "https://safety.google/cybersecurity-advancements/saif/"
                    ],
                    "justification": "Google mentions a 'defense in depth approach' in their SAIF framework and implements multiple security controls, but there is no specific public evidence of two independent security layers specifically for AI model weights protection as required for Security Level 3."
                  },
                  "xAI": {
                    "score": 0,
                    "sources": [],
                    "justification": "No specific public information found about xAI's implementation of two independent security layers for AI model weights protection."
                  },
                  "Meta": {
                    "score": 0,
                    "sources": [],
                    "justification": "No specific public information found."
                  }
                }
              }
            ]
          }
        ]
      }
    ]
  },
  {
    "level": 4,
    "description": "A system that can likely thwart most standard operations by leading cyber-capable institutions (OC4). This includes the operations of many of the world's leading state-sponsored groups, many intelligence agencies across the world, and the top cyber-capable nations worldwide, which are able to execute such operations more than 100 times a year.",
    "categories": [
      {
        "name": "Implementation of Previous Security Levels",
        "subcategories": [
          {
            "name": "",
            "controls": [
              {
                "name": "The organization has implemented all the controls from SL1\u00e2\u20ac\u201cSL3.",
                "compliance": {
                  "OpenAI": {
                    "score": 0,
                    "sources": [],
                    "justification": "No specific public information found regarding OpenAI's implementation of the exact SL1-SL3 controls as defined in the RAND report for Security Level 4 compliance."
                  },
                  "Anthropic": {
                    "score": 75,
                    "sources": [
                      "https://venturebeat.com/ai/why-anthropic-and-openai-are-obsessed-with-securing-llm-model-weights/",
                      "https://forum.effectivealtruism.org/posts/fJycPfYuBHKoai98q/ai-companies-are-not-on-track-to-secure-model-weights",
                      "https://www.anthropic.com/news/activating-asl3-protections"
                    ],
                    "justification": "Anthropic has implemented ASL-3 security standards with over 100 security controls including two-party authorization for model weight access, enhanced change management protocols, endpoint software controls through binary allowlisting, and preliminary egress bandwidth controls. Their security team has grown from 2 part-time to several dozen people under experienced CISO Jason Clinton, demonstrating substantial investment in model weights protection. However, explicit confirmation of complete SL1-SL3 RAND controls implementation is not publicly documented."
                  },
                  "Google": {
                    "score": 0,
                    "sources": [],
                    "justification": "No specific public information found."
                  },
                  "xAI": {
                    "score": 0,
                    "sources": [],
                    "justification": "No specific public information found regarding xAI's implementation of RAND Security Level 1-3 controls for AI model weights protection."
                  },
                  "Meta": {
                    "score": 0,
                    "sources": [],
                    "justification": "No specific public information found regarding Meta's implementation of RAND report SL1-SL3 security controls for AI model weights."
                  }
                }
              }
            ]
          }
        ]
      },
      {
        "name": "Weight Security",
        "subcategories": [
          {
            "name": "Weight Storage",
            "controls": [
              {
                "name": "Isolation of weight storage",
                "compliance": {
                  "OpenAI": {
                    "score": 25,
                    "sources": [
                      "https://www.analyticsvidhya.com/blog/2024/05/openai-security-measures/",
                      "https://favtutor.com/articles/openai-security-mesaures/",
                      "https://www.rand.org/pubs/research_briefs/RBA2849-1.html"
                    ],
                    "justification": "OpenAI has publicly announced network and tenant isolation measures and trusted computing for AI accelerators, but no specific information confirms complete isolation of weight storage meeting Security Level 4 requirements as described in the RAND report."
                  },
                  "Anthropic": {
                    "score": 50,
                    "sources": [
                      "https://www.anthropic.com/rsp-updates",
                      "https://www.anthropic.com/news/frontier-model-security",
                      "https://www.anthropic.com/news/activating-asl3-protections"
                    ],
                    "justification": "Anthropic has implemented some isolation measures including access controls, multi-party authorization, and centralized weight storage with monitoring, but lacks explicit mention of air-gapped systems or complete physical isolation expected for SL-4 protection against sophisticated state actors."
                  },
                  "Google": {
                    "score": 0,
                    "sources": [],
                    "justification": "No specific public information found regarding Google's implementation of 'Isolation of weight storage' as required for Security Level 4 in the RAND report."
                  },
                  "xAI": {
                    "score": 0,
                    "sources": [],
                    "justification": "No specific public information found regarding xAI's implementation of weight storage isolation practices that would align with RAND Security Level 4 requirements."
                  },
                  "Meta": {
                    "score": 0,
                    "sources": [
                      "https://www.rand.org/pubs/research_reports/RRA2849-1.html",
                      "https://www.facebook.com/legal/terms/data_security_terms",
                      "https://www.rand.org/pubs/research_briefs/RBA2849-1.html"
                    ],
                    "justification": "No specific public information found regarding Meta's isolation of AI model weight storage practices that would address Security Level 4 requirements from the RAND report."
                  }
                }
              },
              {
                "name": "Weight storage setup is protected against eavesdropping and the simplest of TEMPEST attacks.",
                "compliance": {
                  "OpenAI": {
                    "score": 0,
                    "sources": [],
                    "justification": "No specific public information found regarding OpenAI's implementation of TEMPEST protections or eavesdropping countermeasures for weight storage as required for Security Level 4."
                  },
                  "Anthropic": {
                    "score": 25,
                    "sources": [
                      "https://www.anthropic.com/rsp-updates",
                      "https://www.cnbc.com/2025/03/31/anthropic-announces-updates-on-security-safeguards-for-its-ai-models.html",
                      "https://www.anthropic.com/news/activating-asl3-protections"
                    ],
                    "justification": "Anthropic has implemented Technical Surveillance Countermeasures (TSCM) with advanced detection equipment and regular sweeps for surveillance devices, but there is no specific public disclosure of TEMPEST-certified equipment or electromagnetic shielding measures required for protection against eavesdropping and TEMPEST attacks as specified in Security Level 4."
                  },
                  "Google": {
                    "score": 0,
                    "sources": [],
                    "justification": "No specific public information found regarding Google's implementation of TEMPEST protections for AI model weights storage."
                  },
                  "xAI": {
                    "score": 0,
                    "sources": [],
                    "justification": "No specific public information found about xAI's implementation of TEMPEST security controls or electromagnetic eavesdropping protection for AI model weights storage."
                  },
                  "Meta": {
                    "score": 0,
                    "sources": [],
                    "justification": "No specific public information found regarding Meta's implementation of TEMPEST-resistant storage or electromagnetic shielding for AI model weights. While Meta has extensive AI security initiatives and infrastructure investments, publicly available information does not address protection against eavesdropping or TEMPEST attacks for model weights storage."
                  }
                }
              },
              {
                "name": "Hardware-enforced limits on output rate",
                "compliance": {
                  "OpenAI": {
                    "score": 0,
                    "sources": [
                      "https://www.analyticsvidhya.com/blog/2024/05/openai-security-measures/",
                      "https://www.rand.org/pubs/research_reports/RRA2849-1.html",
                      "https://platform.openai.com/docs/guides/rate-limits",
                      "https://www.rand.org/pubs/research_briefs/RBA2849-1.html"
                    ],
                    "justification": "No specific public information found regarding hardware-enforced limits on output rate designed to prevent model weight exfiltration as required by RAND Security Level 4. OpenAI has standard API rate limits but no evidence of hardware-enforced security controls specifically targeting weight protection."
                  },
                  "Anthropic": {
                    "score": 25,
                    "sources": [
                      "https://www.anthropic.com/research/confidential-inference-trusted-vms",
                      "https://www.rand.org/pubs/research_reports/RRA2849-1.html",
                      "https://www.anthropic.com/news/activating-asl3-protections"
                    ],
                    "justification": "Anthropic has implemented egress bandwidth controls as part of their ASL-3 security measures, but these appear to be network-based rather than hardware-enforced. While they mention limiting outbound network traffic rates to prevent model weight exfiltration, there is no specific public information about hardware-enforced output rate limits."
                  },
                  "Google": {
                    "score": 0,
                    "sources": [
                      "https://deepmind.google/discover/blog/taking-a-responsible-path-to-agi/",
                      "https://deepmind.google/discover/blog/introducing-the-frontier-safety-framework/",
                      "https://www.rand.org/pubs/research_briefs/RBA2849-1.html"
                    ],
                    "justification": "No specific public information found regarding Google's implementation of hardware-enforced limits on output rate for AI model weights security, which is expected for Security Level 4 compliance according to the RAND report."
                  },
                  "xAI": {
                    "score": 0,
                    "sources": [],
                    "justification": "No specific public information found about xAI implementing hardware-enforced limits on output rate as described in the RAND Security Level 4 framework."
                  },
                  "Meta": {
                    "score": 0,
                    "sources": [],
                    "justification": "No specific public information found regarding Meta's implementation of hardware-enforced limits on output rate for AI model weights security as described in RAND's Security Level 4 requirements."
                  }
                }
              },
              {
                "name": "Reduced communication capabilities",
                "compliance": {
                  "OpenAI": {
                    "score": 0,
                    "sources": [],
                    "justification": "No specific public information found about OpenAI implementing 'reduced communication capabilities' as defined for Security Level 4 in the RAND report."
                  },
                  "Anthropic": {
                    "score": 0,
                    "sources": [],
                    "justification": "No specific public information found regarding Anthropic's compliance with 'reduced communication capabilities' as defined for Security Level 4 in the RAND report."
                  },
                  "Google": {
                    "score": 0,
                    "sources": [],
                    "justification": "No specific public information found regarding Google's implementation of 'reduced communication capabilities' for AI model weights security as outlined in RAND's Security Level 4 framework."
                  },
                  "xAI": {
                    "score": 0,
                    "sources": [],
                    "justification": "No specific public information found regarding xAI's implementation of reduced communication capabilities for AI model weights security as described in Security Level 4 requirements."
                  },
                  "Meta": {
                    "score": 0,
                    "sources": [],
                    "justification": "No specific public information found regarding Meta's compliance with 'reduced communication capabilities' as defined in Security Level 4 of the RAND report on AI model weights security."
                  }
                }
              }
            ]
          },
          {
            "name": "Security During Transport and Use",
            "controls": [
              {
                "name": "Confidential computing (when available)",
                "compliance": {
                  "OpenAI": {
                    "score": 25,
                    "sources": [
                      "https://www.analyticsvidhya.com/blog/2024/05/openai-security-measures/",
                      "https://www.rand.org/pubs/research_reports/RRA2849-1.html",
                      "https://www.decentriq.com/article/the-key-to-secure-llms-is-hidden-in-confidential-computing"
                    ],
                    "justification": "OpenAI has publicly discussed confidential computing as important for AI security and proposes it as one of six security measures for advanced AI infrastructure. However, there is no specific evidence of actual implementation of confidential computing for model weights protection in their current production systems."
                  },
                  "Anthropic": {
                    "score": 75,
                    "sources": [
                      "https://www.anthropic.com/research/confidential-inference-trusted-vms",
                      "https://www.rand.org/pubs/research_reports/RRA2849-1.html",
                      "https://www.rand.org/pubs/research_briefs/RBA2849-1.html"
                    ],
                    "justification": "Anthropic is actively implementing confidential computing for AI model weights security. They have published research on 'Confidential Inference via Trusted Virtual Machines' and are working on trusted execution environments to secure model weights during use. While their implementation is still in development and not fully production-ready for all systems, they demonstrate strong commitment to this Security Level 4 control."
                  },
                  "Google": {
                    "score": 75,
                    "sources": [
                      "https://www.rand.org/pubs/research_briefs/RBA2849-1.html",
                      "https://cloud.google.com/blog/products/identity-security/expanding-confidential-computing-for-ai-workloads-next24",
                      "https://cloud.google.com/blog/products/identity-security/how-confidential-accelerators-can-boost-ai-workload-security",
                      "https://cloud.google.com/architecture/confidential-computing-analytics-ai"
                    ],
                    "justification": "Google offers comprehensive confidential computing capabilities including Confidential VMs with NVIDIA H100 GPUs and Intel TDX, Confidential Space, and specific AI workload protections. However, the RAND report notes that confidential computing for GPUs 'is still nascent and may not be production-ready for frontier systems' and lacks complete physical attack protections."
                  },
                  "xAI": {
                    "score": 0,
                    "sources": [],
                    "justification": "No specific public information found regarding xAI's implementation or posture on confidential computing for AI model weights security as described in the RAND Security Level 4 framework."
                  },
                  "Meta": {
                    "score": 50,
                    "sources": [
                      "https://www.rand.org/pubs/research_briefs/RBA2849-1.html",
                      "https://www.edgeless.systems/solutions/ai-model-protection",
                      "https://engineering.fb.com/2025/04/29/security/whatsapp-private-processing-ai-tools/"
                    ],
                    "justification": "Meta has implemented confidential computing through 'Private Processing' using TEEs and CVMs for AI workloads, demonstrating technical capability and alignment with RAND recommendations. However, the implementation appears focused on protecting user data during inference rather than specifically securing AI model weights themselves, and past incidents like the LLaMA-3 leak suggest incomplete protection of model weights."
                  }
                }
              }
            ]
          },
          {
            "name": "Physical Security",
            "controls": [
              {
                "name": "Increased guarding (compared with SL3) via manned and digital systems",
                "compliance": {
                  "OpenAI": {
                    "score": 0,
                    "sources": [
                      "https://venturebeat.com/ai/why-anthropic-and-openai-are-obsessed-with-securing-llm-model-weights/",
                      "https://www.analyticsvidhya.com/blog/2024/05/openai-security-measures/",
                      "https://openai.com/enterprise-privacy",
                      "https://www.rand.org/pubs/research_briefs/RBA2849-1.html"
                    ],
                    "justification": "No specific public information found regarding OpenAI's implementation of 'increased guarding via manned and digital systems' as required for Security Level 4 in the RAND report."
                  },
                  "Anthropic": {
                    "score": 25,
                    "sources": [
                      "https://www.anthropic.com/rsp-updates",
                      "https://www.cnbc.com/2025/03/31/anthropic-announces-updates-on-security-safeguards-for-its-ai-models.html",
                      "https://www.anthropic.com/news/activating-asl3-protections"
                    ],
                    "justification": "Anthropic has implemented some physical security measures like TSCM sweeps and established security teams, but public information lacks specific details about manned guarding systems or explicit compliance with RAND SL4 requirements for increased guarding compared to SL3."
                  },
                  "Google": {
                    "score": 0,
                    "sources": [],
                    "justification": "No specific public information found about Google's implementation of increased guarding via manned and digital systems as described for Security Level 4 in the RAND report."
                  },
                  "xAI": {
                    "score": 0,
                    "sources": [
                      "https://www.oracle.com/artificial-intelligence/ai-open-weights-models/",
                      "https://embracethered.com/blog/posts/2024/security-probllms-in-xai-grok/",
                      "https://www.rand.org/pubs/research_briefs/RBA2849-1.html"
                    ],
                    "justification": "No specific public information found regarding xAI's implementation of increased guarding systems for model weights security as expected for Security Level 4. Additionally, xAI's open-weights approach for Grok appears inconsistent with typical SL4 security requirements."
                  },
                  "Meta": {
                    "score": 0,
                    "sources": [],
                    "justification": "No specific public information found regarding Meta's compliance with increased manned and digital guarding systems for AI model weights security as specified in RAND's Security Level 4 requirements."
                  }
                }
              },
              {
                "name": "Meticulous logging of all access",
                "compliance": {
                  "OpenAI": {
                    "score": 0,
                    "sources": [],
                    "justification": "No specific public information found about OpenAI's implementation of meticulous logging of all access to AI model weights as required for Security Level 4."
                  },
                  "Anthropic": {
                    "score": 50,
                    "sources": [
                      "https://www.anthropic.com/news/frontier-model-security",
                      "https://www.rand.org/pubs/research_reports/RRA2849-1.html",
                      "https://www.anthropic.com/news/activating-asl3-protections"
                    ],
                    "justification": "Anthropic has implemented comprehensive security measures including access controls, monitoring systems, and two-party authorization for model weight access, but no specific public information was found detailing meticulous logging of all access as required for Security Level 4."
                  },
                  "Google": {
                    "score": 50,
                    "sources": [
                      "https://www.rand.org/pubs/research_reports/RRA2849-1.html",
                      "https://cloud.google.com/security/securing-ai",
                      "https://cloud.google.com/use-cases/secure-ai-framework",
                      "https://blog.google/technology/safety-security/introducing-googles-secure-ai-framework/"
                    ],
                    "justification": "Google has comprehensive logging infrastructure and mentions access controls with monitoring for AI models, but lacks specific public documentation about meticulous logging of all access to AI model weights as defined in RAND's Security Level 4."
                  },
                  "xAI": {
                    "score": 0,
                    "sources": [],
                    "justification": "No specific public information found regarding xAI's implementation of meticulous logging for AI model weights access as described in RAND's Security Level 4."
                  },
                  "Meta": {
                    "score": 0,
                    "sources": [],
                    "justification": "No specific public information found regarding Meta's compliance with Security Level 4 meticulous logging requirements for AI model weights access as described in the RAND report."
                  }
                }
              },
              {
                "name": "Prohibiting devices near the setup",
                "compliance": {
                  "OpenAI": {
                    "score": 0,
                    "sources": [],
                    "justification": "No specific public information found regarding OpenAI's compliance with 'Prohibiting devices near the setup' control measures for AI model weights security as described in Security Level 4 requirements."
                  },
                  "Anthropic": {
                    "score": 0,
                    "sources": [],
                    "justification": "No specific public information found."
                  },
                  "Google": {
                    "score": 0,
                    "sources": [],
                    "justification": "No specific public information found regarding Google's implementation of 'prohibiting devices near the setup' controls for AI model weights security at Security Level 4 as described in the RAND report."
                  },
                  "xAI": {
                    "score": 0,
                    "sources": [
                      "https://x.ai/security"
                    ],
                    "justification": "No specific public information found regarding xAI's policies on prohibiting devices near AI model weight setups or similar Security Level 4 controls."
                  },
                  "Meta": {
                    "score": 0,
                    "sources": [],
                    "justification": "No specific public information found."
                  }
                }
              }
            ]
          },
          {
            "name": "Permitted Interfaces",
            "controls": [
              {
                "name": "Specialized hardware for all external interfaces",
                "compliance": {
                  "OpenAI": {
                    "score": 0,
                    "sources": [
                      "https://web.swipeinsight.app/posts/openai-unveils-security-architecture-for-frontier-ai-model-training-7065",
                      "https://www.rand.org/pubs/research_briefs/RBA2849-1.html"
                    ],
                    "justification": "No specific public information found regarding OpenAI's implementation of specialized hardware for all external interfaces related to AI model weights security as required for Security Level 4."
                  },
                  "Anthropic": {
                    "score": 0,
                    "sources": [],
                    "justification": "No specific public information found regarding Anthropic's implementation of specialized hardware for all external interfaces as a Security Level 4 control for AI model weights security."
                  },
                  "Google": {
                    "score": 0,
                    "sources": [
                      "https://cloud.google.com/security/securing-ai",
                      "https://cloud.google.com/use-cases/secure-ai-framework",
                      "https://www.rand.org/pubs/research_reports/RRA2849-1.html",
                      "https://www.rand.org/pubs/research_briefs/RBA2849-1.html"
                    ],
                    "justification": "No specific public information found regarding Google's implementation of specialized hardware for all external interfaces related to AI model weights security at Security Level 4."
                  },
                  "xAI": {
                    "score": 0,
                    "sources": [],
                    "justification": "No specific public information found regarding xAI's implementation of specialized hardware for external interfaces related to AI model weights security as required by RAND Security Level 4."
                  },
                  "Meta": {
                    "score": 0,
                    "sources": [
                      "https://www.rand.org/pubs/research_reports/RRA2849-1.html",
                      "https://en.wikipedia.org/wiki/Meta_AI",
                      "https://en.wikipedia.org/wiki/Llama_(language_model)",
                      "https://engineering.fb.com/2024/10/15/data-infrastructure/metas-open-ai-hardware-vision/",
                      "https://www.rand.org/pubs/research_briefs/RBA2849-1.html"
                    ],
                    "justification": "No specific public information found regarding Meta's implementation of specialized hardware for securing external interfaces to AI model weights as required for Security Level 4 compliance."
                  }
                }
              }
            ]
          },
          {
            "name": "Monitoring",
            "controls": [
              {
                "name": "Enforcement of time-buffered review (software limitation)",
                "compliance": {
                  "OpenAI": {
                    "score": 0,
                    "sources": [],
                    "justification": "No specific public information found regarding OpenAI's compliance with 'Enforcement of time-buffered review (software limitation)' as defined in the RAND report's Security Level 4 framework."
                  },
                  "Anthropic": {
                    "score": 0,
                    "sources": [
                      "https://www.anthropic.com/rsp-updates",
                      "https://www.rand.org/pubs/research_reports/RRA2849-1.html",
                      "https://www.anthropic.com/news/activating-asl3-protections"
                    ],
                    "justification": "No specific public information found regarding time-buffered review or software limitation controls. While Anthropic has implemented ASL-3 security measures including egress bandwidth controls that could limit data exfiltration rates, there is no evidence of specific time-buffered review mechanisms for software access as would be expected for Security Level 4 compliance."
                  },
                  "Google": {
                    "score": 0,
                    "sources": [],
                    "justification": "No specific public information found regarding Google's compliance with 'time-buffered review (software limitation)' as defined in the RAND Security Level 4 requirements."
                  },
                  "xAI": {
                    "score": 0,
                    "sources": [],
                    "justification": "No specific public information found."
                  },
                  "Meta": {
                    "score": 0,
                    "sources": [
                      "https://www.rand.org/pubs/research_reports/RRA2849-1.html",
                      "https://www.bankinfosecurity.com/meta-plans-to-restrict-high-risk-ai-models-a-27447",
                      "https://www.rand.org/pubs/research_briefs/RBA2849-1.html"
                    ],
                    "justification": "No specific public information found regarding Meta's implementation of time-buffered review or software limitations for AI model weights security as described in the RAND Security Level 4 framework."
                  }
                }
              },
              {
                "name": "Protection of the monitoring logs at the hardware level",
                "compliance": {
                  "OpenAI": {
                    "score": 0,
                    "sources": [
                      "https://openai.com/enterprise-privacy",
                      "https://learn.microsoft.com/en-us/legal/cognitive-services/openai/data-privacy",
                      "https://www.rand.org/pubs/research_briefs/RBA2849-1.html"
                    ],
                    "justification": "No specific public information found regarding OpenAI's implementation of hardware-level protection of monitoring logs for AI model weights security as expected for Security Level 4."
                  },
                  "Anthropic": {
                    "score": 0,
                    "sources": [],
                    "justification": "No specific public information found regarding Anthropic's compliance with hardware-level protection of monitoring logs as required for Security Level 4."
                  },
                  "Google": {
                    "score": 0,
                    "sources": [
                      "https://cloud.google.com/blog/products/identity-security/how-confidential-accelerators-can-boost-ai-workload-security",
                      "https://cloud.google.com/use-cases/secure-ai-framework",
                      "https://github.com/AI-Hypercomputer/cloud-tpu-monitoring-debugging",
                      "https://www.rand.org/pubs/research_briefs/RBA2849-1.html"
                    ],
                    "justification": "No specific public information found regarding Google's implementation of hardware-level protection for monitoring logs related to AI model weights security as expected for Security Level 4."
                  },
                  "xAI": {
                    "score": 0,
                    "sources": [],
                    "justification": "No specific public information found regarding xAI's compliance with hardware-level protection of monitoring logs for AI model weights security, as required for Security Level 4."
                  },
                  "Meta": {
                    "score": 50,
                    "sources": [
                      "https://www.infosecurity-magazine.com/news/meta-new-advances-ai-security/",
                      "https://www.rand.org/pubs/research_reports/RRA2849-1.html",
                      "https://engineering.fb.com/2025/04/29/security/whatsapp-private-processing-ai-tools/"
                    ],
                    "justification": "Meta has implemented some advanced security measures through Private Processing technology, including confidential computing with CPU-based trusted execution environments and log-filtering systems for restricted environments. However, there is no specific public information indicating comprehensive hardware-level monitoring logs protection as outlined in RAND Security Level 4 requirements."
                  }
                }
              },
              {
                "name": "Comprehensive anomaly detection and alert system over the monitoring logs",
                "compliance": {
                  "OpenAI": {
                    "score": 0,
                    "sources": [],
                    "justification": "No specific public information found regarding OpenAI's implementation of comprehensive anomaly detection and alert systems specifically for AI model weights security monitoring logs as required for RAND Security Level 4."
                  },
                  "Anthropic": {
                    "score": 25,
                    "sources": [
                      "https://www.anthropic.com/research/confidential-inference-trusted-vms",
                      "https://www.rand.org/pubs/research_reports/RRA2849-1.html",
                      "https://www.anthropic.com/news/activating-asl3-protections"
                    ],
                    "justification": "Anthropic has implemented some monitoring and detection capabilities as part of their ASL-3 security standards, including bandwidth monitoring for model weight exfiltration detection. However, no specific public information was found about comprehensive anomaly detection and alert systems specifically focused on monitoring logs for model weights security as required for Security Level 4."
                  },
                  "Google": {
                    "score": 0,
                    "sources": [],
                    "justification": "No specific public information found regarding Google's comprehensive anomaly detection and alert system specifically for AI model weights security as outlined in the RAND report's Security Level 4 requirements."
                  },
                  "xAI": {
                    "score": 0,
                    "sources": [
                      "https://krebsonsecurity.com/2025/05/xai-dev-leaks-api-key-for-private-spacex-tesla-llms/",
                      "https://www.rand.org/pubs/research_briefs/RBA2849-1.html"
                    ],
                    "justification": "No specific public information found regarding xAI's implementation of comprehensive anomaly detection and alert systems for monitoring logs related to AI model weights security. A recent security incident involving a leaked API key exposed for two months suggests insufficient monitoring capabilities."
                  },
                  "Meta": {
                    "score": 0,
                    "sources": [
                      "https://www.rand.org/pubs/research_reports/RRA2849-1.html",
                      "https://www.npr.org/2025/05/31/nx-s1-5407870/meta-ai-facebook-instagram-risks",
                      "https://engineering.fb.com/2020/07/21/security/data-classification-system/",
                      "https://engineering.fb.com/2025/04/29/security/whatsapp-private-processing-ai-tools/"
                    ],
                    "justification": "No specific public information found regarding comprehensive anomaly detection and alert systems specifically for AI model weights security monitoring logs."
                  }
                }
              }
            ]
          }
        ]
      },
      {
        "name": "AI Model Resilience",
        "subcategories": [
          {
            "name": "Model Robustness",
            "controls": [
              {
                "name": "Adversarial output detection",
                "compliance": {
                  "OpenAI": {
                    "score": 0,
                    "sources": [],
                    "justification": "No specific public information found regarding OpenAI's compliance with 'Adversarial output detection' as defined in the RAND Security Level 4 framework."
                  },
                  "Anthropic": {
                    "score": 0,
                    "sources": [],
                    "justification": "No specific public information found addressing 'adversarial output detection' as defined for Security Level 4 in the RAND model weights security framework."
                  },
                  "Google": {
                    "score": 0,
                    "sources": [],
                    "justification": "No specific public information found regarding Google's implementation of adversarial output detection as a control for AI model weights security at Security Level 4 as defined in the RAND report."
                  },
                  "xAI": {
                    "score": 0,
                    "sources": [],
                    "justification": "No specific public information found regarding xAI's implementation of adversarial output detection for AI model weights security."
                  },
                  "Meta": {
                    "score": 25,
                    "sources": [
                      "https://github.com/meta-llama/PurpleLlama",
                      "https://about.fb.com/news/2023/12/purple-llama-safe-responsible-ai-development/",
                      "https://mindgard.ai/blog/what-is-ai-red-teaming"
                    ],
                    "justification": "Meta has implemented some adversarial detection capabilities through Llama Guard for output moderation and Prompt Guard for input protection, along with red team testing. However, no specific public information was found regarding adversarial output detection measures specifically designed to protect AI model weights at Security Level 4 standards."
                  }
                }
              }
            ]
          },
          {
            "name": "Oracle Protection",
            "controls": [
              {
                "name": "Output reconstruction",
                "compliance": {
                  "OpenAI": {
                    "score": 0,
                    "sources": [],
                    "justification": "No specific public information found regarding OpenAI's compliance with 'Output reconstruction' measures for Security Level 4 as described in the RAND report."
                  },
                  "Anthropic": {
                    "score": 0,
                    "sources": [],
                    "justification": "No specific public information found regarding Anthropic's compliance with 'output reconstruction' controls as defined in the RAND report's Security Level 4 requirements."
                  },
                  "Google": {
                    "score": 0,
                    "sources": [],
                    "justification": "No specific public information found."
                  },
                  "xAI": {
                    "score": 0,
                    "sources": [
                      "https://www.oracle.com/artificial-intelligence/ai-open-weights-models/",
                      "https://huggingface.co/xai-org/grok-1",
                      "https://x.ai/news/grok-os",
                      "https://www.rand.org/pubs/research_reports/RRA2849-1.html",
                      "https://www.rand.org/pubs/research_briefs/RBA2849-1.html"
                    ],
                    "justification": "xAI has openly released Grok-1's model weights and architecture under Apache 2.0 license, making them freely downloadable by anyone. This open-source approach directly contradicts the RAND Security Level 4 framework which requires protecting model weights against state-sponsored attackers. No evidence found of output reconstruction controls or other advanced security measures for model weights protection."
                  },
                  "Meta": {
                    "score": 0,
                    "sources": [
                      "https://www.infosecurity-magazine.com/news/meta-new-advances-ai-security/",
                      "https://www.unite.ai/from-jailbreaks-to-injections-how-meta-is-strengthening-ai-security-with-llama-firewall/",
                      "https://www.rand.org/pubs/research_briefs/RBA2849-1.html"
                    ],
                    "justification": "No specific public information found about Meta implementing output reconstruction protections for their AI model weights. While Meta has released security tools like LlamaFirewall and Guard models for input/output filtering, there is no evidence of confidential computing implementations or TEE-based protections that would prevent model weight reconstruction as required by RAND Security Level 4."
                  }
                }
              }
            ]
          }
        ]
      },
      {
        "name": "Security of Network and Other (Nonweight) Sensitive Assets",
        "subcategories": [
          {
            "name": "Software",
            "controls": [
              {
                "name": "Limiting the attack surface (e.g., the limited interaction interfaces of a Chromebook)",
                "compliance": {
                  "OpenAI": {
                    "score": 75,
                    "sources": [
                      "https://www.oracle.com/artificial-intelligence/ai-open-weights-models/",
                      "https://openai.com/enterprise-privacy",
                      "https://openai.com/index/openai-api/",
                      "https://openai.com/global-affairs/openai-s-comment-to-the-ntia-on-open-model-weights"
                    ],
                    "justification": "OpenAI implements limited interaction interfaces through its API-only access model rather than direct weight distribution, uses access controls and production reviews to limit exposure, and employs rate limiting and content filtering. However, no specific evidence of hardened, Chromebook-like limited interfaces for weight protection was found."
                  },
                  "Anthropic": {
                    "score": 50,
                    "sources": [
                      "https://www.anthropic.com/rsp-updates",
                      "https://venturebeat.com/ai/why-anthropic-and-openai-are-obsessed-with-securing-llm-model-weights/",
                      "https://www.anthropic.com/news/frontier-model-security",
                      "https://www.anthropic.com/news/activating-asl3-protections"
                    ],
                    "justification": "Anthropic implements some attack surface limitation measures including API-only access (preventing direct weight access), egress bandwidth controls, and compartmentalized access controls. However, specific 'Chromebook-like' limited interaction interfaces are not explicitly documented."
                  },
                  "Google": {
                    "score": 0,
                    "sources": [],
                    "justification": "No specific public information found."
                  },
                  "xAI": {
                    "score": 0,
                    "sources": [],
                    "justification": "No specific public information found regarding xAI's implementation of limited interaction interfaces or attack surface reduction measures for AI model weights security as described in RAND Security Level 4."
                  },
                  "Meta": {
                    "score": 0,
                    "sources": [],
                    "justification": "No specific public information found regarding Meta's implementation of limited interaction interfaces (like Chromebook-style security) for AI model weights protection as described in RAND's Security Level 4 controls."
                  }
                }
              }
            ]
          },
          {
            "name": "Access, Permissions, and Credentials",
            "controls": [
              {
                "name": "Enforcement of strong random passwords and keys for enhanced security",
                "compliance": {
                  "OpenAI": {
                    "score": 0,
                    "sources": [],
                    "justification": "No specific public information found regarding OpenAI's compliance with strong random password and key enforcement for AI model weights security at Security Level 4 as defined in the RAND report."
                  },
                  "Anthropic": {
                    "score": 50,
                    "sources": [
                      "https://www.rand.org/pubs/research_reports/RRA2849-1.html",
                      "https://www.anthropic.com/news/frontier-model-security",
                      "https://privacy.anthropic.com/en/articles/10458704-how-does-anthropic-protect-the-personal-data-of-claude-ai-users",
                      "https://support.anthropic.com/en/articles/9767949-api-key-best-practices-keeping-your-keys-safe-and-secure",
                      "https://www.anthropic.com/news/activating-asl3-protections"
                    ],
                    "justification": "Anthropic implements several relevant security measures including two-party control systems, multi-factor authentication, and password management best practices for API keys, but lacks specific public documentation on strong random password/key enforcement for model weights access at the Security Level 4 standard."
                  },
                  "Google": {
                    "score": 0,
                    "sources": [],
                    "justification": "No specific public information found regarding Google's implementation of strong random passwords and keys specifically for AI model weights security as described in the RAND Security Level 4 requirements."
                  },
                  "xAI": {
                    "score": 0,
                    "sources": [
                      "https://www.rand.org/pubs/research_reports/RRA2849-1.html",
                      "https://gbhackers.com/xai-api-key-leak-exposes-proprietary-language-models/",
                      "https://krebsonsecurity.com/2025/05/xai-dev-leaks-api-key-for-private-spacex-tesla-llms/"
                    ],
                    "justification": "Recent security incident revealed xAI employee leaked private API key on GitHub for 2+ months, exposing 60+ proprietary models. This demonstrates weak key management practices and insufficient monitoring, directly contradicting Security Level 4 requirements for strong password/key enforcement."
                  },
                  "Meta": {
                    "score": 0,
                    "sources": [],
                    "justification": "No specific public information found regarding Meta's enforcement of strong random passwords and keys specifically for AI model weights security."
                  }
                }
              },
              {
                "name": "Zero Trust architecture (adherence to at least the standards in the \"Optimal\" level of CISA's Zero Trust Maturity Model)",
                "compliance": {
                  "OpenAI": {
                    "score": 25,
                    "sources": [
                      "https://www.rand.org/pubs/research_briefs/RBA2849-1.html",
                      "https://web.swipeinsight.app/posts/openai-unveils-security-architecture-for-frontier-ai-model-training-7065",
                      "https://community.openai.com/t/zero-trust-architecture-for-mcp-servers-using-pomerium/1288157",
                      "https://www.cisa.gov/sites/default/files/2023-04/zero_trust_maturity_model_v2_508.pdf"
                    ],
                    "justification": "OpenAI has published details on their security architecture that includes some Zero Trust elements like identity management, role-based access control, and defense-in-depth measures for protecting model weights. However, there is no specific public information confirming adherence to CISA's Zero Trust Maturity Model 'Optimal' level requirements or comprehensive implementation of the full RAND Security Level 4 standards for AI model weights protection."
                  },
                  "Anthropic": {
                    "score": 50,
                    "sources": [
                      "https://www.cisa.gov/zero-trust-maturity-model",
                      "https://www.anthropic.com/research/confidential-inference-trusted-vms",
                      "https://venturebeat.com/ai/why-anthropic-and-openai-are-obsessed-with-securing-llm-model-weights/",
                      "https://www.anthropic.com/news/activating-asl3-protections",
                      "https://www.rand.org/pubs/research_briefs/RBA2849-1.html"
                    ],
                    "justification": "Anthropic has implemented substantial security measures including 100+ security controls, confidential computing, and egress bandwidth controls, but lacks specific Zero Trust architecture implementation details or adherence to CISA's Optimal level requirements."
                  },
                  "Google": {
                    "score": 50,
                    "sources": [
                      "https://www.rand.org/pubs/research_briefs/RBA2849-1.html",
                      "https://cloud.google.com/architecture/framework/security/implement-zero-trust",
                      "https://workspace.google.com/blog/identity-and-security/accelerating-zero-trust-and-digital-sovereignty-ai",
                      "https://cloud.google.com/blog/topics/public-sector/strengthening-federal-cybersecurity-cisa-zero-trust-and-google-workspace-exclusive-sessions-at-next-24/"
                    ],
                    "justification": "Google demonstrates strong Zero Trust implementation with BeyondCorp architecture, context-aware access controls, and CISA compliance features, but lacks specific public documentation of AI model weights security measures at the 'Optimal' level required for Security Level 4 protection against state-sponsored attacks."
                  },
                  "xAI": {
                    "score": 0,
                    "sources": [],
                    "justification": "No specific public information found regarding xAI's implementation of Zero Trust architecture controls or adherence to CISA's Zero Trust Maturity Model standards for AI model weights security."
                  },
                  "Meta": {
                    "score": 0,
                    "sources": [],
                    "justification": "No specific public information found regarding Meta's compliance with Zero Trust architecture standards for AI model weights security at the CISA 'Optimal' level or RAND Security Level 4."
                  }
                }
              }
            ]
          },
          {
            "name": "Hardware",
            "controls": [
              {
                "name": "All hardware used on devices must undergo source-code auditing and be validated as secure.",
                "compliance": {
                  "OpenAI": {
                    "score": 0,
                    "sources": [
                      "https://openai.com/security/",
                      "https://www.analyticsvidhya.com/blog/2024/05/openai-security-measures/",
                      "https://web.swipeinsight.app/posts/openai-unveils-security-architecture-for-frontier-ai-model-training-7065",
                      "https://www.rand.org/pubs/research_briefs/RBA2849-1.html"
                    ],
                    "justification": "No specific public information found regarding OpenAI's implementation of hardware source-code auditing and validation for securing AI model weights as required for Security Level 4."
                  },
                  "Anthropic": {
                    "score": 25,
                    "sources": [
                      "https://www.anthropic.com/research/confidential-inference-trusted-vms",
                      "https://venturebeat.com/ai/why-anthropic-and-openai-are-obsessed-with-securing-llm-model-weights/",
                      "https://www.anthropic.com/news/activating-asl3-protections"
                    ],
                    "justification": "Anthropic has implemented some relevant security measures including confidential computing with signed code verification and extensive security controls, but no specific public information confirms comprehensive hardware source-code auditing and validation as required for Security Level 4."
                  },
                  "Google": {
                    "score": 0,
                    "sources": [],
                    "justification": "No specific public information found regarding Google's compliance with Security Level 4 hardware source-code auditing requirements for AI model weights protection."
                  },
                  "xAI": {
                    "score": 0,
                    "sources": [],
                    "justification": "No specific public information found regarding xAI's compliance with hardware source-code auditing requirements for AI model weights security as specified in Security Level 4 controls."
                  },
                  "Meta": {
                    "score": 0,
                    "sources": [],
                    "justification": "No specific public information found regarding Meta's compliance with hardware source-code auditing and validation requirements for AI model weights security as specified in Security Level 4 of the RAND report."
                  }
                }
              },
              {
                "name": "Secure hardware required for access",
                "compliance": {
                  "OpenAI": {
                    "score": 25,
                    "sources": [
                      "https://www.analyticsvidhya.com/blog/2024/05/openai-security-measures/",
                      "https://www.rand.org/news/press/2024/05/30.html",
                      "https://www.rand.org/pubs/research_briefs/RBA2849-1.html"
                    ],
                    "justification": "OpenAI proposes trusted computing for AI accelerators to encrypt model weights until execution, but there is no evidence of full implementation of secure hardware meeting Security Level 4 requirements, which specifically need protection against physical attacks."
                  },
                  "Anthropic": {
                    "score": 25,
                    "sources": [
                      "https://www.anthropic.com/research/confidential-inference-trusted-vms",
                      "https://www.rand.org/pubs/research_briefs/RBA2849-1.html",
                      "https://www.anthropic.com/news/frontier-model-security",
                      "https://www.anthropic.com/news/activating-asl3-protections"
                    ],
                    "justification": "Anthropic has implemented confidential computing research for model weights protection and mentions hardware-based security controls, but no specific public information indicates full implementation of secure hardware requirements for access as defined in RAND Security Level 4. Their ASL-3 protections focus primarily on software controls and egress monitoring."
                  },
                  "Google": {
                    "score": 75,
                    "sources": [
                      "https://cloud.google.com/use-cases/secure-ai-framework",
                      "https://cloud.google.com/blog/products/identity-security/how-confidential-computing-lays-the-foundation-for-trusted-ai/",
                      "https://cloud.google.com/blog/products/identity-security/how-confidential-accelerators-can-boost-ai-workload-security",
                      "https://cloud.google.com/architecture/security/confidential-computing-analytics-ai",
                      "https://www.rand.org/pubs/research_briefs/RBA2849-1.html"
                    ],
                    "justification": "Google demonstrates strong commitment to secure hardware for AI model access through multiple Confidential Computing offerings including H100 GPUs with hardware-based TEEs, Intel TDX support, and AMD SEV-SNP technology. However, no specific evidence was found of Google's internal AI model weights being protected by these secure hardware requirements."
                  },
                  "xAI": {
                    "score": 0,
                    "sources": [],
                    "justification": "No specific public information found regarding xAI's implementation of secure hardware requirements for AI model weights protection as described in Security Level 4 of the RAND report."
                  },
                  "Meta": {
                    "score": 0,
                    "sources": [],
                    "justification": "No specific public information found regarding Meta's implementation of secure hardware requirements for AI model weights access, such as confidential computing or hardware-based trusted execution environments."
                  }
                }
              },
              {
                "name": "Ongoing compromise assessment on all devices with access (server or employee)",
                "compliance": {
                  "OpenAI": {
                    "score": 25,
                    "sources": [
                      "https://trust.openai.com/",
                      "https://openai.com/policies/supplier-security-measures",
                      "https://openai.com/enterprise-privacy",
                      "https://openai.com/global-affairs/our-approach-to-frontier-risk"
                    ],
                    "justification": "OpenAI has general security monitoring, SOC 2 compliance, and ongoing cybersecurity investments for model weights protection, but no specific public information found about continuous compromise assessment on all devices with model weights access."
                  },
                  "Anthropic": {
                    "score": 50,
                    "sources": [
                      "https://www.anthropic.com/rsp-updates",
                      "https://www.rand.org/pubs/research_reports/RRA2849-1.html",
                      "https://www.anthropic.com/news/activating-asl3-protections"
                    ],
                    "justification": "Anthropic implements comprehensive security monitoring including centralized log management, access monitoring for critical assets, and deception technology, but specific public information about ongoing compromise assessment on all devices with access is not explicitly documented."
                  },
                  "Google": {
                    "score": 0,
                    "sources": [],
                    "justification": "No specific public information found regarding ongoing compromise assessment on all devices accessing AI model weights."
                  },
                  "xAI": {
                    "score": 0,
                    "sources": [],
                    "justification": "No specific public information found regarding xAI's implementation of ongoing compromise assessment on devices with access to AI model weights."
                  },
                  "Meta": {
                    "score": 0,
                    "sources": [],
                    "justification": "No specific public information found regarding Meta's implementation of ongoing compromise assessment on all devices with access to AI model weights, as required for Security Level 4 in the RAND report."
                  }
                }
              }
            ]
          },
          {
            "name": "Supply Chain",
            "controls": [
              {
                "name": "Strict application allowlisting (especially for sandboxes)",
                "compliance": {
                  "OpenAI": {
                    "score": 50,
                    "sources": [
                      "https://openai.com/enterprise-privacy",
                      "https://web.swipeinsight.app/posts/openai-unveils-security-architecture-for-frontier-ai-model-training-7065",
                      "https://learn.microsoft.com/en-us/security/benchmark/azure/baselines/azure-openai-security-baseline"
                    ],
                    "justification": "OpenAI implements some allowlisting controls including network egress controls that only allow traffic to predefined targets and deny unlisted hosts, plus role-based access controls through Azure. However, there is limited public information specifically detailing comprehensive strict application allowlisting for sandboxes as required for SL4."
                  },
                  "Anthropic": {
                    "score": 0,
                    "sources": [
                      "https://www.rand.org/pubs/research_briefs/RBA2849-1.html",
                      "https://www.anthropic.com/news/frontier-model-security",
                      "https://www.anthropic.com/news/activating-asl3-protections"
                    ],
                    "justification": "No specific public information found addressing strict application allowlisting for sandboxes related to AI model weights security at Security Level 4 standards."
                  },
                  "Google": {
                    "score": 0,
                    "sources": [],
                    "justification": "No specific public information found regarding Google's compliance with strict application allowlisting for sandboxes related to AI model weights security at Security Level 4."
                  },
                  "xAI": {
                    "score": 0,
                    "sources": [
                      "https://www.oracle.com/news/announcement/xais-grok-models-are-now-on-oracle-cloud-infrastructure-2025-06-17/",
                      "https://embracethered.com/blog/posts/2024/security-probllms-in-xai-grok/",
                      "https://x.ai/news/grok-os"
                    ],
                    "justification": "No specific public information found regarding xAI's implementation of strict application allowlisting controls for AI model weights security. Available evidence suggests security vulnerabilities in sandboxing implementations and an open-source approach to model distribution rather than restrictive access controls."
                  },
                  "Meta": {
                    "score": 0,
                    "sources": [],
                    "justification": "No specific public information found regarding Meta's implementation of strict application allowlisting for AI model weights security, particularly for sandbox environments."
                  }
                }
              },
              {
                "name": "SLSA Level 3 specification for all software used",
                "compliance": {
                  "OpenAI": {
                    "score": 0,
                    "sources": [],
                    "justification": "No specific public information found regarding OpenAI's compliance with SLSA Level 3 specification for software used in AI model weights security as expected for Security Level 4."
                  },
                  "Anthropic": {
                    "score": 25,
                    "sources": [
                      "https://www.anthropic.com/rsp-updates",
                      "https://www.rand.org/pubs/research_briefs/RBA2849-1.html",
                      "https://www.anthropic.com/news/activating-asl3-protections"
                    ],
                    "justification": "Anthropic mentions leveraging SLSA frameworks for artifact integrity and provenance in their RSP updates, but no specific evidence of full SLSA Level 3 implementation for model weights security is publicly documented. Their ASL-3 security controls focus on model weight protection but don't explicitly detail SLSA Level 3 compliance."
                  },
                  "Google": {
                    "score": 25,
                    "sources": [
                      "https://www.rand.org/pubs/research_briefs/RBA2849-1.html",
                      "https://blog.google/technology/safety-security/introducing-googles-secure-ai-framework/",
                      "https://opensource.googleblog.com/2025/01/creating-safe-secure-ai-models.html",
                      "https://www.bankinfosecurity.com/google-ai-security-plan-bug-bounty-supply-chain-safety-a-23399"
                    ],
                    "justification": "Google has developed SLSA framework and SAIF (Secure AI Framework) for AI security, with mentions of applying SLSA to ML models and workflows. However, no specific public information confirms comprehensive SLSA Level 3 compliance for all software used in AI model weights security as required for Security Level 4."
                  },
                  "xAI": {
                    "score": 0,
                    "sources": [],
                    "justification": "No specific public information found."
                  },
                  "Meta": {
                    "score": 0,
                    "sources": [],
                    "justification": "No specific public information found regarding Meta's compliance with SLSA Level 3 specification for software used in AI model weights security."
                  }
                }
              }
            ]
          },
          {
            "name": "Security Tooling",
            "controls": [
              {
                "name": "Significant investment in advanced security systems",
                "compliance": {
                  "OpenAI": {
                    "score": 75,
                    "sources": [
                      "https://web.swipeinsight.app/posts/openai-unveils-security-architecture-for-frontier-ai-model-training-7065",
                      "https://ifp.org/an-action-plan-for-american-leadership-in-ai/",
                      "https://www.analyticsvidhya.com/blog/2024/05/openai-security-measures/",
                      "https://openai.com/global-affairs/our-approach-to-frontier-risk",
                      "https://www.rand.org/pubs/research_briefs/RBA2849-1.html"
                    ],
                    "justification": "OpenAI demonstrates significant investment in advanced security systems for model weights protection, including defense-in-depth approaches, multi-party approvals, trusted computing for GPUs, network isolation, and dedicated security teams. However, the RAND report indicates that no AI lab currently achieves SL-4 security levels, suggesting room for improvement to fully meet the stringent requirements for defending against top-tier nation-state adversaries."
                  },
                  "Anthropic": {
                    "score": 75,
                    "sources": [
                      "https://www.anthropic.com/news/frontier-model-security",
                      "https://www.anthropic.com/news/announcing-our-updated-responsible-scaling-policy",
                      "https://venturebeat.com/ai/why-anthropic-and-openai-are-obsessed-with-securing-llm-model-weights/",
                      "https://www.anthropic.com/news/activating-asl3-protections",
                      "https://www.rand.org/pubs/research_briefs/RBA2849-1.html"
                    ],
                    "justification": "Anthropic demonstrates significant security investments with dedicated CISO leadership, ASL-3 protections including egress controls and multi-party authorization, and robust model weights protection systems. However, they appear to be at ASL-3 level rather than the SL-4 equivalent described in RAND's framework."
                  },
                  "Google": {
                    "score": 75,
                    "sources": [
                      "https://www.rand.org/pubs/research_reports/RRA2849-1.html",
                      "https://cloud.google.com/blog/products/identity-security/introducing-ai-protection-security-for-the-ai-era",
                      "https://ifp.org/an-action-plan-for-american-leadership-in-ai/",
                      "https://safety.google/cybersecurity-advancements/saif/",
                      "https://deepmind.google/discover/blog/introducing-the-frontier-safety-framework/"
                    ],
                    "justification": "Google has demonstrated significant investment in advanced AI security systems through SAIF, AI Protection, Confidential Computing, and Frontier Safety Framework, showing comprehensive security measures. However, public information suggests no AI company currently operates at SL-4 level for model weights protection, indicating partial but substantial progress toward the RAND Security Level 4 standard."
                  },
                  "xAI": {
                    "score": 25,
                    "sources": [
                      "https://github.com/xai-org/grok-1/discussions/246",
                      "https://en.wikipedia.org/wiki/Grok_(chatbot)",
                      "https://x.ai/security",
                      "https://embracethered.com/blog/posts/2024/security-probllms-in-xai-grok/"
                    ],
                    "justification": "xAI has implemented basic security measures including VPN requirements, IAM controls, and cloud-based protections, but lacks evidence of the advanced security systems expected for Security Level 4. While the company made Grok-1 open-source and has general security documentation, there's no publicly available information about specialized AI model weight protection measures such as confidential computing, physical bandwidth limitations, or advanced threat protection systems specifically for model weights."
                  },
                  "Meta": {
                    "score": 25,
                    "sources": [
                      "https://www.infosecurity-magazine.com/news/meta-new-advances-ai-security/",
                      "https://thehackernews.com/2025/01/metas-llama-framework-flaw-exposes-ai.html",
                      "https://www.rand.org/pubs/research_briefs/RBA2849-1.html"
                    ],
                    "justification": "Limited public information available on Meta's specific advanced security systems for AI model weights protection. Recent security vulnerabilities and open-source model approach suggest basic security measures rather than SL4-level sophisticated infrastructure investments."
                  }
                }
              }
            ]
          },
          {
            "name": "Physical Security",
            "controls": [
              {
                "name": "Banning of unauthorized devices",
                "compliance": {
                  "OpenAI": {
                    "score": 0,
                    "sources": [],
                    "justification": "No specific public information found regarding OpenAI's compliance with 'banning unauthorized devices' as a Security Level 4 control for AI model weights security."
                  },
                  "Anthropic": {
                    "score": 75,
                    "sources": [
                      "https://www.cnbc.com/2025/03/31/anthropic-announces-updates-on-security-safeguards-for-its-ai-models.html",
                      "https://www.anthropic.com/news/activating-asl3-protections"
                    ],
                    "justification": "Anthropic has implemented comprehensive security measures including physical security controls such as 'sweeping physical offices for hidden devices' and 'technical surveillance countermeasures' to detect unauthorized surveillance devices. They have activated ASL-3 protections with over 100 security controls including endpoint software controls and binary allowlisting to prevent unauthorized device access to model weights."
                  },
                  "Google": {
                    "score": 0,
                    "sources": [],
                    "justification": "No specific public information found regarding Google's compliance with 'banning of unauthorized devices' controls as defined for Security Level 4 in the RAND report on securing AI model weights."
                  },
                  "xAI": {
                    "score": 0,
                    "sources": [
                      "https://www.rand.org/pubs/research_reports/RRA2849-1.html",
                      "https://x.ai/security",
                      "https://www.rand.org/pubs/research_briefs/RBA2849-1.html"
                    ],
                    "justification": "No specific public information found regarding xAI's compliance with 'Banning of unauthorized devices' controls for AI model weights security as described in RAND Security Level 4."
                  },
                  "Meta": {
                    "score": 0,
                    "sources": [],
                    "justification": "No specific public information found regarding Meta's policies or implementation of unauthorized device banning controls for AI model weights security."
                  }
                }
              }
            ]
          }
        ]
      },
      {
        "name": "Personnel Security",
        "subcategories": [
          {
            "name": "Filtering and Monitoring",
            "controls": [
              {
                "name": "Preventing third-party access and reporting suspected illegitimate incidents",
                "compliance": {
                  "OpenAI": {
                    "score": 0,
                    "sources": [],
                    "justification": "No specific public information found regarding OpenAI's compliance with Security Level 4 requirements for preventing third-party access and reporting suspected illegitimate incidents related to AI model weights security as defined in the RAND report."
                  },
                  "Anthropic": {
                    "score": 75,
                    "sources": [
                      "https://www.anthropic.com/news/frontier-model-security",
                      "https://venturebeat.com/ai/why-anthropic-and-openai-are-obsessed-with-securing-llm-model-weights/",
                      "https://www.anthropic.com/transparency",
                      "https://www.anthropic.com/news/activating-asl3-protections"
                    ],
                    "justification": "Anthropic demonstrates strong security measures for preventing third-party access to model weights, including ASL-3 protections with over 100 security controls, two-party authorization, egress bandwidth controls, and endpoint software controls. However, no specific public information was found regarding formal reporting of suspected illegitimate incidents to external authorities."
                  },
                  "Google": {
                    "score": 0,
                    "sources": [],
                    "justification": "No specific public information found addressing third-party access controls and incident reporting procedures specifically for AI model weights security that would meet Security Level 4 requirements."
                  },
                  "xAI": {
                    "score": 25,
                    "sources": [
                      "https://cybermaterial.com/xai-api-key-leak-exposes-sensitive-data/",
                      "https://x.ai/legal/data-processing-addendum",
                      "https://x.ai/security",
                      "https://blog.gitguardian.com/xai-secret-leak-disclosure/"
                    ],
                    "justification": "xAI has established basic security incident notification processes and a bug bounty program, but a major API key leak in 2024 exposed unreleased AI models for over two months with delayed remediation, indicating significant gaps in preventing third-party access to model weights."
                  },
                  "Meta": {
                    "score": 25,
                    "sources": [
                      "https://github.com/meta-llama/llama-models/blob/main/models/llama3_2/USE_POLICY.md",
                      "https://www.rand.org/pubs/research_reports/RRA2849-1.html",
                      "https://www.bankinfosecurity.com/meta-plans-to-restrict-high-risk-ai-models-a-27447"
                    ],
                    "justification": "Meta recently announced its Frontier AI Framework with risk categories and plans to limit internal access to high-risk models and implement security measures to prevent leaks. However, no specific public documentation was found regarding comprehensive third-party access prevention controls or structured incident reporting mechanisms specifically for model weights security as expected at Security Level 4."
                  }
                }
              },
              {
                "name": "Advanced insider threat program",
                "compliance": {
                  "OpenAI": {
                    "score": 50,
                    "sources": [
                      "https://web.swipeinsight.app/posts/openai-unveils-security-architecture-for-frontier-ai-model-training-7065",
                      "https://openai.com/careers/security-engineer-insider-threat-detection-and-response/",
                      "https://www.ccn.com/news/technology/openai-hiring-insider-risk-investigator-collaborating-white-house/",
                      "https://openai.com/global-affairs/our-approach-to-frontier-risk"
                    ],
                    "justification": "OpenAI has publicly committed to insider threat safeguards and is actively hiring specialized insider threat personnel, but lacks detailed public disclosure of advanced program capabilities required for Security Level 4."
                  },
                  "Anthropic": {
                    "score": 75,
                    "sources": [
                      "https://www.rand.org/pubs/research_reports/RRA2849-1.html",
                      "https://www.anthropic.com/news/frontier-model-security",
                      "https://www.anthropic.com/transparency/voluntary-commitments",
                      "https://www.anthropic.com/rsp-updates",
                      "https://www.anthropic.com/news/activating-asl3-protections"
                    ],
                    "justification": "Anthropic has implemented several key components of an advanced insider threat program including structured insider threat programs with risk reporting, multi-party authorization for model weight access, employee education on insider risk, monitoring and detection capabilities, and comprehensive security controls. However, full Security Level 4 compliance against sophisticated nation-state actors requires additional measures not clearly documented in public information."
                  },
                  "Google": {
                    "score": 25,
                    "sources": [
                      "https://safety.google/cybersecurity-advancements/saif/",
                      "https://www.rand.org/pubs/research_reports/RRA2849-1.html",
                      "https://cloud.google.com/security/consulting/mandiant-insider-threats"
                    ],
                    "justification": "Google provides general insider threat services through Mandiant and has security frameworks for AI, but no specific public information was found detailing advanced insider threat programs specifically for AI model weights security meeting Security Level 4 requirements."
                  },
                  "xAI": {
                    "score": 0,
                    "sources": [],
                    "justification": "No specific public information found regarding xAI's implementation of advanced insider threat programs for AI model weights security as defined in Security Level 4 requirements."
                  },
                  "Meta": {
                    "score": 0,
                    "sources": [],
                    "justification": "No specific public information found regarding Meta's advanced insider threat program implementation for AI model weights security as required by RAND Security Level 4."
                  }
                }
              },
              {
                "name": "Occasional employee integrity testing",
                "compliance": {
                  "OpenAI": {
                    "score": 0,
                    "sources": [],
                    "justification": "No specific public information found regarding OpenAI's implementation of occasional employee integrity testing for AI model weights security as described in Security Level 4 requirements."
                  },
                  "Anthropic": {
                    "score": 0,
                    "sources": [
                      "https://www.anthropic.com/news/activating-asl3-protections",
                      "https://www.anthropic.com/news/reflections-on-our-responsible-scaling-policy",
                      "https://www.rand.org/pubs/research_reports/RRA2849-1.html",
                      "https://www.rand.org/pubs/research_briefs/RBA2849-1.html"
                    ],
                    "justification": "No specific public information found regarding Anthropic's implementation of occasional employee integrity testing for AI model weights security."
                  },
                  "Google": {
                    "score": 0,
                    "sources": [],
                    "justification": "No specific public information found regarding Google's implementation of 'occasional employee integrity testing' specifically for AI model weights security as described in the RAND Security Level 4 controls."
                  },
                  "xAI": {
                    "score": 0,
                    "sources": [],
                    "justification": "No specific public information found regarding xAI's implementation of occasional employee integrity testing for AI model weights security as described in RAND's Security Level 4 controls."
                  },
                  "Meta": {
                    "score": 0,
                    "sources": [],
                    "justification": "No specific public information found."
                  }
                }
              }
            ]
          }
        ]
      },
      {
        "name": "Security Assurance and Testing",
        "subcategories": [
          {
            "name": "Red-Teaming and Penetration Testing",
            "controls": [
              {
                "name": "Ongoing research and red-teaming to identify potential attack methods on the weight interface(s)",
                "compliance": {
                  "OpenAI": {
                    "score": 50,
                    "sources": [
                      "https://hiddenlayer.com/innovation-hub/three-distinct-categories-of-ai-red-teaming/",
                      "https://transparency.oecd.ai/reports/b167db92-67c8-47d8-966a-427e2ce8c008",
                      "https://openai.com/global-affairs/our-approach-to-frontier-risk",
                      "https://openai.com/index/advancing-red-teaming-with-people-and-ai/"
                    ],
                    "justification": "OpenAI demonstrates ongoing red-teaming activities and security research for AI models generally, but limited specific public information about weight interface attack research. They conduct continuous adversarial testing and have dedicated security programs, but specific weight security research details are not extensively documented publicly."
                  },
                  "Anthropic": {
                    "score": 50,
                    "sources": [
                      "https://www.anthropic.com/news/frontier-threats-red-teaming-for-ai-safety",
                      "https://www.anthropic.com/news/strategic-warning-for-ai-risk-progress-and-insights-from-our-frontier-red-team",
                      "https://www.anthropic.com/research/red-teaming-language-models-to-reduce-harms-methods-scaling-behaviors-and-lessons-learned",
                      "https://www.anthropic.com/news/challenges-in-red-teaming-ai-systems",
                      "https://www.anthropic.com/news/activating-asl3-protections"
                    ],
                    "justification": "Anthropic demonstrates substantial red-teaming activities but limited specific evidence of weight interface attack research. They conduct extensive red-teaming programs including frontier threats, multimodal approaches, and model evaluations, with dedicated red-teaming teams and government partnerships. However, most publicly available information focuses on model behavior/capability assessment rather than specific research into weight interface attack vectors."
                  },
                  "Google": {
                    "score": 75,
                    "sources": [
                      "https://cloud.google.com/blog/transform/prompt-findings-our-ai-red-teams-first-report-qa",
                      "https://safety.google/cybersecurity-advancements/saif/",
                      "https://blog.google/technology/safety-security/googles-ai-red-team-the-ethical-hackers-making-ai-safer/",
                      "https://saif.google/secure-ai-framework/risks"
                    ],
                    "justification": "Google has demonstrated strong commitment to AI red-teaming through their dedicated AI Red Team, published research on AI security threats, and development of SAIF framework. However, there's no specific public evidence of ongoing research focused explicitly on weight interface attack methods as would be expected for Security Level 4."
                  },
                  "xAI": {
                    "score": 0,
                    "sources": [],
                    "justification": "No specific public information found regarding xAI's ongoing research and red-teaming activities to identify potential attack methods on weight interfaces."
                  },
                  "Meta": {
                    "score": 50,
                    "sources": [
                      "https://github.com/meta-llama/PurpleLlama",
                      "https://venturebeat.com/security/red-team-ai-now-to-build-safer-smarter-models-tomorrow/",
                      "https://ailabwatch.org/companies/meta",
                      "https://huggingface.co/meta-llama/Llama-4-Scout-17B-16E-Instruct",
                      "https://mindgard.ai/blog/what-is-ai-red-teaming",
                      "https://github.com/meta-llama/llama-models/blob/main/models/llama4/MODEL_CARD.md"
                    ],
                    "justification": "Meta demonstrates partial compliance through their PurpleLlama project and ongoing red-teaming activities for model safety, but evidence suggests focus is primarily on model behavior rather than specific weight interface security as outlined in Security Level 4 requirements."
                  }
                }
              },
              {
                "name": "Ensuring physical security through red-teaming",
                "compliance": {
                  "OpenAI": {
                    "score": 50,
                    "sources": [
                      "https://openai.com/security/",
                      "https://hiddenlayer.com/innovation-hub/three-distinct-categories-of-ai-red-teaming/",
                      "https://web.swipeinsight.app/posts/openai-unveils-security-architecture-for-frontier-ai-model-training-7065",
                      "https://transparency.oecd.ai/reports/b167db92-67c8-47d8-966a-427e2ce8c008"
                    ],
                    "justification": "OpenAI conducts internal and external red-teaming and has advanced physical security measures for datacenters, but public information lacks specific details about red-teaming focused on physical security of model weights infrastructure as expected for Security Level 4."
                  },
                  "Anthropic": {
                    "score": 75,
                    "sources": [
                      "https://www.anthropic.com/rsp-updates",
                      "https://www.anthropic.com/news/activating-asl3-protections"
                    ],
                    "justification": "Anthropic has implemented comprehensive physical security measures including regular Technical Surveillance Countermeasures (TSCM) sweeps and physical security red-teaming as part of their ASL-3 Security Standard. They conduct 'regular sweeps of physical premises for intruders' and have over 100 different security controls targeting model weight protection."
                  },
                  "Google": {
                    "score": 25,
                    "sources": [
                      "https://cloud.google.com/docs/security/overview/whitepaper",
                      "https://blog.google/technology/safety-security/googles-ai-red-team-the-ethical-hackers-making-ai-safer/",
                      "https://www.rand.org/pubs/research_reports/RRA2849-1.html"
                    ],
                    "justification": "Google has established AI red-teaming capabilities and robust data center physical security, but no specific public information was found about physical security red-teaming for AI model weights as defined in the RAND SL4 framework."
                  },
                  "xAI": {
                    "score": 0,
                    "sources": [],
                    "justification": "No specific public information found."
                  },
                  "Meta": {
                    "score": 0,
                    "sources": [],
                    "justification": "No specific public information found."
                  }
                }
              },
              {
                "name": "Experience dealing with intelligence agencies",
                "compliance": {
                  "OpenAI": {
                    "score": 50,
                    "sources": [
                      "https://www.nist.gov/news-events/news/2024/12/pre-deployment-evaluation-openais-o1-model",
                      "https://en.wikipedia.org/wiki/OpenAI",
                      "https://www.rand.org/pubs/research_briefs/RBA2849-1.html"
                    ],
                    "justification": "OpenAI demonstrates some experience with intelligence agencies through board appointments (former NSA head), government contracts, and collaboration with security agencies on evaluations. However, insufficient public information exists about specific security controls or practices related to intelligence agency experience for model weights protection."
                  },
                  "Anthropic": {
                    "score": 75,
                    "sources": [
                      "https://fedscoop.com/anthropic-eyes-fedramp-accreditation-in-quest-to-sell-more-ai-to-government/",
                      "https://www.anthropic.com/news/expanding-access-to-claude-for-government",
                      "https://www.anthropic.com/news/claude-gov-models-for-u-s-national-security-customers"
                    ],
                    "justification": "Anthropic demonstrates substantial experience dealing with intelligence agencies through deployed Claude Gov models at highest security levels, senior staff with NSC experience, and active IC marketplace presence. However, as a relatively young company, depth of institutional experience may be limited compared to traditional defense contractors."
                  },
                  "Google": {
                    "score": 0,
                    "sources": [],
                    "justification": "No specific public information found about Google's direct experience dealing with intelligence agencies regarding AI model weights security, as would be expected for Security Level 4 compliance in the RAND framework."
                  },
                  "xAI": {
                    "score": 0,
                    "sources": [],
                    "justification": "No specific public information found regarding xAI's experience dealing with intelligence agencies for AI model weights security."
                  },
                  "Meta": {
                    "score": 75,
                    "sources": [
                      "https://www.cio.com/article/3599448/meta-offers-llama-ai-to-us-government-for-national-security.html",
                      "https://spectrum.ieee.org/ai-used-by-military",
                      "https://www.bloomberg.com/news/articles/2024-11-04/meta-opens-llama-ai-models-to-us-defense-agencies-contractors",
                      "https://about.fb.com/news/2024/11/open-source-ai-america-global-security/",
                      "https://decrypt.co/290197/meta-ai-us-defense"
                    ],
                    "justification": "Meta demonstrates substantial engagement with intelligence agencies through direct partnerships with U.S. government agencies and defense contractors to provide Llama AI models for national security applications, working with agencies across multiple cloud platforms and contractors like Lockheed Martin and Palantir."
                  }
                }
              }
            ]
          },
          {
            "name": "Risk and Security Assessments",
            "controls": [
              {
                "name": "Automated weight exfiltration attempts",
                "compliance": {
                  "OpenAI": {
                    "score": 0,
                    "sources": [
                      "https://www.rand.org/pubs/research_reports/RRA2849-1.html",
                      "https://web.swipeinsight.app/posts/openai-unveils-security-architecture-for-frontier-ai-model-training-7065",
                      "https://www.rand.org/pubs/research_briefs/RBA2849-1.html"
                    ],
                    "justification": "No specific public information found regarding OpenAI's compliance with automated weight exfiltration detection controls as defined in the RAND Security Level 4 framework."
                  },
                  "Anthropic": {
                    "score": 75,
                    "sources": [
                      "https://www.rand.org/pubs/research_briefs/RBA2849-1.html",
                      "https://www.anthropic.com/news/reflections-on-our-responsible-scaling-policy",
                      "https://www.rand.org/pubs/research_reports/RRA2849-1.html",
                      "https://www.anthropic.com/news/activating-asl3-protections"
                    ],
                    "justification": "Anthropic has implemented ASL-3 security standards including egress bandwidth controls that detect unusual bandwidth usage and block suspicious traffic to prevent model weight exfiltration, with multi-party authorization and time-bounded access controls specifically targeting insider device compromise risks. While not explicitly at SL-4 level (which targets state-sponsored groups), their implemented controls demonstrate strong defenses against automated exfiltration attempts."
                  },
                  "Google": {
                    "score": 0,
                    "sources": [],
                    "justification": "No specific public information found about Google's implementation of automated weight exfiltration security controls for Security Level 4 as defined in the RAND report."
                  },
                  "xAI": {
                    "score": 0,
                    "sources": [],
                    "justification": "No specific public information found regarding xAI's compliance with automated weight exfiltration prevention measures as described in RAND Security Level 4 requirements."
                  },
                  "Meta": {
                    "score": 0,
                    "sources": [],
                    "justification": "No specific public information found about Meta's compliance with automated weight exfiltration attempt detection controls as defined in the RAND Security Level 4 framework."
                  }
                }
              },
              {
                "name": "Manual weight exfiltration attempts",
                "compliance": {
                  "OpenAI": {
                    "score": 25,
                    "sources": [
                      "https://trust.openai.com/",
                      "https://transparency.oecd.ai/reports/b167db92-67c8-47d8-966a-427e2ce8c008",
                      "https://www.rand.org/pubs/research_briefs/RBA2849-1.html"
                    ],
                    "justification": "OpenAI publicly acknowledges treating model weights as core IP with strict access controls and insider threat programs, but lacks specific public disclosure of manual weight exfiltration prevention measures expected for SL4 compliance."
                  },
                  "Anthropic": {
                    "score": 75,
                    "sources": [
                      "https://venturebeat.com/ai/why-anthropic-and-openai-are-obsessed-with-securing-llm-model-weights/",
                      "https://www.anthropic.com/news/reflections-on-our-responsible-scaling-policy",
                      "https://www.anthropic.com/news/activating-asl3-protections"
                    ],
                    "justification": "Anthropic has implemented comprehensive ASL-3 security measures including egress bandwidth controls, multi-party authorization for model weight access, time-bounded access controls, and over 100 security controls targeting sophisticated non-state actors. Their CISO dedicates significant resources to protecting model weights specifically against manual exfiltration attempts."
                  },
                  "Google": {
                    "score": 0,
                    "sources": [
                      "https://www.rand.org/pubs/research_reports/RRA2849-1.html",
                      "https://cloud.google.com/blog/products/identity-security/introducing-ai-protection-security-for-the-ai-era",
                      "https://safety.google/cybersecurity-advancements/saif/",
                      "https://cloud.google.com/use-cases/secure-ai-framework",
                      "https://www.rand.org/pubs/research_briefs/RBA2849-1.html"
                    ],
                    "justification": "No specific public information found about Google's internal controls for preventing manual exfiltration of AI model weights by employees or sophisticated attackers, which is a key requirement for RAND Security Level 4."
                  },
                  "xAI": {
                    "score": 0,
                    "sources": [],
                    "justification": "No specific public information found regarding xAI's implementation of manual weight exfiltration prevention controls as would be expected for RAND Security Level 4 compliance."
                  },
                  "Meta": {
                    "score": 0,
                    "sources": [],
                    "justification": "No specific public information found."
                  }
                }
              },
              {
                "name": "Compliance with the FedRAMP High standards for security",
                "compliance": {
                  "OpenAI": {
                    "score": 50,
                    "sources": [
                      "https://openai.com/enterprise-privacy",
                      "https://www.rand.org/pubs/research_reports/RRA2849-1.html",
                      "https://devblogs.microsoft.com/azuregov/azure-openai-fedramp-high-for-government/",
                      "https://web.swipeinsight.app/posts/openai-unveils-security-architecture-for-frontier-ai-model-training-7065"
                    ],
                    "justification": "OpenAI operates through Microsoft's Azure OpenAI Service which has achieved FedRAMP High authorization, providing strong foundational security. However, public information shows gaps in direct model weights protection measures specific to Security Level 4 requirements like network isolation, insider threat programs, and confidential computing implementations."
                  },
                  "Anthropic": {
                    "score": 75,
                    "sources": [
                      "https://www.anthropic.com/news/claude-on-google-cloud-fedramp-high",
                      "https://www.rand.org/pubs/research_reports/RRA2849-1.html",
                      "https://aws.amazon.com/blogs/publicsector/accelerating-government-innovation-amazon-bedrock-models-get-fedramp-high-and-dod-il-4-5-approval-in-aws-govcloud-us/",
                      "https://www.anthropic.com/news/claude-in-amazon-bedrock-fedramp-high"
                    ],
                    "justification": "Anthropic has achieved FedRAMP High authorization for Claude models through both Amazon Bedrock and Google Cloud's Vertex AI platforms, demonstrating compliance with stringent government security requirements. While this represents significant progress toward the security measures expected for RAND's Security Level 4 (designed to foil standard operations by leading cyber-capable institutions), it is delivered through cloud partners rather than direct authorization."
                  },
                  "Google": {
                    "score": 25,
                    "sources": [
                      "https://fedscoop.com/google-earns-fedramp-high-authorization-for-more-than-100-additional-commercial-services/",
                      "https://www.nextgov.com/acquisition/2024/05/google-cloud-obtains-fedramp-high-certification-more-100-services/396767/",
                      "https://cloud.google.com/vertex-ai/docs/general/vertexai-security-controls",
                      "https://cloud.google.com/security/compliance/fedramp"
                    ],
                    "justification": "Google has achieved FedRAMP High authorization for Vertex AI platform and implements general security controls, but no specific public information was found regarding AI model weights security measures that would align with RAND Security Level 4 requirements."
                  },
                  "xAI": {
                    "score": 0,
                    "sources": [
                      "https://www.rand.org/pubs/research_reports/RRA2849-1.html",
                      "https://x.ai/security",
                      "https://www.rand.org/pubs/research_briefs/RBA2849-1.html"
                    ],
                    "justification": "No specific public information found regarding xAI's FedRAMP High compliance or authorization status. While xAI has published general security measures on their website, there is no evidence of FedRAMP High certification or compliance with the specific AI model weights security requirements described in Security Level 4 of the RAND report."
                  },
                  "Meta": {
                    "score": 25,
                    "sources": [
                      "https://fedscoop.com/claude-llama-ai-models-amazon-government-cloud/",
                      "https://aws.amazon.com/blogs/publicsector/accelerating-government-innovation-amazon-bedrock-models-get-fedramp-high-and-dod-il-4-5-approval-in-aws-govcloud-us/",
                      "https://www.nextgov.com/artificial-intelligence/2025/06/aws-govcloud-gets-high-level-security-approvals-anthropic-and-meta-ai-models/405995/?oref=ng-skybox-hp"
                    ],
                    "justification": "Meta's Llama models have achieved FedRAMP High authorization through cloud providers like AWS GovCloud, but this represents hosted service compliance rather than Meta's direct adherence to FedRAMP High standards for model weights security controls. No specific evidence found of Meta's own infrastructure meeting FedRAMP High requirements for AI model weights protection."
                  }
                }
              }
            ]
          },
          {
            "name": "Security Team Capacity",
            "controls": [
              {
                "name": "General increased capacity (compared with SL3)",
                "compliance": {
                  "OpenAI": {
                    "score": 0,
                    "sources": [],
                    "justification": "No specific public information found regarding OpenAI's implementation of the 'General increased capacity' control specifically defined for Security Level 4 in the RAND report."
                  },
                  "Anthropic": {
                    "score": 0,
                    "sources": [
                      "https://www.anthropic.com/news/frontier-model-security",
                      "https://www.rand.org/pubs/research_briefs/RBA2849-1.html",
                      "https://www.anthropic.com/news/activating-asl3-protections"
                    ],
                    "justification": "No specific public information found regarding Anthropic's compliance with the specific 'General increased capacity' control for Security Level 4 as defined in the RAND report. While Anthropic has implemented ASL-3 security measures, detailed SL4-specific compliance information is not publicly available."
                  },
                  "Google": {
                    "score": 25,
                    "sources": [
                      "https://ai.google/responsibility/safety/",
                      "https://safety.google/cybersecurity-advancements/saif/",
                      "https://cloud.google.com/blog/products/identity-security/introducing-ai-protection-security-for-the-ai-era",
                      "https://www.rand.org/pubs/research_briefs/RBA2849-1.html"
                    ],
                    "justification": "Google has comprehensive AI security frameworks like SAIF and AI Protection, but no specific public information was found detailing SL4-level model weights security measures such as physical bandwidth limitations or hardware-secured weights."
                  },
                  "xAI": {
                    "score": 0,
                    "sources": [],
                    "justification": "No specific public information found regarding xAI's compliance with Security Level 4 'General increased capacity' requirements as defined in the RAND report."
                  },
                  "Meta": {
                    "score": 0,
                    "sources": [],
                    "justification": "No specific public information found regarding Meta's AI model weights security posture or compliance with RAND Security Level 4 requirements for 'General increased capacity'."
                  }
                }
              },
              {
                "name": "Greater concrete experience with APTs (compared with SL3)",
                "compliance": {
                  "OpenAI": {
                    "score": 25,
                    "sources": [
                      "https://www.microsoft.com/en-us/security/blog/2024/02/14/staying-ahead-of-threat-actors-in-the-age-of-ai/",
                      "https://www.rand.org/pubs/research_reports/RRA2849-1.html",
                      "https://openai.com/index/red-teaming-network/",
                      "https://openai.com/global-affairs/our-approach-to-frontier-risk",
                      "https://www.rand.org/pubs/research_briefs/RBA2849-1.html"
                    ],
                    "justification": "OpenAI demonstrates some APT-related security capabilities through their cybersecurity grant program, bug bounty program, and red teaming network, but lacks specific public evidence of concrete experience defending against Advanced Persistent Threats. Their security measures appear focused on general AI safety red teaming and bug bounties rather than specialized APT defense capabilities that would be expected for SL4."
                  },
                  "Anthropic": {
                    "score": 25,
                    "sources": [
                      "https://www.anthropic.com/news/activating-asl3-protections",
                      "https://www.anthropic.com/news/challenges-in-red-teaming-ai-systems",
                      "https://forum.effectivealtruism.org/posts/fJycPfYuBHKoai98q/ai-companies-are-not-on-track-to-secure-model-weights",
                      "https://www.rand.org/pubs/research_briefs/RBA2849-1.html"
                    ],
                    "justification": "Anthropic has established red teaming programs and expanded security teams, but lacks documented state-level APT experience required for SL4. Current protections target sophisticated non-state actors rather than leading state-sponsored groups."
                  },
                  "Google": {
                    "score": 75,
                    "sources": [
                      "https://cloud.google.com/transform/how-google-does-it-red-teaming-at-scale",
                      "https://blog.google/technology/safety-security/googles-ai-red-team-the-ethical-hackers-making-ai-safer/",
                      "https://www.rand.org/pubs/research_briefs/RBA2849-1.html"
                    ],
                    "justification": "Google demonstrates substantial APT experience through established red team operations simulating nation-state and APT group attacks, world-class threat intelligence teams including Mandiant, and specialized AI red team capabilities with AI-specific expertise for complex technical attacks."
                  },
                  "xAI": {
                    "score": 0,
                    "sources": [],
                    "justification": "No specific public information found about xAI's concrete experience with Advanced Persistent Threats (APTs) related to AI model weights security, which is required for Security Level 4 compliance according to the RAND report."
                  },
                  "Meta": {
                    "score": 0,
                    "sources": [],
                    "justification": "No specific public information found regarding Meta's APT experience related to AI model weights security as defined in the RAND Security Level 4 requirements."
                  }
                }
              },
              {
                "name": "Zero-day vulnerability discovery capabilities",
                "compliance": {
                  "OpenAI": {
                    "score": 0,
                    "sources": [],
                    "justification": "No specific public information found regarding OpenAI's compliance with Security Level 4 zero-day vulnerability discovery capabilities for AI model weights security as defined in the RAND report."
                  },
                  "Anthropic": {
                    "score": 25,
                    "sources": [
                      "https://www.rand.org/pubs/research_briefs/RBA2849-1.html",
                      "https://www.anthropic.com/transparency/voluntary-commitments",
                      "https://www.anthropic.com/responsible-disclosure-policy",
                      "https://www.anthropic.com/news/activating-asl3-protections"
                    ],
                    "justification": "Anthropic has established vulnerability reporting programs and bug bounties, but no specific public information indicates active zero-day vulnerability discovery capabilities to proactively identify unknown vulnerabilities before sophisticated attackers, as would be expected for RAND SL4 compliance."
                  },
                  "Google": {
                    "score": 50,
                    "sources": [
                      "https://cloud.google.com/blog/transform/prompt-findings-our-ai-red-teams-first-report-qa",
                      "https://thehackernews.com/2024/11/googles-ai-tool-big-sleep-finds-zero.html",
                      "https://blog.google/technology/safety-security/googles-ai-red-team-the-ethical-hackers-making-ai-safer/"
                    ],
                    "justification": "Google demonstrates strong zero-day vulnerability discovery capabilities through their Big Sleep AI agent (successfully finding SQLite vulnerabilities) and established AI Red Team conducting adversarial testing. However, no specific evidence found regarding these capabilities being applied to protect their own AI model weights specifically."
                  },
                  "xAI": {
                    "score": 0,
                    "sources": [],
                    "justification": "No specific public information found regarding xAI's implementation of zero-day vulnerability discovery capabilities for AI model weights security."
                  },
                  "Meta": {
                    "score": 0,
                    "sources": [
                      "https://engineering.fb.com/2025/02/13/security/looking-back-at-our-bug-bounty-program-in-2024/",
                      "https://www.rand.org/pubs/research_reports/RRA2849-1.html",
                      "https://www.securityweek.com/meta-paid-out-over-2-3-million-in-bug-bounties-in-2024/"
                    ],
                    "justification": "No specific public information found regarding Meta's internal zero-day vulnerability discovery capabilities for AI model weights security as expected for Security Level 4."
                  }
                }
              },
              {
                "name": "The security team is empowered to not compromise security over other stakeholders.",
                "compliance": {
                  "OpenAI": {
                    "score": 25,
                    "sources": [
                      "https://web.swipeinsight.app/posts/openai-unveils-security-architecture-for-frontier-ai-model-training-7065",
                      "https://www.rand.org/pubs/research_reports/RRA2849-1.html",
                      "https://venturebeat.com/ai/why-anthropic-and-openai-are-obsessed-with-securing-llm-model-weights/",
                      "https://openai.com/global-affairs/our-approach-to-frontier-risk",
                      "https://www.rand.org/pubs/research_briefs/RBA2849-1.html"
                    ],
                    "justification": "While OpenAI demonstrates some security measures for model weights protection, there is no specific public information indicating that their security team is formally empowered to override other stakeholders' decisions on security matters. The RAND report defines SL4 as protecting against 'standard operations by leading cyber capable institutions' and requires comprehensive security measures that would typically include empowered security teams, but OpenAI's public statements focus on technical controls rather than organizational governance structures."
                  },
                  "Anthropic": {
                    "score": 75,
                    "sources": [
                      "https://www.rand.org/pubs/research_briefs/RBA2849-1.html",
                      "https://venturebeat.com/ai/why-anthropic-and-openai-are-obsessed-with-securing-llm-model-weights/",
                      "https://www.rand.org/pubs/research_reports/RRA2849-1.html",
                      "https://www.anthropic.com/news/activating-asl3-protections"
                    ],
                    "justification": "Anthropic demonstrates strong alignment with SL-4 requirements through implemented ASL-3 security controls including over 100 security measures, two-party authorization for model weight access, enhanced change management, endpoint controls, and egress bandwidth controls. Their CISO reports directly to the CEO and prioritizes model weight protection as the top security concern. However, they have not yet fully achieved all SL-4 benchmarks which the RAND report estimates would take 2-3 years to implement."
                  },
                  "Google": {
                    "score": 0,
                    "sources": [],
                    "justification": "No specific public information found regarding Google's security team empowerment structure for AI model weights security that would meet Security Level 4 requirements."
                  },
                  "xAI": {
                    "score": 0,
                    "sources": [
                      "https://x.ai/security"
                    ],
                    "justification": "No specific public information found regarding xAI's security team empowerment structure or their authority to prioritize AI model weights security over other stakeholders."
                  },
                  "Meta": {
                    "score": 0,
                    "sources": [
                      "https://www.infosecurity-magazine.com/news/meta-new-advances-ai-security/",
                      "https://www.npr.org/2025/05/31/nx-s1-5407870/meta-ai-facebook-instagram-risks"
                    ],
                    "justification": "No specific public information found regarding Meta's security team empowerment structure for AI model weights protection. Available information suggests recent changes may have actually reduced centralized security authority."
                  }
                }
              }
            ]
          }
        ]
      },
      {
        "name": "Other Organization Policies",
        "subcategories": [
          {
            "name": "",
            "controls": [
              {
                "name": "Designating sensitive details of the weight security system",
                "compliance": {
                  "OpenAI": {
                    "score": 0,
                    "sources": [],
                    "justification": "No specific public information found regarding OpenAI's approach to designating sensitive details of their weight security system as outlined in the RAND report's Security Level 4 requirements."
                  },
                  "Anthropic": {
                    "score": 0,
                    "sources": [],
                    "justification": "No specific public information found regarding Anthropic's compliance with the specific Security Level 4 control of 'Designating sensitive details of the weight security system' as defined in the RAND report."
                  },
                  "Google": {
                    "score": 0,
                    "sources": [
                      "https://blog.google/technology/ai/responsible-ai-2024-report-ongoing-work/",
                      "https://cloud.google.com/blog/products/identity-security/introducing-ai-protection-security-for-the-ai-era",
                      "https://www.rand.org/pubs/research_briefs/RBA2849-1.html"
                    ],
                    "justification": "No specific public information found regarding Google's implementation of 'Designating sensitive details of the weight security system' controls at Security Level 4 as defined in the RAND report."
                  },
                  "xAI": {
                    "score": 0,
                    "sources": [],
                    "justification": "No specific public information found."
                  },
                  "Meta": {
                    "score": 0,
                    "sources": [],
                    "justification": "No specific public information found regarding Meta's designation of sensitive details of their weight security system as would be expected for Security Level 4 compliance in the RAND report framework."
                  }
                }
              },
              {
                "name": "Vetting of investors and other positions of influence",
                "compliance": {
                  "OpenAI": {
                    "score": 0,
                    "sources": [],
                    "justification": "No specific public information found regarding OpenAI's vetting of investors and other positions of influence related to AI model weights security as would be expected for Security Level 4."
                  },
                  "Anthropic": {
                    "score": 0,
                    "sources": [],
                    "justification": "No specific public information found regarding Anthropic's implementation of investor vetting or controls over positions of influence as a security control for AI model weights protection."
                  },
                  "Google": {
                    "score": 0,
                    "sources": [],
                    "justification": "No specific public information found regarding Google's vetting of investors and other positions of influence related to AI model weights security."
                  },
                  "xAI": {
                    "score": 0,
                    "sources": [],
                    "justification": "No specific public information found regarding xAI's implementation of investor vetting controls or security procedures for positions of influence as required by Security Level 4 in the RAND report."
                  },
                  "Meta": {
                    "score": 0,
                    "sources": [],
                    "justification": "No specific public information found about Meta's policies for vetting investors or other positions of influence related to AI model weights security as described in the RAND Security Level 4 requirements."
                  }
                }
              },
              {
                "name": "Prioritizing leak prevention over other organizational goals",
                "compliance": {
                  "OpenAI": {
                    "score": 0,
                    "sources": [],
                    "justification": "No specific public information found regarding OpenAI's explicit prioritization of leak prevention over other organizational goals at Security Level 4 standards."
                  },
                  "Anthropic": {
                    "score": 75,
                    "sources": [
                      "https://www.rand.org/pubs/research_briefs/RBA2849-1.html",
                      "https://venturebeat.com/ai/why-anthropic-and-openai-are-obsessed-with-securing-llm-model-weights/",
                      "https://www.anthropic.com/news/activating-asl3-protections"
                    ],
                    "justification": "Anthropic demonstrates strong prioritization of model weight security with over 100 security controls, dedicating nearly half of their CISO's time to weight protection, and implementing ASL-3 security standards. However, they appear to be at ASL-3 level rather than the SL-4 equivalent, which would protect against state-sponsored groups."
                  },
                  "Google": {
                    "score": 0,
                    "sources": [],
                    "justification": "No specific public information found."
                  },
                  "xAI": {
                    "score": 0,
                    "sources": [
                      "https://blog.gitguardian.com/xai-secret-leak-disclosure/",
                      "https://gbhackers.com/xai-api-key-leak-exposes-proprietary-language-models/",
                      "https://x.ai/security",
                      "https://krebsonsecurity.com/2025/05/xai-dev-leaks-api-key-for-private-spacex-tesla-llms/"
                    ],
                    "justification": "xAI demonstrated poor leak prevention posture through a major 2-month exposure of API keys providing access to 60+ private models including SpaceX and Tesla data, delayed response to security reports, lack of clear security contacts, and insufficient internal monitoring - directly contradicting SL4 requirements for prioritizing leak prevention."
                  },
                  "Meta": {
                    "score": 0,
                    "sources": [
                      "https://www.theregister.com/2023/03/08/meta_llama_ai_leak/",
                      "https://aragonresearch.com/metas-new-ai-model-leaks/",
                      "https://www.rand.org/pubs/research_briefs/RBA2849-1.html"
                    ],
                    "justification": "No specific public information found about Meta's compliance with Security Level 4 requirements for prioritizing leak prevention over other organizational goals, though historical model leaks suggest challenges in this area."
                  }
                }
              },
              {
                "name": "Four independent security layers",
                "compliance": {
                  "OpenAI": {
                    "score": 75,
                    "sources": [
                      "https://web.swipeinsight.app/posts/openai-unveils-security-architecture-for-frontier-ai-model-training-7065",
                      "https://openai.com/global-affairs/our-approach-to-frontier-risk",
                      "https://www.rand.org/pubs/research_briefs/RBA2849-1.html"
                    ],
                    "justification": "OpenAI implements a multi-layered defense-in-depth security architecture with four documented layers for protecting model weights: Authorization (multi-party approvals), Access (private-linked storage with authentication), Egress Controls (network restrictions), and Detection (monitoring controls). However, specific compliance with all SL4 requirements from the RAND framework cannot be fully verified from public information."
                  },
                  "Anthropic": {
                    "score": 75,
                    "sources": [
                      "https://www.anthropic.com/rsp-updates",
                      "https://time.com/7287806/anthropic-claude-4-opus-safety-bio-risk/",
                      "https://www.rand.org/pubs/research_reports/RRA2849-1.html",
                      "https://www.anthropic.com/news/activating-asl3-protections"
                    ],
                    "justification": "Anthropic implements a defense-in-depth strategy with multiple security layers including access controls, real-time classifiers, monitoring systems, and incident response. However, publicly available information doesn't explicitly confirm four fully independent security layers matching RAND's Security Level 4 specifications."
                  },
                  "Google": {
                    "score": 50,
                    "sources": [
                      "https://safety.google/cybersecurity-advancements/saif/",
                      "https://deepmind.google/discover/blog/advancing-geminis-security-safeguards/",
                      "https://cloud.google.com/blog/products/identity-security/introducing-ai-protection-security-for-the-ai-era"
                    ],
                    "justification": "Google implements defense-in-depth with multiple security layers for AI systems but no specific public documentation confirms four independent security layers for model weights as per RAND SL4."
                  },
                  "xAI": {
                    "score": 0,
                    "sources": [],
                    "justification": "No specific public information found regarding xAI's compliance with or implementation of the 'Four independent security layers' control as described in the RAND report's Security Level 4 framework for AI model weights security."
                  },
                  "Meta": {
                    "score": 0,
                    "sources": [],
                    "justification": "No specific public information found regarding Meta's compliance with 'four independent security layers' for Security Level 4 as described in the RAND report. The search results provided general information about the RAND security levels but did not contain specific details about this control or Meta's posture regarding it."
                  }
                }
              }
            ]
          }
        ]
      }
    ]
  },
  {
    "level": 5,
    "description": "A system that could plausibly be claimed to thwart most top-priority operations by the top cyber-capable institutions (OC5). This includes the handful of operations prioritized by the world's most capable nation-states.",
    "categories": [
      {
        "name": "Implementation of Previous Security Levels",
        "subcategories": [
          {
            "name": "",
            "controls": [
              {
                "name": "The organization has implemented all the controls from SL1\u00e2\u20ac\u201cSL4.",
                "compliance": {
                  "OpenAI": {
                    "score": 50,
                    "sources": [
                      "https://www.analyticsvidhya.com/blog/2024/05/openai-security-measures/",
                      "https://web.swipeinsight.app/posts/openai-unveils-security-architecture-for-frontier-ai-model-training-7065",
                      "https://openai.com/enterprise-privacy"
                    ],
                    "justification": "OpenAI has publicly disclosed several security measures including defense-in-depth approach, multi-party approvals, access controls, and network egress controls. However, specific implementation details of SL1-SL4 controls are not publicly documented."
                  },
                  "Anthropic": {
                    "score": 0,
                    "sources": [
                      "https://www.anthropic.com/news/announcing-our-updated-responsible-scaling-policy",
                      "https://ai-2027.com/research/security-forecast",
                      "https://www.anthropic.com/news/activating-asl3-protections"
                    ],
                    "justification": "No specific public information found regarding Anthropic's compliance with RAND's SL1-SL4 controls. While Anthropic has implemented their own ASL-3 security standards for model weights protection, there is no publicly available mapping or confirmation of compliance with the specific RAND framework controls."
                  },
                  "Google": {
                    "score": 0,
                    "sources": [
                      "https://safety.google/cybersecurity-advancements/saif/",
                      "https://www.rand.org/pubs/research_reports/RRA2849-1.html",
                      "https://www.rand.org/pubs/research_briefs/RBA2849-1.html"
                    ],
                    "justification": "No specific public information found regarding Google's compliance with RAND's SL1-SL4 controls or achievement of SL5 security level for AI model weights protection."
                  },
                  "xAI": {
                    "score": 0,
                    "sources": [
                      "https://www.rand.org/pubs/research_reports/RRA2849-1.html",
                      "https://x.ai/news/grok-os",
                      "https://www.rand.org/pubs/perspectives/PEA3776-1.html",
                      "https://x.ai/security",
                      "https://www.rand.org/pubs/research_briefs/RBA2849-1.html"
                    ],
                    "justification": "No specific public information found regarding xAI's implementation of RAND report SL1-SL4 security controls for AI model weights. xAI's open release of Grok-1 weights contradicts core weight protection principles."
                  },
                  "Meta": {
                    "score": 0,
                    "sources": [],
                    "justification": "No specific public information found regarding Meta's compliance with RAND SL1-SL4 model weights security controls."
                  }
                }
              }
            ]
          }
        ]
      },
      {
        "name": "Weight Security",
        "subcategories": [
          {
            "name": "Weight Storage",
            "controls": [
              {
                "name": "Extreme isolation of weight storage (completely isolated network)",
                "compliance": {
                  "OpenAI": {
                    "score": 25,
                    "sources": [
                      "https://www.analyticsvidhya.com/blog/2024/05/openai-security-measures/",
                      "https://web.swipeinsight.app/posts/openai-unveils-security-architecture-for-frontier-ai-model-training-7065",
                      "https://www.rand.org/pubs/research_briefs/RBA2849-1.html"
                    ],
                    "justification": "OpenAI implements network isolation and access controls but has not achieved the 'completely isolated networks' required for SL5. Their current measures appear to be at SL3-4 level, with SL5 requiring years more development according to industry estimates."
                  },
                  "Anthropic": {
                    "score": 0,
                    "sources": [
                      "https://www.rand.org/pubs/research_briefs/RBA2849-1.html",
                      "https://venturebeat.com/ai/why-anthropic-and-openai-are-obsessed-with-securing-llm-model-weights/",
                      "https://www.anthropic.com/news/activating-asl3-protections"
                    ],
                    "justification": "While Anthropic has implemented ASL-3 security measures including egress bandwidth controls and enhanced weight access controls, there is no evidence of completely isolated networks for weight storage as required for Security Level 5."
                  },
                  "Google": {
                    "score": 0,
                    "sources": [
                      "https://deepmind.google/discover/blog/updating-the-frontier-safety-framework/",
                      "https://www.rand.org/pubs/research_reports/RRA2849-1.html",
                      "https://deepmind.google/discover/blog/taking-a-responsible-path-to-agi/"
                    ],
                    "justification": "No specific public information found about Google implementing completely isolated networks for AI model weights storage as described in Security Level 5 of the RAND report."
                  },
                  "xAI": {
                    "score": 0,
                    "sources": [],
                    "justification": "No specific public information found."
                  },
                  "Meta": {
                    "score": 0,
                    "sources": [
                      "https://www.rand.org/pubs/research_reports/RRA2849-1.html",
                      "https://thehackernews.com/2025/04/meta-launches-llamafirewall-framework.html",
                      "https://www.artificialintelligence-news.com/news/meta-beefs-up-ai-security-new-llama-tools/",
                      "https://www.securityweek.com/meta-releases-llama-ai-open-source-protection-tools/",
                      "https://www.rand.org/pubs/research_briefs/RBA2849-1.html"
                    ],
                    "justification": "No specific public information found regarding Meta's implementation of Security Level 5 'extreme isolation of weight storage' with completely isolated networks as described in the RAND report. While Meta has implemented various AI security measures like LlamaFirewall and safeguards, there is no public documentation of the specific network isolation requirements for this security level."
                  }
                }
              },
              {
                "name": "Advanced preventive measures for side-channel attacks (e.g., noise injection, time delays, and other tools)",
                "compliance": {
                  "OpenAI": {
                    "score": 0,
                    "sources": [
                      "https://www.okoone.com/spark/technology-innovation/how-side-channel-attacks-bypass-ai-assistant-chat-encryption/",
                      "https://venturebeat.com/ai/why-anthropic-and-openai-are-obsessed-with-securing-llm-model-weights/",
                      "https://www.ericsson.com/en/blog/2023/4/side-channel-analysis"
                    ],
                    "justification": "No specific public information found regarding OpenAI's implementation of advanced preventive measures for side-channel attacks (noise injection, time delays) to protect AI model weights."
                  },
                  "Anthropic": {
                    "score": 0,
                    "sources": [],
                    "justification": "No specific public information found about Anthropic implementing advanced preventive measures for side-channel attacks such as noise injection or time delays as described in RAND Security Level 5 requirements."
                  },
                  "Google": {
                    "score": 25,
                    "sources": [
                      "https://www.okoone.com/spark/technology-innovation/how-side-channel-attacks-bypass-ai-assistant-chat-encryption/",
                      "https://safety.google/cybersecurity-advancements/saif/",
                      "https://cloud.google.com/blog/products/identity-security/how-confidential-computing-lays-the-foundation-for-trusted-ai/",
                      "https://cloud.google.com/security/products/confidential-computing"
                    ],
                    "justification": "Google demonstrates some foundational protection measures through confidential computing with trusted execution environments (TEEs) and hardware-based security, but lacks specific publicly documented advanced side-channel countermeasures like noise injection or time delays for model weights protection. While Google implements confidential computing for AI workloads and has some side-channel attack research, there's no clear evidence of the comprehensive advanced preventive measures expected for Security Level 5."
                  },
                  "xAI": {
                    "score": 0,
                    "sources": [],
                    "justification": "No specific public information found regarding xAI's implementation of advanced preventive measures for side-channel attacks on AI model weights."
                  },
                  "Meta": {
                    "score": 0,
                    "sources": [],
                    "justification": "No specific public information found regarding Meta's implementation of advanced preventive measures for side-channel attacks (noise injection, time delays, etc.) for AI model weights security."
                  }
                }
              },
              {
                "name": "Formal hardware verification of key components",
                "compliance": {
                  "OpenAI": {
                    "score": 0,
                    "sources": [],
                    "justification": "No specific public information found about formal hardware verification requirements for Security Level 5 or OpenAI's implementation of such measures."
                  },
                  "Anthropic": {
                    "score": 0,
                    "sources": [],
                    "justification": "No specific public information found regarding Anthropic implementing formal hardware verification of key components as would be expected for Security Level 5. While Anthropic has implemented ASL-3 security measures including confidential computing and hardware-based security controls, there is no evidence of the sophisticated formal hardware verification typically required for SL5 protections against top-tier nation-state attacks."
                  },
                  "Google": {
                    "score": 0,
                    "sources": [],
                    "justification": "No specific public information found regarding Google's implementation of formal hardware verification of key components for AI model weights security as described in RAND Security Level 5."
                  },
                  "xAI": {
                    "score": 0,
                    "sources": [],
                    "justification": "No specific public information found about xAI's implementation of formal hardware verification of key components for AI model weights security as outlined in the RAND report's Security Level 5 requirements."
                  },
                  "Meta": {
                    "score": 0,
                    "sources": [],
                    "justification": "No specific public information found regarding Meta's implementation of formal hardware verification for AI model weights security components."
                  }
                }
              }
            ]
          },
          {
            "name": "Physical Security",
            "controls": [
              {
                "name": "Increased significant guarding (compared with SL4) via multiple armed guards and digital security systems at all times.",
                "compliance": {
                  "OpenAI": {
                    "score": 25,
                    "sources": [
                      "https://www.yahoo.com/tech/openai-seems-hiring-undercover-security-183023279.html",
                      "https://sfstandard.com/2024/06/08/openai-security-guards-secretive-mission-office/",
                      "https://openai.com/global-affairs/our-approach-to-frontier-risk",
                      "https://www.1ai.net/en/12777.html"
                    ],
                    "justification": "OpenAI employs plainclothes security guards at its San Francisco offices, but no evidence suggests the multiple armed guards and enhanced digital security systems characteristic of Security Level 5 requirements."
                  },
                  "Anthropic": {
                    "score": 0,
                    "sources": [
                      "https://www.anthropic.com/news/frontier-model-security",
                      "https://www.cnbc.com/2025/03/31/anthropic-announces-updates-on-security-safeguards-for-its-ai-models.html",
                      "https://venturebeat.com/ai/why-anthropic-and-openai-are-obsessed-with-securing-llm-model-weights/",
                      "https://www.anthropic.com/news/activating-asl3-protections"
                    ],
                    "justification": "No specific public information found regarding armed guards or physical security systems at Anthropic facilities, despite extensive public documentation of their ASL-3 security measures which focus on digital security and technical surveillance countermeasures rather than armed guards."
                  },
                  "Google": {
                    "score": 0,
                    "sources": [
                      "https://www.google.com/about/datacenters/data-security/",
                      "https://workspace.google.com/learn-more/security/security-whitepaper/page-4/",
                      "https://blog.google/inside-google/infrastructure/how-data-center-security-works/",
                      "https://cloud.google.com/blog/products/gcp/google-shares-data-center-security-and-design-best-practices"
                    ],
                    "justification": "No specific public information found regarding multiple armed guards specifically for AI model weights security as required by RAND Security Level 5. While Google employs extensive physical security including guards, vehicle barriers, biometric access, and 24/7 monitoring at data centers, there is no publicly disclosed information about armed guards specifically protecting AI model weights or meeting the heightened Security Level 5 requirements."
                  },
                  "xAI": {
                    "score": 25,
                    "sources": [
                      "https://x.ai/security"
                    ],
                    "justification": "xAI's security documentation mentions 'full time security personnel' and 'defense-in-depth access controls' at their data centers, but does not specifically describe multiple armed guards or the comprehensive physical security measures expected for Security Level 5."
                  },
                  "Meta": {
                    "score": 0,
                    "sources": [],
                    "justification": "No specific public information found regarding Meta's implementation of armed guards or enhanced physical security systems for AI model weights protection as described in Security Level 5."
                  }
                }
              },
              {
                "name": "Supervised access for everyone",
                "compliance": {
                  "OpenAI": {
                    "score": 0,
                    "sources": [],
                    "justification": "No specific public information found regarding OpenAI's compliance with 'Supervised access for everyone' as a Security Level 5 control from the RAND AI model weights security framework."
                  },
                  "Anthropic": {
                    "score": 0,
                    "sources": [],
                    "justification": "No specific public information found regarding 'supervised access for everyone' as a Security Level 5 control for AI model weights security."
                  },
                  "Google": {
                    "score": 0,
                    "sources": [],
                    "justification": "No specific public information found regarding Google's compliance with 'Supervised access for everyone' as a Security Level 5 control for AI model weights security."
                  },
                  "xAI": {
                    "score": 0,
                    "sources": [
                      "https://www.rand.org/pubs/research_briefs/RBA2849-1.html",
                      "https://en.wikipedia.org/wiki/XAI_(company)",
                      "https://www.oracle.com/artificial-intelligence/ai-open-weights-models/",
                      "https://winbuzzer.com/2025/05/26/elon-musks-grok-ai-use-in-us-gov-sparks-ethics-security-alarms-xcxwbn/"
                    ],
                    "justification": "No specific public information found regarding xAI's implementation of supervised access controls for AI model weights as outlined in RAND's Security Level 5 requirements. Recent security incidents including unauthorized system prompt modifications and inadvertent API key exposure suggest potential gaps in access controls."
                  },
                  "Meta": {
                    "score": 0,
                    "sources": [],
                    "justification": "No specific public information found regarding Meta's compliance with 'Supervised access for everyone' control as defined for Security Level 5 in the RAND report on AI model weights security."
                  }
                }
              },
              {
                "name": "Routine rigorous device inspections",
                "compliance": {
                  "OpenAI": {
                    "score": 0,
                    "sources": [],
                    "justification": "No specific public information found regarding OpenAI's implementation of routine rigorous device inspections for AI model weights security as defined in Security Level 5 specifications."
                  },
                  "Anthropic": {
                    "score": 75,
                    "sources": [
                      "https://www.anthropic.com/rsp-updates",
                      "https://www.cnbc.com/2025/03/31/anthropic-announces-updates-on-security-safeguards-for-its-ai-models.html",
                      "https://www.webpronews.com/anthropic-announces-security-measures-to-combat-corporate-espionage/"
                    ],
                    "justification": "Anthropic has implemented Technical Surveillance Countermeasures (TSCM) including regular sweeps of physical premises for hidden surveillance devices and intruders, using advanced detection equipment. However, public information lacks specific details about the rigor, frequency, and comprehensiveness of device inspections compared to Security Level 5 standards."
                  },
                  "Google": {
                    "score": 0,
                    "sources": [],
                    "justification": "No specific public information found."
                  },
                  "xAI": {
                    "score": 0,
                    "sources": [
                      "https://www.rand.org/pubs/research_reports/RRA2849-1.html",
                      "https://x.ai/security",
                      "https://www.rand.org/pubs/research_briefs/RBA2849-1.html"
                    ],
                    "justification": "No specific public information found regarding xAI's implementation of routine rigorous device inspections for AI model weights security as required for RAND Security Level 5."
                  },
                  "Meta": {
                    "score": 0,
                    "sources": [],
                    "justification": "No specific public information found regarding Meta's implementation of routine rigorous device inspections for AI model weights security as described in Security Level 5 of the RAND report."
                  }
                }
              },
              {
                "name": "Disabling of most communication at the hardware level",
                "compliance": {
                  "OpenAI": {
                    "score": 0,
                    "sources": [],
                    "justification": "No specific public information found regarding OpenAI's implementation of hardware-level communication disabling controls for AI model weights security as described in Security Level 5 of the RAND report."
                  },
                  "Anthropic": {
                    "score": 25,
                    "sources": [
                      "https://www.rand.org/pubs/research_briefs/RBA2849-1.html",
                      "https://www.anthropic.com/news/frontier-model-security",
                      "https://www.anthropic.com/news/activating-asl3-protections"
                    ],
                    "justification": "Anthropic has implemented some relevant hardware-related security measures including egress bandwidth controls that restrict data flow from secure computing environments. However, there is no specific public information indicating they have implemented the 'disabling of most communication at the hardware level' as required for Security Level 5 in the RAND report."
                  },
                  "Google": {
                    "score": 0,
                    "sources": [],
                    "justification": "No specific public information found regarding Google's implementation of hardware-level communication disabling for AI model weights security as described in the RAND Security Level 5 requirements."
                  },
                  "xAI": {
                    "score": 0,
                    "sources": [
                      "https://www.rand.org/pubs/research_reports/RRA2849-1.html",
                      "https://x.ai/security",
                      "https://www.rand.org/pubs/research_briefs/RBA2849-1.html"
                    ],
                    "justification": "No specific public information found regarding xAI's implementation of hardware-level communication disabling controls for AI model weights security as required for Security Level 5. While xAI's security page mentions standard security practices like TLS encryption and data center controls, there is no evidence of the specialized hardware-level communication restrictions that the RAND report indicates are necessary for the highest security level against nation-state threats."
                  },
                  "Meta": {
                    "score": 0,
                    "sources": [],
                    "justification": "No specific public information found."
                  }
                }
              }
            ]
          },
          {
            "name": "Permitted Interfaces",
            "controls": [
              {
                "name": "Strict limitation of external connections to the completely isolated network",
                "compliance": {
                  "OpenAI": {
                    "score": 25,
                    "sources": [
                      "https://www.analyticsvidhya.com/blog/2024/05/openai-security-measures/",
                      "https://web.swipeinsight.app/posts/openai-unveils-security-architecture-for-frontier-ai-model-training-7065",
                      "https://www.rand.org/pubs/research_briefs/RBA2849-1.html"
                    ],
                    "justification": "OpenAI has announced network isolation measures but no specific evidence of completely isolated networks as required for SL5. Their public statements discuss 'strong network and tenant isolation' and 'deny-by-default egress policy' but fall short of the RAND SL5 requirement for 'secure, completely isolated networks' for model weights training and research."
                  },
                  "Anthropic": {
                    "score": 25,
                    "sources": [
                      "https://www.rand.org/pubs/research_briefs/RBA2849-1.html",
                      "https://www.rand.org/pubs/research_reports/RRA2849-1.html",
                      "https://www.anthropic.com/news/activating-asl3-protections"
                    ],
                    "justification": "Anthropic has implemented egress bandwidth controls to limit outbound network traffic as part of their ASL-3 security measures, but this falls short of the complete network isolation expected at Security Level 5. Their controls focus on rate limiting rather than strict isolation."
                  },
                  "Google": {
                    "score": 0,
                    "sources": [],
                    "justification": "No specific public information found."
                  },
                  "xAI": {
                    "score": 0,
                    "sources": [],
                    "justification": "No specific public information found."
                  },
                  "Meta": {
                    "score": 0,
                    "sources": [
                      "https://engineering.fb.com/2024/10/15/data-infrastructure/metas-open-ai-hardware-vision/",
                      "https://ailabwatch.org/companies/meta",
                      "https://engineering.fb.com/2024/08/05/data-center-engineering/roce-network-distributed-ai-training-at-scale/"
                    ],
                    "justification": "No specific public information found regarding Meta's implementation of completely isolated networks for AI model weights protection. While Meta describes network isolation for AI training infrastructure and mentions security measures, there is no concrete evidence of Security Level 5 isolated network controls specifically for model weights."
                  }
                }
              }
            ]
          },
          {
            "name": "Access Control",
            "controls": [
              {
                "name": "Irrecoverable key policy (barring alternative access or key retrieval systems)",
                "compliance": {
                  "OpenAI": {
                    "score": 0,
                    "sources": [
                      "https://techcommunity.microsoft.com/blog/azure-ai-services-blog/azure-ai-confidential-inferencing-preview/4248181",
                      "https://openai.com/enterprise-privacy",
                      "https://web.swipeinsight.app/posts/openai-unveils-security-architecture-for-frontier-ai-model-training-7065",
                      "https://www.rand.org/pubs/research_briefs/RBA2849-1.html"
                    ],
                    "justification": "No specific public information found regarding OpenAI's implementation of irrecoverable key policies for AI model weights security as described in RAND's Security Level 5 requirements."
                  },
                  "Anthropic": {
                    "score": 0,
                    "sources": [
                      "https://venturebeat.com/ai/why-anthropic-and-openai-are-obsessed-with-securing-llm-model-weights/",
                      "https://www.rand.org/pubs/research_reports/RRA2849-1.html",
                      "https://www.anthropic.com/news/activating-asl3-protections"
                    ],
                    "justification": "No specific public information found regarding Anthropic's implementation of irrecoverable key policies (barring alternative access or key retrieval systems). While Anthropic has implemented ASL-3 security measures including two-party authorization for model weight access and over 100 security controls, there is no publicly available documentation specifically addressing irrecoverable key policies as would be expected for Security Level 5."
                  },
                  "Google": {
                    "score": 0,
                    "sources": [],
                    "justification": "No specific public information found regarding Google's implementation of irrecoverable key policies for AI model weights security. While Google has comprehensive encryption key management systems and AI security frameworks like SAIF, there is no publicly available documentation confirming the use of irrecoverable keys specifically for protecting AI model weights."
                  },
                  "xAI": {
                    "score": 0,
                    "sources": [
                      "https://gbhackers.com/xai-api-key-leak-exposes-proprietary-language-models/",
                      "https://krebsonsecurity.com/2025/05/xai-dev-leaks-api-key-for-private-spacex-tesla-llms/",
                      "https://www.rand.org/pubs/research_briefs/RBA2849-1.html"
                    ],
                    "justification": "No specific public information found regarding xAI's implementation of irrecoverable key policies for AI model weights security. Recent security incidents involving exposed API keys for months demonstrate poor key management practices inconsistent with Security Level 5 requirements."
                  },
                  "Meta": {
                    "score": 0,
                    "sources": [],
                    "justification": "No specific public information found regarding Meta's implementation of irrecoverable key policies for AI model weights security at Security Level 5 standards."
                  }
                }
              }
            ]
          },
          {
            "name": "Standard Compliance",
            "controls": [
              {
                "name": "Protection equivalent to that required for Top Secret (TS)/Sensitive Compartmented Information (SCI)",
                "compliance": {
                  "OpenAI": {
                    "score": 50,
                    "sources": [
                      "https://venturebeat.com/ai/why-anthropic-and-openai-are-obsessed-with-securing-llm-model-weights/",
                      "https://www.analyticsvidhya.com/blog/2024/05/openai-security-measures/",
                      "https://web.swipeinsight.app/posts/openai-unveils-security-architecture-for-frontier-ai-model-training-7065",
                      "https://www.rand.org/pubs/research_briefs/RBA2849-1.html"
                    ],
                    "justification": "OpenAI has implemented comprehensive security measures including multi-party authorization, defense-in-depth architecture, and trusted computing initiatives, but no public evidence indicates achievement of SL5-level protections against the most capable nation-state threats."
                  },
                  "Anthropic": {
                    "score": 50,
                    "sources": [
                      "https://www.rand.org/pubs/research_reports/RRA2849-1.html",
                      "https://venturebeat.com/ai/why-anthropic-and-openai-are-obsessed-with-securing-llm-model-weights/",
                      "https://www.anthropic.com/rsp-updates",
                      "https://www.anthropic.com/news/activating-asl3-protections",
                      "https://www.rand.org/pubs/research_briefs/RBA2849-1.html"
                    ],
                    "justification": "Anthropic has implemented ASL-3 security measures including multi-party authorization, egress controls, and over 100 security controls targeting sophisticated non-state actors. However, ASL-3 explicitly excludes protection against state-level threats, while SL-5 requires defense against top-priority nation-state operations."
                  },
                  "Google": {
                    "score": 25,
                    "sources": [
                      "https://www.rand.org/pubs/research_reports/RRA2849-1.html",
                      "https://cloud.google.com/blog/products/identity-security/introducing-ai-protection-security-for-the-ai-era",
                      "https://safety.google/cybersecurity-advancements/saif/",
                      "https://cloud.google.com/blog/products/identity-security/how-confidential-computing-lays-the-foundation-for-trusted-ai/",
                      "https://www.rand.org/pubs/research_briefs/RBA2849-1.html"
                    ],
                    "justification": "Google has implemented confidential computing, AI Protection framework, and SAIF security measures, but lacks publicly documented evidence of comprehensive physical air-gapping, bandwidth limitations, and other advanced measures required for RAND Security Level 5/TS-SCI equivalent protection."
                  },
                  "xAI": {
                    "score": 0,
                    "sources": [],
                    "justification": "No specific public information found regarding xAI's implementation of Top Secret/SCI-equivalent security measures for AI model weights protection as defined in the RAND Security Level 5 framework."
                  },
                  "Meta": {
                    "score": 0,
                    "sources": [
                      "https://www.infosecurity-magazine.com/news/meta-new-advances-ai-security/",
                      "https://www.edgeless.systems/solutions/ai-model-protection",
                      "https://about.fb.com/news/2024/11/open-source-ai-america-global-security/"
                    ],
                    "justification": "No specific public information found regarding Meta implementing TS/SCI-equivalent protection measures for AI model weights. Available evidence shows actual security breaches of model weights and standard cybersecurity measures rather than classification-level protections."
                  }
                }
              }
            ]
          }
        ]
      },
      {
        "name": "AI Model Resilience",
        "subcategories": [
          {
            "name": "Oracle Protection",
            "controls": [
              {
                "name": "Constant inference time",
                "compliance": {
                  "OpenAI": {
                    "score": 0,
                    "sources": [
                      "https://openai.com/index/trading-inference-time-compute-for-adversarial-robustness/",
                      "https://www.rand.org/pubs/research_reports/RRA2849-1.html",
                      "https://www.rand.org/pubs/research_briefs/RBA2849-1.html"
                    ],
                    "justification": "No specific public information found regarding OpenAI's implementation of 'constant inference time' security measures as defined in the RAND report's Security Level 5 framework for AI model weights protection."
                  },
                  "Anthropic": {
                    "score": 0,
                    "sources": [],
                    "justification": "No specific public information found addressing 'constant inference time' as a security control for AI model weights in either the RAND report or Anthropic's public security documentation."
                  },
                  "Google": {
                    "score": 0,
                    "sources": [],
                    "justification": "No specific public information found regarding Google's compliance with 'constant inference time' requirements as specified in RAND's Security Level 5 for AI model weights security."
                  },
                  "xAI": {
                    "score": 0,
                    "sources": [],
                    "justification": "No specific public information found regarding xAI's compliance with constant inference time requirements or Security Level 5 measures from the RAND report."
                  },
                  "Meta": {
                    "score": 0,
                    "sources": [],
                    "justification": "No specific public information found regarding Meta's compliance with 'constant inference time' requirements for AI model weights security as defined in the RAND report's Security Level 5."
                  }
                }
              }
            ]
          }
        ]
      },
      {
        "name": "Security of Network and Other (Nonweight) Sensitive Assets",
        "subcategories": [
          {
            "name": "Supply Chain",
            "controls": [
              {
                "name": "Strong limitations on software providers (e.g., only developed internally or by an extremely reliable source)",
                "compliance": {
                  "OpenAI": {
                    "score": 0,
                    "sources": [
                      "https://web.swipeinsight.app/posts/openai-unveils-security-architecture-for-frontier-ai-model-training-7065",
                      "https://www.lesswrong.com/posts/gG4EhhWtD2is9Cx7m/implications-of-the-ai-security-gap",
                      "https://openai.com/global-affairs/our-approach-to-frontier-risk"
                    ],
                    "justification": "No specific public information found regarding OpenAI's compliance with 'Strong limitations on software providers' as required for Security Level 5 in the RAND report."
                  },
                  "Anthropic": {
                    "score": 0,
                    "sources": [],
                    "justification": "No specific public information found regarding Anthropic's compliance with Security Level 5 software provider limitations requiring internal development or extremely reliable sources."
                  },
                  "Google": {
                    "score": 0,
                    "sources": [],
                    "justification": "No specific public information found regarding Google's implementation of strong limitations on software providers (only internally developed or extremely reliable sources) for AI model weights security as specified in Security Level 5 of the RAND report."
                  },
                  "xAI": {
                    "score": 0,
                    "sources": [],
                    "justification": "No specific public information found."
                  },
                  "Meta": {
                    "score": 0,
                    "sources": [
                      "https://www.rand.org/pubs/research_reports/RRA2849-1.html",
                      "https://en.wikipedia.org/wiki/Llama_(language_model)",
                      "https://theconversation.com/meta-now-allows-military-agencies-to-access-its-ai-software-it-poses-a-moral-dilemma-for-everybody-who-uses-it-243250",
                      "https://www.reversinglabs.com/blog/secure-ai-supply-chain-mlbom",
                      "https://www.rand.org/pubs/research_briefs/RBA2849-1.html"
                    ],
                    "justification": "No specific public information found regarding Meta implementing strong limitations on software providers for AI model weights security. Meta's Llama models are distributed as open-weight models with widespread accessibility, which contradicts the Security Level 5 requirement for extremely restricted software provider limitations."
                  }
                }
              },
              {
                "name": "Strong limitations on hardware providers (e.g., only developed internally or by an extremely reliable source)",
                "compliance": {
                  "OpenAI": {
                    "score": 0,
                    "sources": [],
                    "justification": "No specific public information found regarding OpenAI implementing strong limitations on hardware providers or developing AI infrastructure internally/through extremely reliable sources as required for Security Level 5."
                  },
                  "Anthropic": {
                    "score": 0,
                    "sources": [
                      "https://www.anthropic.com/research/confidential-inference-trusted-vms",
                      "https://www.anthropic.com/news/activating-asl3-protections",
                      "https://www.rand.org/pubs/research_briefs/RBA2849-1.html",
                      "https://www.anthropic.com/news/frontier-model-security"
                    ],
                    "justification": "No specific public information found about Anthropic implementing strong limitations on hardware providers as required for Security Level 5. While Anthropic has implemented ASL-3 security measures and mentions confidential computing and hardware security considerations, there is no evidence of the extreme hardware provider restrictions that Security Level 5 requires."
                  },
                  "Google": {
                    "score": 25,
                    "sources": [
                      "https://www.theinformation.com/articles/to-reduce-ai-costs-google-wants-to-ditch-broadcom-as-its-tpu-server-chip-supplier",
                      "https://www.ainvest.com/news/google-ai-chip-strategy-partnering-mediatek-gen-tpu-production-2503/",
                      "https://en.wikipedia.org/wiki/Tensor_Processing_Unit",
                      "https://www.theregister.com/2023/09/22/google_broadcom_tpus/"
                    ],
                    "justification": "Google develops TPUs internally but relies on external partners like Broadcom, MediaTek, and TSMC for key components and fabrication. While they design their own AI hardware, they don't meet the strict 'only developed internally or by an extremely reliable source' requirement for Security Level 5."
                  },
                  "xAI": {
                    "score": 0,
                    "sources": [
                      "https://research.contrary.com/company/xai",
                      "https://www.oracle.com/artificial-intelligence/ai-open-weights-models/",
                      "https://www.rtinsights.com/elon-musks-xai-releases-grok-1-model-to-public-for-open-development/",
                      "https://siliconangle.com/2024/09/03/elon-musks-xai-launches-colossus-ai-training-system-100000-nvidia-chips/"
                    ],
                    "justification": "No specific public information found about xAI implementing strong limitations on hardware providers. xAI has taken an open-source approach with Grok, releasing model weights publicly, and uses standard Nvidia hardware infrastructure including 100,000+ H100 GPUs in their Colossus system."
                  },
                  "Meta": {
                    "score": 0,
                    "sources": [
                      "https://engineering.fb.com/2024/10/15/data-infrastructure/metas-open-ai-hardware-vision/",
                      "https://atscaleconference.com/meta-capacity-planning-for-ai-training-hardware/"
                    ],
                    "justification": "Meta explicitly embraces open hardware development and vendor-agnostic approaches. Their infrastructure uses multiple external vendors (NVIDIA, AMD, Broadcom) and they actively promote open standards like OCP, directly contradicting the Security Level 5 requirement for internal-only or extremely reliable hardware providers."
                  }
                }
              }
            ]
          }
        ]
      },
      {
        "name": "Personnel Security",
        "subcategories": [
          {
            "name": "Personal Protection",
            "controls": [
              {
                "name": "Proactive protection of executives and individuals handling sensitive materials",
                "compliance": {
                  "OpenAI": {
                    "score": 0,
                    "sources": [
                      "https://forum.effectivealtruism.org/posts/fJycPfYuBHKoai98q/ai-companies-are-not-on-track-to-secure-model-weights",
                      "https://openai.com/global-affairs/our-approach-to-frontier-risk",
                      "https://www.rand.org/pubs/research_briefs/RBA2849-1.html"
                    ],
                    "justification": "No specific public information found regarding OpenAI's proactive protection of executives and individuals handling sensitive materials related to AI model weights security at Security Level 5."
                  },
                  "Anthropic": {
                    "score": 0,
                    "sources": [],
                    "justification": "No specific public information found regarding proactive protection of executives and individuals handling sensitive materials related to AI model weights security."
                  },
                  "Google": {
                    "score": 0,
                    "sources": [],
                    "justification": "No specific public information found."
                  },
                  "xAI": {
                    "score": 0,
                    "sources": [],
                    "justification": "No specific public information found regarding xAI's executive protection measures or security practices for personnel handling AI model weights."
                  },
                  "Meta": {
                    "score": 0,
                    "sources": [],
                    "justification": "No specific public information found regarding Meta's proactive protection of executives and individuals handling sensitive AI model weights materials."
                  }
                }
              }
            ]
          }
        ]
      },
      {
        "name": "Security Assurance and Testing",
        "subcategories": [
          {
            "name": "Red-Teaming and Penetration Testing",
            "controls": [
              {
                "name": "Proactive search for crucial vulnerabilities (e.g., zero-days)",
                "compliance": {
                  "OpenAI": {
                    "score": 25,
                    "sources": [
                      "https://openai.com/index/red-teaming-network/",
                      "https://arxiv.org/html/2503.16431v1",
                      "https://openai.com/global-affairs/our-approach-to-frontier-risk"
                    ],
                    "justification": "OpenAI has established bug bounty programs, external red teaming, and cybersecurity initiatives, but there is no specific public evidence of proactive zero-day vulnerability hunting specifically focused on model weights security as expected for Security Level 5."
                  },
                  "Anthropic": {
                    "score": 0,
                    "sources": [
                      "https://www.anthropic.com/news/strategic-warning-for-ai-risk-progress-and-insights-from-our-frontier-red-team",
                      "https://www.anthropic.com/news/activating-asl3-protections",
                      "https://www.anthropic.com/transparency/voluntary-commitments",
                      "https://www.rand.org/pubs/research_briefs/RBA2849-1.html"
                    ],
                    "justification": "No specific public information found about proactive vulnerability research or zero-day hunting programs specifically for AI model weights security as would be expected for Security Level 5."
                  },
                  "Google": {
                    "score": 25,
                    "sources": [
                      "https://safety.google/cybersecurity-advancements/saif/",
                      "https://thehackernews.com/2024/11/googles-ai-tool-big-sleep-finds-zero.html",
                      "https://www.rand.org/pubs/research_reports/RRA2849-1.html"
                    ],
                    "justification": "Google demonstrates proactive vulnerability detection capabilities through Big Sleep AI tool that discovered zero-day vulnerabilities, but no specific evidence found of proactive vulnerability search specifically focused on AI model weights security as outlined in RAND Security Level 5."
                  },
                  "xAI": {
                    "score": 0,
                    "sources": [],
                    "justification": "No specific public information found."
                  },
                  "Meta": {
                    "score": 50,
                    "sources": [
                      "https://engineering.fb.com/2025/04/29/ai-research/autopatchbench-benchmark-ai-powered-security-fixes/",
                      "https://thehackernews.com/2025/01/metas-llama-framework-flaw-exposes-ai.html",
                      "https://www.infosecurity-magazine.com/news/meta-new-advances-ai-security/",
                      "https://thehackernews.com/2025/04/meta-launches-llamafirewall-framework.html"
                    ],
                    "justification": "Meta demonstrates some proactive security practices including developing AutoPatchBench for vulnerability assessment, LlamaFirewall for threat detection, and responsive patching of discovered vulnerabilities. However, no specific evidence found of systematic proactive zero-day vulnerability hunting specifically targeting AI model weights security as would be expected for Security Level 5."
                  }
                }
              }
            ]
          },
          {
            "name": "Maintenance",
            "controls": [
              {
                "name": "Security is strongly prioritized over availability (e.g., barring connecting external devices to the completely isolated network to debug a critical production issue).",
                "compliance": {
                  "OpenAI": {
                    "score": 50,
                    "sources": [
                      "https://www.analyticsvidhya.com/blog/2024/05/openai-security-measures/",
                      "https://openai.com/index/reimagining-secure-infrastructure-for-advanced-ai/",
                      "https://web.swipeinsight.app/posts/openai-unveils-security-architecture-for-frontier-ai-model-training-7065",
                      "https://www.rand.org/pubs/research_briefs/RBA2849-1.html"
                    ],
                    "justification": "OpenAI demonstrates some security-over-availability practices including network isolation, defense-in-depth for model weights protection, and restricted access controls, but public information lacks explicit confirmation of complete isolation policies that would sacrifice availability for security in critical production scenarios."
                  },
                  "Anthropic": {
                    "score": 25,
                    "sources": [
                      "https://www.anthropic.com/news/frontier-model-security",
                      "https://www.cnbc.com/2025/03/31/anthropic-announces-updates-on-security-safeguards-for-its-ai-models.html",
                      "https://www.rand.org/pubs/research_briefs/RBA2849-1.html",
                      "https://www.anthropic.com/news/activating-asl3-protections"
                    ],
                    "justification": "Anthropic has implemented significant security measures including ASL-3 controls, egress bandwidth limits, two-party authorization, and physical security sweeps, but no specific public information indicates they prioritize security over availability to the extent required for Security Level 5 (e.g., barring external device connections to isolated networks for debugging critical issues)."
                  },
                  "Google": {
                    "score": 25,
                    "sources": [
                      "https://situational-awareness.ai/lock-down-the-labs/",
                      "https://cloud.google.com/use-cases/secure-ai-framework",
                      "https://deepmind.google/discover/blog/introducing-the-frontier-safety-framework/"
                    ],
                    "justification": "Limited public information suggests Google has some security measures like VPC Service Controls and isolation capabilities, but no specific evidence found regarding complete network isolation or policies barring external devices for debugging critical production issues."
                  },
                  "xAI": {
                    "score": 0,
                    "sources": [],
                    "justification": "No specific public information found."
                  },
                  "Meta": {
                    "score": 0,
                    "sources": [],
                    "justification": "No specific public information found."
                  }
                }
              }
            ]
          }
        ]
      },
      {
        "name": "Other Organization Policies",
        "subcategories": [
          {
            "name": "",
            "controls": [
              {
                "name": "Eight independent security layers",
                "compliance": {
                  "OpenAI": {
                    "score": 0,
                    "sources": [
                      "https://www.analyticsvidhya.com/blog/2024/05/openai-security-measures/",
                      "https://web.swipeinsight.app/posts/openai-unveils-security-architecture-for-frontier-ai-model-training-7065",
                      "https://www.rand.org/pubs/research_briefs/RBA2849-1.html"
                    ],
                    "justification": "No specific public information found regarding OpenAI's compliance with the 'Eight independent security layers' framework as defined in the RAND report's Security Level 5 requirements."
                  },
                  "Anthropic": {
                    "score": 50,
                    "sources": [
                      "https://www.anthropic.com/rsp-updates",
                      "https://www.rand.org/pubs/research_reports/RRA2849-1.html",
                      "https://www.rand.org/pubs/research_briefs/RBA2849-1.html",
                      "https://www.anthropic.com/news/activating-asl3-protections"
                    ],
                    "justification": "Anthropic has implemented ASL-3 security standards with 'more than 100 different security controls' and multi-layered defense approaches, but specific details about eight independent security layers are not publicly disclosed. The RAND report itself doesn't define this as a specific SL-5 requirement."
                  },
                  "Google": {
                    "score": 0,
                    "sources": [
                      "https://www.rand.org/pubs/research_reports/RRA2849-1.html",
                      "https://safety.google/cybersecurity-advancements/saif/",
                      "https://deepmind.google/discover/blog/advancing-geminis-security-safeguards/",
                      "https://blog.google/technology/safety-security/introducing-googles-secure-ai-framework/",
                      "https://www.rand.org/pubs/research_briefs/RBA2849-1.html"
                    ],
                    "justification": "No specific public information found regarding Google's implementation of 'eight independent security layers' for AI model weights security as specified in the RAND Security Level 5 framework."
                  },
                  "xAI": {
                    "score": 0,
                    "sources": [],
                    "justification": "No specific public information found regarding xAI's implementation of the 'Eight independent security layers' for AI model weights security as outlined in the RAND Security Level 5 framework. While xAI has released Grok model weights publicly and mentions general security measures like encryption and audits, there are no detailed disclosures about comprehensive multi-layer security controls specifically addressing model weights protection against state-level actors."
                  },
                  "Meta": {
                    "score": 0,
                    "sources": [],
                    "justification": "No specific public information found regarding Meta's compliance with 'eight independent security layers' as described for Security Level 5 in the RAND report."
                  }
                }
              }
            ]
          }
        ]
      }
    ]
  }
]
